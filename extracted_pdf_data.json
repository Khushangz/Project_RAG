{
    "2404.08189v1.pdf": "Reducing hallucination in structured outputs via Retrieval-Augmented\nGeneration\nPatrice B\u00e9chard\nServiceNow\npatrice.bechard@servicenow.com\nOrlando Marquez Ayala\nServiceNow\norlando.marquez@servicenow.com\nAbstract\nA current limitation of Generative AI (GenAI)\nis its propensity to hallucinate. While Large\nLanguage Models (LLM) have taken the world\nby storm, without eliminating or at least reduc-\ning hallucination, real-world GenAI systems\nwill likely continue to face challenges in user\nadoption. In the process of deploying an enter-\nprise application that produces workflows from\nnatural language requirements, we devised a\nsystem leveraging Retrieval-Augmented Gen-\neration (RAG) to improve the quality of the\nstructured output that represents such work-\nflows. Thanks to our implementation of RAG,\nour proposed system significantly reduces hal-\nlucination and allows the generalization of our\nLLM to out-of-domain settings. In addition,\nwe show that using a small, well-trained re-\ntriever can reduce the size of the accompanying\nLLM at no loss in performance, thereby mak-\ning deployments of LLM-based systems less\nresource-intensive.\n1\nIntroduction\nWith the advent of Large Language Models\n(LLMs), structured output tasks such as converting\nnatural language to code or to SQL have become\ncommercially viable. A similar application is trans-\nlating a natural language requirement to a work-\nflow, a series of steps along with logic elements\nspecifying their relationships. These workflows\nencapsulate processes that are executed automati-\ncally upon certain conditions, thereby increasing\nemployee productivity. While enterprise systems\noffer such functionality to automate repetitive work\nand standardize processes, the barrier to entry is\nhigh, as building workflows requires specialized\nknowledge. Generative AI (GenAI) can lower this\nbarrier since novice users can specify in natural lan-\nguage what they want their workflows to execute.\nHowever, as with any GenAI application, using\nLLMs naively can produce untrustworthy outputs.\nFigure 1: Sample structured output (JSON) to generate\ngiven a natural language requirement.\nSuch is the public concern for LLMs producing hal-\nlucinations that the Cambridge Dictionary chose\nhallucinate as its Word of the Year in 2023 (Cam-\nbridge, 2023). Retrieval-Augmented Generation\n(RAG) is a well-known method that can reduce hal-\nlucination and improve output quality, especially\nwhen generating the correct output requires access\nto external knowledge sources (Gao et al., 2024).\nIn this work, we describe how, in the process\nof building a commercial application that converts\nnatural language to workflows, we employ RAG\nto improve the trustworthiness of the output by re-\nducing hallucination. Workflows are represented as\nJSON documents where each step is a JSON object.\nFigure 1 shows an example of a text requirement\nand its associated JSON document. For simplic-\nity, we include only the basic properties needed\nto identify a step along with properties indicating\nthe relationship between steps. Besides the work-\nflow steps, there may also be a trigger step that\ndetermines when the workflow should start, and\nsometimes this trigger requires a database table\nname. Hallucination in this task means generating\nproperties such as steps or tables that do not exist.\nWhile fine-tuning a sufficiently large LLM can\n1\narXiv:2404.08189v1  [cs.LG]  12 Apr 2024\n\nproduce reasonably good workflows, the model\nmay hallucinate, particularly if the natural language\ninput is out-of-distribution. As the nature of enter-\nprise users requires them to customize their appli-\ncations, in this case by adding their own type of\nworkflow steps, a commercial GenAI application\nneeds to minimize the out-of-distribution mismatch.\nWhile one could fine-tune the LLM per enterprise,\nthis may be prohibitively expensive due to the high\ninfrastructure costs of fine-tuning LLMs. Another\nconsideration when deploying LLMs is their foot-\nprint, making it preferable to deploy the smallest\nLLM that can perform the task.\nOur contributions are the following:\n\u2022 We provide an application of RAG in work-\nflow generation, a structured output task.\n\u2022 We show that using RAG reduces hallucina-\ntion and improves results.\n\u2022 We demonstrate that RAG allows deploying\na smaller LLM while using a very small re-\ntriever model, at no loss in performance.\n2\nRelated Work\nRetrieval-Augmented Generation is a common\napproach to limit generation of false or outdated\ninformation in classical NLP tasks such as question\nanswering and summarization (Lewis et al., 2020;\nIzacard and Grave, 2021; Shuster et al., 2021). In\nthe GenAI era, it refers to a process where relevant\ninformation from specific data sources is retrieved\nprior to generating text; the generation is then based\non this retrieved information (Gao et al., 2024). Our\nwork differs from standard RAG as we apply it to\na structured output task. Instead of retrieving facts,\nwe retrieve JSON objects that could be part of the\nJSON output document. Providing plausible JSON\nobjects to the LLM before generation increases the\nlikelihood that the output JSON properties exist\nand that the generated JSON can be executed.\nA crucial ingredient of RAG is the retriever since\nits output will be part of the LLM input. Compared\nto classical methods such as TF-IDF or BM25 that\nuse lexical information, Dense Retrieval has been\nshown to be more effective as it maps the semantics\nto a multidimensional space where both queries and\ndocuments are represented (Reimers and Gurevych,\n2019; Gao et al., 2021; Karpukhin et al., 2020;\nXiong et al., 2020). These retrievers are often\nused in open-domain question answering systems\n(Guu et al., 2020; Lee et al., 2019), where both\nqueries and documents are unstructured data and\nthus share the same semantic space. In our case, the\nqueries are unstructured (natural language) and the\ndocuments (JSON objects) are structured. Our re-\ntrieval training is similar to Structure Aware DeNse\nReTrievAl (SANTA), which proposes a training\nmethod to align the semantics between code and\ntext (Li et al., 2023b).\nGenerating structured data falls within the realm\nof Structured Output tasks, which consist of gen-\nerating a valid structured output from natural lan-\nguage, such as text-to-code, text-to-SQL (Zhong\net al., 2017; Yu et al., 2018; Wang et al., 2020)\nor if-then program synthesis (Quirk et al., 2015;\nLiu et al., 2016; Dalal and Galbraith, 2020). They\nare challenging as they not only require generating\noutput that can be parsed, but also entities or field\nvalues that exist in a given lexicon; otherwise the\nresulting output cannot be interpreted or compiled.\nFor simple database schemas or small lexicons, this\nextra information can be included in the prompt.\nHowever, in our task the available pool of steps that\ncan be part of a workflow is potentially very large\nand customizable per deployment, thereby making\nin-context learning impractical.\nWith the arrival of LLMs, these tasks have be-\ncome more accessible. In particular, Code LLMs\nenable developers to write code faster by providing\ninstructions to the LLM to generate code snippets\n(Chen et al., 2021; Nijkamp et al., 2022; Li et al.,\n2023a; Roziere et al., 2023). These models, trained\non large datasets of source code (Kocetkov et al.,\n2022), have acquired broad knowledge of many\nprogramming languages and have been shown to\nperform better at tasks that necessitate reasoning\n(Madaan et al., 2022). Since the JSON schema to\nrepresent workflows is domain-specific, we cannot\nuse these models off-the-shelf. While fine-tuning\nthem on a small dataset increases the quality of\nresults, extra steps are required to reduce hallucina-\ntion and support out-of-domain queries.\nLastly, an alternative and complementary tech-\nnique to reduce hallucination with LLMs is Guided\nGeneration using tools such as Outlines (Willard\nand Louf, 2023). A sufficiently expressive context-\nfree grammar could ensure that the steps generated\nby the model exist, but it does not provide extra\nknowledge as to which steps the flow should in-\nclude given the natural language query.\n2\n\nFigure 2: High-level architecture diagram showing how the user query is used by both the retriever and the LLM to\ngenerate the structured JSON output.\n3\nMethodology\nFigure 2 depicts the high-level architecture of our\nRAG system. During initialization, indices of steps\nand tables are created using the retriever. When a\nuser submits a request, the retriever is called to sug-\ngest steps and tables. The suggestions are then ap-\npended to the user query to form the LLM prompt.\nThe LLM is then called to generate the workflow\nin the JSON format via greedy decoding.\nTo build our system, we first train a retriever\nencoder to align natural language with JSON ob-\njects. We then train an LLM in a RAG fashion by\nincluding the retriever\u2019s output in its prompt.\n3.1\nRetriever training\nWe expect the LLM to learn to construct JSON doc-\numents including the relationship between work-\nflow steps, given sufficient examples. The risk of\nhallucination comes mainly from the step names\nsince there are tens of thousands of possible steps\nand every customer can add their own steps if the\ndefault set does not meet their needs. In addition,\nas some trigger steps require database table names\nas a property, these names can also be hallucinated.\nWe therefore require the retriever to map natural\nlanguage to existing step and database table names.\nWe choose to fine-tune a retriever model for two\nreasons: to improve the mapping between text and\nJSON objects, and to create a better representa-\ntion of the domain of our application. While there\nexist a myriad of open-source sentence encoders\n(Reimers and Gurevych, 2019; Ni et al., 2022), they\nhave been trained in a setting where both queries\nand documents are in the same natural language\nsemantic space. But in our case, the query or work-\nflow requirement is unstructured while the JSON\nobjects are structured data. Consistent with the\nresults reported by Li et al. (2023b), who search\ncode snippets based on text, fine-tuning improves\nthe retrieval results greatly. Similarly, fine-tuning\na model using our domain-specific data allows the\nretriever to learn the nuances and technicalities of\nthe text and JSON that are particular to our setting.\nWe use a siamese transformer encoder with mean\npooling similar to Reimers and Gurevych (2019)\nto encode both the user query and the step or table\nJSON object into fixed-length vectors. We include\na normalization layer in our model so that the re-\nsulting embeddings have a norm of 1. We generate\nthree embeddings vq \u2208Rn, vs \u2208Rn, vt \u2208Rn:\nvq = R(q)\nvs = R(s)\nvt = R(t)\n(1)\nwhere q, s, t are the user query, step, and table\nrespectively. Retriever R can be decomposed as:\nR(q) = Norm(MeanPool(Enc(q)))\n(2)\nThe retriever model is trained on pairs of user\nqueries and corresponding steps or tables. Since\ntable names are used only in certain examples de-\npending on the type of trigger, a query can be\nmapped to zero tables. For instance, the work-\nflow in Figure 1 has four steps, forming four posi-\ntive training pairs, each pair consisting of the same\nquery and one of the steps in the flow. As the daily\ntrigger step does not need a table name, the query\nis mapped to an empty list of tables.\nWe also construct negative training pairs by sam-\npling steps or tables that are not relevant to the user\nquery. We experiment with three different negative\nsampling strategies: random, BM25-based, and\nANCE-based (Xiong et al., 2020).\nThe retriever is trained using a contrastive loss\n(Hadsell et al., 2006) to minimize the distance be-\ntween positive pairs (Y = 1) and negative pairs\n(Y = 0). Given the cosine similarity between\nthe query and step (or table) vectors, and cosine\n3\n\ndistance D = 1 \u2212cossim(vq, vs), we define con-\ntrastive loss L as:\nL = 1\n2\n\u0012\nY D2 + (1 \u2212Y ) \u00b7 max(0, 1\n2 \u2212D)2\n\u0013\n(3)\nDuring initialization, we build an index of steps\nand tables using FAISS (Douze et al., 2024). When\na user submits a natural language query, we embed\nthe incoming query using our retriever and use\ncosine similarity to retrieve the max K steps and\ntables associated with this requirement.\n3.2\nLLM training\nContrary to end-to-end RAG systems such as Lewis\net al. (2020), we opted to train both the retriever and\nLLM separately, for simplicity. We use the trained\nretriever to augment our dataset with suggested\nstep and table names for each example. We then\nproceed with standard LLM supervised fine-tuning.\nFigure 3: Training example, where the last four lines\nare the expected output (in red). The underlined text\ncomes from the retriever\u2019s output.\nBy inserting the retriever\u2019s output in JSON for-\nmat into the LLM input, we effectively make this\nstructured output task easier as the LLM can copy\nthe relevant JSON objects during generation. Fig-\nure 3 shows an example of a training example. Ev-\nery line except the last four make up the LLM\nprompt. The suggested tables and steps come be-\nfore the user query and are underlined in the figure.\nWe exclude the most frequent steps from these sug-\ngestions as we expect the LLM to memorize them.\nAlso, in every LLM training example, we assume\nthe retriever has 100% recall: the steps and table\nrequired to build the structured output are always in\nthe suggestions, except for the most frequent steps.\nAs we are showing the LLM thousands of exam-\nples during training, we did not find it necessary to\nexperiment with complicated or verbose prompts:\nwe used a short and simple format, similar to Figure\n3, to reduce the number of input tokens while mak-\ning it clear that this is a structured output task. As\nshown in section 5.2, this approach yielded good\nperformance.\n4\nExperiments\nAs the task we are interested in is part of a commer-\ncial enterprise system, we had to devise our own\ndatasets as well as evaluation metrics.\n4.1\nDatasets\nFrom internal deployments of our enterprise plat-\nform, we extracted around 4,000 examples of de-\nployed workflows and asked annotators to write\nnatural language requirements for them. In addi-\ntion, using deterministic rules, we created around\n1,000 samples having simple and few steps in or-\nder to teach the model to handle input where the\nuser is incrementally building their workflow. To\nhave an unbiased estimate of the quality of results\nonce the system is deployed, we asked expert users\nto simulate interacting with the system through a\nsimple user interface where they typed their require-\nment. We used these interactions and the expected\nJSON documents to create an additional dataset\nsplit, named \"Human Eval.\" Our final metrics are\nbased on this split instead of the \"Test\" split, due\nto its higher quality and more realistic input. Table\n1 shows statistics for all of our in-domain splits.\nNot all samples require triggers, and a small subset\nrequire the model to generate tables.\nSplit\nSize\n# Triggers\n# Tables\nTrain\n2867\n823\n556\nDev\n318\n77\n44\nTest\n798\n247\n163\nHuman Eval\n157\n99\n60\nTable 1: Data statistics for in-domain training and eval-\nuation.\nA drawback of our data labeling approach is\nthat these internal datasets are mostly in the IT do-\nmain, whereas our RAG system can be deployed\nin diverse domains such as HR and finance. With-\nout assessing the quality of the system in out-of-\ndistribution settings, we cannot be confident that\nthe system will behave as expected. We therefore\nasked annotators to label five other splits, which\ncome from other deployments of our enterprise\nplatform. These are real workflows that have been\ncreated by real users.\n4\n\nTable 2 includes statistics for these out-of-\ndomain splits. A measure of how different they\nare from our training data is the % of steps that\nare not in the set of steps in the \"Train\" split. This\ndiscrepancy ranges from less than 10% to more\nthan 70%, highlighting the need to use a retriever\nand to customize the indices per deployment.\nSplit\nSize\n# Triggers\n# Tables\n% Steps not\nin Train\nOOD1\n146\n133\n47\n49%\nOOD2\n162\n111\n21\n76%\nOOD3\n429\n226\n114\n34%\nOOD4\n42\n25\n11\n33%\nOOD5\n353\n271\n26\n7%\nTable 2: Data statistics for out-of-domain evaluation.\nTo train the retriever encoder, we create pair ex-\namples out of the 4,000 extracted and 1,000 deter-\nministically generated samples, resulting in around\n15,000 pairs in the step names dataset and 1,500 in\nthe table names dataset. The quality of this encoder\nis evaluated on the \"Human Eval\" split described\nabove.\n4.2\nMetrics\nWe evaluate the entire RAG system using three\nmetrics, which can all range from 0 to 1:\n\u2022 Trigger Exact Match (EM) verifies whether\nthe generated JSON trigger is exactly the same\nas the ground-truth, including the table name\nif this trigger requires it.\n\u2022 Bag of Steps (BofS) measures the overlap\nbetween the generated JSON steps and the\nground-truth steps in an order-agnostic fash-\nion, akin to a bag-of-words approach.\n\u2022 Hallucinated Tables (HT) and Hallucinated\nSteps (HS) measure the % of generated ta-\nbles/steps that do not exist per workflow, in-\ndicating that they were invented by the LLM.\nThis is the only metric where lower is better.\nTo evaluate the retriever, we use Recall@15 for\nsteps and Recall@10 for tables. That is, given a\nnatural language requirement, we retrieve the top\nK steps/tables from their respective indices and\nverify whether they cover the set of steps and the\ntable, if required, included in the JSON document\nrepresenting the workflow.\n4.3\nModels\nAs this is a production system, we have a trade-off\nbetween model size and performance for both the\nLLM and the retriever encoder.\nWe fine-tune models of different sizes to mea-\nsure the impact of model size on the final metrics.\nAs StarCoderBase (Li et al., 2023a) has been pre-\ntrained on JSON in addition to many programming\nlanguages and comes in different sizes, we fine-\ntune its 1B, 3B, 7B and 15.5B variants. Given our\ninfrastructure constraints, we could deploy an LLM\nof at most 7B parameters. Thus we also fine-tune\nother pretrained LLMs of this size: CodeLlama-7B\n(Roziere et al., 2023) and Mistral-7B-v0.1 (Jiang\net al., 2023). All the LLMs were fine-tuned using\nthe same datasets and hyperparameters.\nWe use all-mpnet-base-v21 as the base retriever\nmodel. As it has only 110M parameters, it is suit-\nable for deployment. We compare our fine-tuned\nmodel against different sizes of off-the-shelf GTR-\nT5 models (Ni et al., 2022) to see whether larger\nencoders impact the performance.\nPlease see Appendix A for training details for\nboth the LLM and the retriever encoder.\n5\nResults\n5.1\nRetriever encoder\nTable 3 shows the results of retrieval on the \"Human\nEval\" split for both steps and tables. Scaling the\nsize of the off-the-shelf encoders, as we did with\nGTR-T5, does not yield significant improvements\non both retrieval metrics. A similar observation\nwas made by Neelakantan et al. (2022) for code\nretrieval. What was crucial to significantly improve\nthe performance was fine-tuning the encoder.\nModel (# Params)\nStep\nTable\nRecall@15\nRecall@10\ngtr-t5-base (110M)\n0.505\n0.489\ngtr-t5-large (355M)\n0.575\n0.511\ngtr-t5-xl (1.24B)\n0.579\n0.489\ngtr-t5-xxl (4.8B)\n0.561\n0.489\nall-mpnet-base-v2 (110M)\n0.425\n0.170\n+ Random\n0.640\n0.752\n+ BM25\n0.537\n0.586\n+ ANCE\n0.556\n0.699\n+ All\n0.743\n0.766\nTable 3: Evaluation of different encoders on step and\ntable retrieval. The last four rows represent encoders\nfine-tuned using different negative sampling strategies.\nDue to deployment considerations, we fine-tune\nthe smallest encoders (110M parameters), and\nfound that all-mpnet-base-v2 yielded the best per-\nformance after fine-tuning with all negative sam-\npling strategies.\n1https://huggingface.co/sentence-transformers/all-mpnet-\nbase-v2\n5\n\nTrigger\nBag of\nHallucinated\nHallucinated\nModel\nEM\nSteps\nSteps\nTables\nNo Retriever\nStarCoderBase-1B\n0.580\n0.645\n0.157\n0.192\nStarCoderBase-3B\n0.551\n0.648\n0.140\n0.214\nStarCoderBase-7B\n0.547\n0.669\n0.137\n0.206\nStarCoderBase (15.5B)\n0.632\n0.662\n0.160\n0.194\nWith Retriever\nStarCoderBase-1B\n0.591\n0.619\n0.072\n0.044\nStarCoderBase-3B\n0.615\n0.641\n0.017\n0.030\nStarCoderBase-7B\n0.664\n0.672\n0.019\n0.042\nStarCoderBase (15.5B)\n0.667\n0.667\n0.040\n0.016\nCodeLlama-7B\n0.623\n0.617\n0.039\n0.108\nMistral-7B-v0.1\n0.596\n0.617\n0.049\n0.045\nTable 4: Performance of various model types and sizes on the \"Human Eval\" split. Lower is better for the\nhallucination metrics. Results within 0.005 of the best score are highlighted in bold.\n5.2\nRetrieval-Augmented Generation\nOur main objective is to reduce hallucination while\nkeeping the overall performance high given our in-\nfrastructure constraints. Table 4 shows that without\na retriever (only LLM fine-tuning), the % of hal-\nlucinated steps and tables can be as high as 21%\non the \"Human Eval\" split. Using a retriever, this\ndecreases to less than 7.5% for steps and less than\n4.5% for tables with all StarCoderBase LLMs. All\nmodels produce valid JSON documents following\nthe expected schema, thanks to fine-tuning.\nWithout a retriever, scaling the size of the Star-\nCoderBase models improves the Bag of Steps and\nTrigger Exact Match metrics, albeit unevenly. Scal-\ning also helps with RAG, but we observe more\nconsistent improvements. This suggests that larger\nLLMs can better copy and paste retrieved steps and\ntables during generation.\nThe smallest RAG fine-tuned model (1B) hallu-\ncinates significantly more than its larger counter-\nparts. Among the other three variants, the 7B ver-\nsion gives us the best trade-off, as the performance\ndifference between 7B and 15.5B is marginal. An-\nother observation is that the 3B version trained with\nRAG is competitive even with the 15.5B version\nwithout RAG on the Trigger EM and Bag of Steps\nmetrics, while keeping hallucination low. This is a\nkey lesson as we could deploy a 3B RAG fine-tuned\nmodel if we had more limited infrastructure.\nLastly,\nwe compare the RAG fine-tuned\nStarCoderBase-7B to fine-tuning more recent\nLLMs of the same size. Despite also fine-tuning\nthem with RAG, CodeLlama-7B and Mistral-7B-\nv0.1 produce worse results across all metrics, even\ncompared to the smaller StarCoderBase-3B. We\nsuspect that pre-training on large amounts of natu-\nral language data may be detrimental to our task.\n5.3\nOOD evaluation\nWe want our approach to perform well on OOD sce-\nnarios without further fine-tuning the retriever or\nthe LLM. Table 5 assesses the performance of our\nchosen RAG fine-tuned StarCoderBase-7B model\non the five OOD splits described by Table 2.\nSplit\nTrigger EM\nBofS\nHS\nHT\nOOD1\n0.662\n0.619\n0.063\n0.051\nOOD2\n0.645\n0.612\n0.020\n0.151\nOOD3\n0.562\n0.743\n0.014\n0.033\nOOD4\n0.400\n0.671\n0.011\n0.154\nOOD5\n0.774\n0.770\n0.005\n0.063\nAvg.\n0.647\n0.714\n0.018\n0.066\nNo RAG Avg.\n0.544\n0.629\n0.020\n0.428\nHuman Eval\n0.664\n0.672\n0.019\n0.042\nTable\n5:\nPerformance\nof\nRAG\nfine-tuned\nStarCoderBase-7B on OOD splits.\nWe observe that on average, thanks to the re-\ntriever, all the OOD metrics are similar to the in-\ndomain results represented by the \"Human Eval\"\nsplit. We use a weighted average based on the\nnumber of samples per split.\nTo quantify the effect of suggesting step and\ntable names, we evaluate the RAG fine-tuned\nStarCoderBase-7B model without suggestions in\nrow \"No RAG Avg.\".\nAll metrics worsen sig-\nnificantly while the \"Hallucinated Steps\" remains\nroughly the same. Upon inspection, we see that the\nRAG fine-tuned model has learned to be conserva-\ntive in generating steps when it does not receive\nsuggestions, relying only on steps that it has seen\nduring training. On the other hand, the \"Halluci-\nnated Tables\" metric is significantly worse as the\nmodel is more creative when it comes to tables.\nPlease see Appendix B for supplementary detail.\n6\n\n5.4\nError Analysis\nWhen investigating error patterns found in the gen-\nerated workflows, we observe issues arising from\nfailures both on the retriever and the LLM.\nFor complex flows where steps that are used less\nfrequently need to be retrieved, if a crucial compo-\nnent is not in the retriever\u2019s suggestions, it becomes\ndifficult for the LLM to generate a valid workflow\nin line with the user query. To improve the re-\ntriever\u2019s recall, we can decompose the query into\nshorter texts to make the retrieval step more precise\nfor each step. This would mean performing several\nretrieval calls, potentially one per step, instead of\nmaking one single retrieval call as we are doing\nnow.\nIn some cases, the LLM did not produce the de-\nsired structure. This is more often seen when using\nsteps that determine the logic of the workflow, such\nas IF, TRY, or FOREACH. These are important errors\nthat can be addressed by synthetic data generation\nafter analyzing which steps are being missed. For\nexamples of perfect output and when the retriever\nand LLM fail, please refer to Appendix C.\n5.5\nImpact on Engineering\nThe obtained results led us to make several deci-\nsions that impacted the scalability and modular-\nity of the system. Since the best overall perfor-\nmance was given by a 7B-parameter model, we\ncould have a larger batch size for incoming user\nrequests, thereby increasing the system throughput\ngiven a single GPU. This implies a trade-off in la-\ntency as larger queries (in number of tokens) result\nin larger number of generated tokens, sometimes\ncausing large queries to become a bottleneck if they\nare included in a batch with many shorter queries.\nOur stress tests and user research reveal that the\ncurrent system overall response time is acceptable.\nObtaining good results after fine-tuning a very\nsmall encoder for the retriever (110M parameters),\nallowed us to deploy it on the same GPU with neg-\nligible effect on the larger LLM. But we could even\ndeploy the retriever on CPU due to its small size.\nA benefit of not performing joint training between\nthe retriever and the LLM is that the retriever can\nbe reused for other use cases involving similar data\nsources. Moreover, decoupling them allows clearer\nseparation of concerns and independent optimiza-\ntion by separate team members. Nevertheless, for\nscientific purposes, it is still worthwhile to experi-\nment with joint training.\nWe have several ideas to reduce the system re-\nsponse time: changing the structured output format\nfrom JSON to YAML to reduce the number of to-\nkens, leveraging speculative decoding (Leviathan\net al., 2023; Chen et al., 2023; Joao Gante, 2023),\nand streaming one step at a time back to the user\ninstead of the entire generated workflow.\n6\nConclusion\nWe propose an approach to deploy a Retrieval-\nAugmented LLM to reduce hallucination and allow\ngeneralization in a structured output task. Reduc-\ning hallucination is a sine qua non for users to\nadopt real-world GenAI systems. We show that\nRAG allows deploying a system in limited-resource\nsettings as a very small retriever can be coupled\nwith a small LLM. Future work includes improv-\ning the synergy between the retriever and the LLM,\nthrough joint training or a model architecture that\nallows them to work better together.\nEthical Considerations\nWhile our work proposes an approach to reduce\nhallucination in structure output tasks, we do not\nclaim that the risk of harm due to hallucination is\neliminated. Our deployed system includes a layer\nof post-processing to clearly indicate to users the\ngenerated steps that do not exist and urge them to\nfix the output before continuing their work.\nAcknowledgements\nWe thank our ServiceNow colleagues who worked\nhard in building the aforementioned system, from\nproject managers to quality engineers. We also\nthank the several colleagues who reviewed an ear-\nlier version of this paper: Lindsay Brin, Hessam\nAmini, Erfan Hosseini, and Gabrielle Gauthier-\nMelan\u00e7on, as well as the NAACL reviewers, for\ntheir valuable feedback.\nReferences\nCambridge. 2023.\nWhy hallucinate?\nhttps://\ndictionary.cambridge.org/editorial/woty.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\n7\n\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nDhairya Dalal and Byron V Galbraith. 2020. Evaluating\nsequence-to-sequence learning models for if-then pro-\ngram synthesis. arXiv preprint arXiv:2002.03485.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00e9. 2022.\nFlashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nAdvances in Neural Information Processing Systems,\n35:16344\u201316359.\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff\nJohnson, Gergely Szilvasy, Pierre-Emmanuel Mazar\u00e9,\nMaria Lomeli, Lucas Hosseini, and Herv\u00e9 J\u00e9gou.\n2024. The faiss library.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021,\npages 6894\u20136910. Association for Computational\nLinguistics (ACL).\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,\nMeng Wang, and Haofen Wang. 2024. Retrieval-\naugmented generation for large language models: A\nsurvey.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, pages 3929\u20133938.\nRaia Hadsell, Sumit Chopra, and Yann LeCun. 2006.\nDimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2006, pages 1735\u20131742.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In EACL 2021-16th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics, pages 874\u2013880.\nAssociation for Computational Linguistics.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nJoao Gante. 2023. Assisted generation: a new direction\ntoward low-latency text generation.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP). Association for\nComputational Linguistics.\nDenis Kocetkov, Raymond Li, LI Jia, Chenghao Mou,\nYacine Jernite, Margaret Mitchell, Carlos Mu\u00f1oz Fer-\nrandis, Sean Hughes, Thomas Wolf, Dzmitry Bah-\ndanau, et al. 2022. The stack: 3 tb of permissively li-\ncensed source code. Transactions on Machine Learn-\ning Research.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open do-\nmain question answering. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 6086\u20136096.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. In Proceedings of\nthe 34th International Conference on Neural Infor-\nmation Processing Systems, pages 9459\u20139474.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023a.\nStarcoder: may the source be with you!\narXiv preprint arXiv:2305.06161.\nXinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu,\nYu Gu, Zhiyuan Liu, and Ge Yu. 2023b. Structure-\naware language model pretraining improves dense\nretrieval on structured data. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 11560\u201311574, Toronto, Canada. Association\nfor Computational Linguistics.\nChang Liu, Xinyun Chen, Eui Chul Shin, Mingcheng\nChen, and Dawn Song. 2016. Latent attention for\nif-then program synthesis. Advances in Neural Infor-\nmation Processing Systems, 29.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of code\nare few-shot commonsense learners. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 1384\u20131403.\nArvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\nford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\nNikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\n2022. Text and code embeddings by contrastive pre-\ntraining. arXiv preprint arXiv:2201.10005.\n8\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-\nnandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith\nHall, Ming-Wei Chang, et al. 2022. Large dual en-\ncoders are generalizable retrievers. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 9844\u20139855.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan\nWang, Yingbo Zhou, Silvio Savarese, and Caiming\nXiong. 2022. Codegen: An open large language\nmodel for code with multi-turn program synthesis. In\nThe Eleventh International Conference on Learning\nRepresentations.\nChris Quirk, Raymond Mooney, and Michel Galley.\n2015. Language to code: Learning semantic parsers\nfor if-this-then-that recipes. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 878\u2013888.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP). Association\nfor Computational Linguistics.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784\u20133803.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. Rat-sql:\nRelation-aware schema encoding and linking for text-\nto-sql parsers. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7567\u20137578.\nBrandon T Willard and R\u00e9mi Louf. 2023.\nEffi-\ncient guided generation for llms.\narXiv preprint\narXiv:2307.09702.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N Bennett, Junaid Ahmed, and\nArnold Overwijk. 2020. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3911\u20133921.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\n9\n\nA\nTraining details for LLM and retriever\nAll LLMs were fine-tuned using the same set of hy-\nperparameters. We use the AdamW optimizer with\na learning rate of 5e \u22124, \u03b21 = 0.9, \u03b22 = 0.999\nand weight decay of 0.01. Models were trained for\n5,000 steps with a cosine learning rate scheduler\nwith 100 warmup steps. We use an effective batch\nsize of 32 for all models, using gradient accumula-\ntion when the batch size would not fit on a single\nGPU. We trained all models using LoRA (Hu et al.,\n2021) with r = 16, \u03b1 = 16 and a dropout rate of\n0.05. All models were trained with flash-attention\n(Dao et al., 2022) on a single A100 80GB GPU.\nWe fine-tuned the retriever model using the\nSentenceTransformers framework (Reimers and\nGurevych, 2019). We use the AdamW optimizer\n(Loshchilov and Hutter, 2018) and a learning rate\nof 2e \u22125. We use a batch size of 128 and train the\nmodel for 10 epochs.\nB\nDifferences in generation with and\nwithout suggestions\nTo understand the impact of suggesting step and\ntable names during generation, for each OOD split,\nwe inspect the % of unique steps and % of unique\ntable names that are hallucinated with and without\nsuggestions.\nTable 6 shows that without suggestions, the RAG\nfine-tuned StarCoderBase-7B tends to generate sig-\nnificantly fewer unique step names. Receiving sug-\ngestions allows the model to copy the suggestions,\nthereby increasing the diversity of what it gener-\nates. In addition, without suggestions a greater\npercentage of the unique step names it generates\nare invented.\nNo suggestions\nWith suggestions\nSplit\n# unique\n% H\n# unique\n% H\nsteps\nsteps\nOOD1\n52\n40%\n100\n13%\nOOD2\n38\n34%\n96\n13%\nOOD3\n122\n37%\n269\n9%\nOOD4\n20\n5%\n32\n9%\nOOD5\n88\n17%\n151\n3%\nTable 6: Statistics of generated step names in terms\nof uniqueness and hallucination. H refers to unique\nhallucinated step names.\nWe also see that even with suggestions, there is\nstill an important gap in the percentage of unique\nstep names that are hallucinated, as in some splits\nmore than 10% of unique steps are invented. While\nthe overall hallucination rate is less than 2%, as\nshown in Table 5, there are cases where the retriever\ndoes not suggest what is expected or the LLM does\nnot take into account the suggestions.\nNo suggestions\nWith suggestions\nSplit\n# unique\n% H\n# unique\n% H\ntables\ntables\nOOD1\n40\n70%\n22\n14%\nOOD2\n31\n71%\n19\n21%\nOOD3\n61\n64%\n44\n9%\nOOD4\n11\n54%\n9\n22%\nOOD5\n38\n68%\n29\n17%\nTable 7: Statistics of generated table names in terms\nof uniqueness and hallucination. H refers to unique\nhallucinated table names\nWhen it comes to table names, there are similar\nand different observations, as shown in Table 7. As\nin the case of step names, without suggestions a\ngreater percentage of unique table names are in-\nvented. However, when provided with suggestions,\nthe model is more conservative as it generates fewer\nunique table names. This may be an artifact of the\ndata, where there is less diversity of tables used\ncompared to step names.\nC\nSample perfect output and errors\nFigure 4 shows three user queries along with their\ngenerated workflows. The first one is a compli-\ncated workflow where the LLM is able to follow\nexactly the structure described in the user query,\nand is able to use the steps that the user expected.\nIn this case, the retriever suggests only the step\npost_incident_details, as the rest are consid-\nered common steps.\nIn the second example, the retriever fails to sug-\ngest the send_slack_message step. The result-\ning workflow is not entirely wrong but it is of\nlesser quality as the LLM uses the common step\nsend_notification, which is not what the user\nexpected.\nIn the last example, the LLM shows that it does\nnot sufficiently understand the semantics of the task.\nThe word Try in the user query should have made\nit use the TRY and CATCH flow logic, but the LLM\nseems to ignore this word, resulting in a workflow\nthat does not reflect what the user asked for.\n10\n\n(a) Perfect output\n(b) Retrieval error\n(c) LLM error\nFigure 4: Examples where both the retriever and the LLM worked perfectly and where each of them failed:\n(a) All expected step names were suggested and used by the LLM. (b) The retriever did not suggest the step\nsend_slack_message and therefore the LLM used the common step send_notification instead. (c) The LLM\nshould have used the TRY step as the parent to all the steps, but it did not fully understand the user query.\n11\n",
    "2409.09543v2.pdf": "Target Speaker ASR with Whisper\nAlexander Polok\u2217\u2020, Dominik Klement\u2217\u2020\u00a7, Matthew Wiesner\u00a7, Sanjeev Khudanpur\u00a7, Jan \u02c7Cernock\u00b4y\u2020, Luk\u00b4a\u02c7s Burget\u2020\n\u2020Brno University of Technology, Brno, Czech Republic\n\u00a7Johns Hopkins University, Baltimore, United States of America\nAbstract\u2014We propose a novel approach to enable the use of\nlarge, single-speaker ASR models, such as Whisper, for target\nspeaker ASR. The key claim of this method is that it is much\neasier to model relative differences among speakers by learning\nto condition on frame-level diarization outputs than to learn\nthe space of all speaker embeddings. We find that adding even\na single bias term per diarization output type before the first\ntransformer block can transform single-speaker ASR models\ninto target-speaker ASR models. Our approach also supports\nspeaker-attributed ASR by sequentially generating transcripts\nfor each speaker in a diarization output. This simplified method\noutperforms baseline speech separation and diarization cascade\nby 12.9 % absolute ORC-WER on the NOTSOFAR-1 dataset.\nIndex Terms\u2014target-speaker ASR, diarization conditioning,\nmulti-speaker ASR, Whisper\nI. INTRODUCTION\nSelf-supervised\nmodels\n[1]\u2013[3],\nLLMs\n[4],\n[5],\nand\nWhisper-style supervised models [6], [7] have demonstrated\nthat scaling up models by using more parameters and ex-\ntremely large amounts of data can enable the development of\naccurate automatic speech recognition (ASR) systems, even in\nrelatively challenging environments. However, these models\nhave primarily been used in single-speaker, single-channel\nASR systems, whereas most conversations involve multiple\nspeakers and are often recorded by one or more microphones.\nApproaches to handling this scenario generally integrate\nmultiple blocks performing source separation, speaker seg-\nmentation, overlapped speech detection, post-hoc speaker clus-\ntering, and ASR in order to produce speaker-attributed conver-\nsation transcripts. Alternatively, there are end-to-end systems\ntranscribing multi-speaker speech directly using special tokens\nor multiple heads [8]\u2013[11]. Another approach is the use of a\nsemi-end-to-end system, known as target-speaker ASR (TS-\nASR) [12]\u2013[17] processing the original input mixture and\ntranscribing each speaker individually.\nThe conventional TS-ASR framework extracts speaker em-\nbeddings corresponding to the target speakers and uses them\nas an auxiliary input to the ASR system [16], [18]. While pre-\ntrained speaker embedding extractors [19]\u2013[21] can guide the\nASR system by highlighting relevant information in the input\nand filtering out irrelevant content, they inherently require\nthe system to learn how to map speaker embeddings to ASR\nspeech embeddings. More recent methods include the use of\nadaptation layers and soft prompts to modify existing ASR\nmodels to incorporate speaker embeddings [14] or speaker\n\u2217These two authors contributed equally.\nenrollment audios [17]. However, since these models are often\ntrained on simulated datasets\u2014due to the scarcity of multi-\nspeaker ASR datasets and the need for a large number of\nspeaker identities\u2014they tend to experience significant perfor-\nmance degradation when applied to real-world multi-speaker\nscenarios [22]\u2013[24].\nFig. 1.\nProposed Diarization-Conditioned Whisper model: An input audio\nsegment with multiple speakers is conditioned by frame-level diarization\noutputs\n\u0002pt\nS\npt\nT\npt\nN\npt\nO\n\u0003T for each STNO class at every frame t.\nAffine transformations, applied to intermediate input representations zl\n1:T ,\ngenerate new embeddings, where l is the layer index. The final frame-level\nembedding is a convex combination of these embeddings for each frame.\nIn this paper, we propose a semi-end-to-end approach to\nTS-ASR that uses Whisper in a new way. Unlike previous\nTS-ASR methods, our system does not rely on speaker embed-\ndings, but instead conditions directly on frame-level diarization\narXiv:2409.09543v2  [eess.AS]  16 Jan 2025\n\noutputs. We believe that, compared to the aforementioned\nembedding-based approaches, only \u201drelative\u201d differentiation\nbetween speakers is needed; the TS-ASR system does not\nneed to adapt to an existing subspace of speaker embeddings.\nTraining our model on labeled examples of both target and\nnon-target speech may also improve speaker discrimination\nand improve robustness to diarization errors.\nTo validate our approach, we fine-tune Whisper models\non the NOTSOFAR-1 [22], AMI [23], and Libri2Mix [25]\ndatasets using ground truth speaker segmentation. Unless\nstated otherwise, ground truth segmentation is also used during\ninference. All experiments follow the NOTSOFAR-1 Chal-\nlenge guidelines1.\nII. DIARIZATION-CONDITIONED WHISPER\nThis section presents the Diarization-Conditioned Whisper,\na model built upon the Whisper architecture, designed to\nperform TS-ASR by conditioning on frame-level diarization\noutputs. An overview of the proposed model is shown in\nFig. 1. We adapt Whisper for TS-ASR by adding Frame-Level\nDiarization Dependent Transformations (FDDT) modules, de-\nscribed in Section II-C. These modules transform the model\u2019s\ninternal representations in order to differentiate between the\ntarget- and non-target speakers in the audio.\nA. Silence, Target, Non-Target, and Overlap Masks\nLet D \u2208[0, 1]S\u00d7T , where S is the number of speakers in\nthe recording, and T is the number of frames, represent the\ndiarization output, with d(s, t) denoting the probability that\nspeaker s is active at time frame t.\nThe dependency on the number of speakers in D can be\na limiting factor for easily incorporating this mask into the\nmodel. To address this, let sk represent the target speaker.\nWe define a distribution over the following mutually exclusive\nevents for a frame at time t:\n\u2022 S: Silence at time t.\n\u2022 T : Only the target speaker sk is active at t.\n\u2022 N: Non-target speaker(s) (s \u0338= sk) active; target sk\ninactive at t.\n\u2022 O: Target sk and at least one non-target (s \u0338= sk) active,\ncausing overlap at t.\nThe probabilities of these events occurring at time frame t\ncan be calculated as:\npt\nS =\nS\nY\ns=1\n(1 \u2212d(s, t)),\npt\nT = d(sk, t)\nS\nY\ns=1\ns\u0338=sk\n(1 \u2212d(s, t))\npt\nN =\n\u00001 \u2212pt\nS\n\u0001\n\u2212d (sk, t) ,\npt\nO = d(sk, t) \u2212pt\nT\n(1)\nThis definition allows us to use a fixed-sized target-speaker-\ndependent STNO (Silence, Target, Non-target, Overlap) mask\n\u0002pt\nS\npt\nT\npt\nN\npt\nO\n\u0003\u22a4.\n1https://www.chimechallenge.org/challenges/chime8/task2/\nB. Input Masking\nHaving the STNO mask, a straightforward way to perform\ntarget speaker ASR is to mask the signal by multiplying\neach frame by the probability that it is target speech or\nthat it involves overlap with the target speaker. Hence, if\nthe target speaker is not active, the audio signal is set to 0\n(i.e., silence). We add pt\nT and pt\nO to ensure that both target\nspeech and overlapping speech are preserved in the masked\nsignal. However, similar to source separation approaches, this\nmethod has limitations. It can introduce artifacts because we\nare creating a modified version of the input signal, and errors\nin diarization can propagate through the system, potentially\naffecting the model\u2019s performance.\nC. Frame-Level Diarization Dependent Transformations\nTo overcome issues of Input Masking, we designed a soft\nversion called Frame-Level Diarization Dependent Transfor-\nmations (FDDT). This approach modifies the frame-by-frame\nmodel inputs based on the diarization outputs.\nLet Zl \u2208Rdm\u00d7T represent the frame-by-frame inputs\nto the l-th (Transformer) layer. We transform these hidden\nrepresentations by applying four affine STNO layer- and class-\nspecific transformations: Wl\nS, Wl\nT , Wl\nN , Wl\nO \u2208Rdm\u00d7dm\ntogether with biases bl\nS, bl\nT , bl\nN , bl\nO \u2208Rdm to obtain new\nspeaker-specific hidden representations \u02c6Zl = [\u02c6zl\n1, . . . , \u02c6zl\nT ] as\n\u02c6zl\nt =\n\u0000Wl\nSzl\nt + bl\nS\n\u0001\npt\nS +\n\u0000Wl\nT zl\nt + bl\nT\n\u0001\npt\nT\n+\n\u0000Wl\nN zl\nt + bl\nN\n\u0001\npt\nN +\n\u0000Wl\nOzl\nt + bl\nO\n\u0001\npt\nO,\n(2)\nThese transformations generate four distinct representa-\ntions of the frame-by-frame inputs, each highlighting one of\nthe STNO classes. A target-speaker-specific representation is\nformed by a convex combination of these terms, with weights\nderived from the STNO mask. The same transformation is\napplied to all frames with identical STNO masks.\nFine-tuning the model using randomly initialized FDDT\nmatrices could easily disrupt the internal representations of\nthe model. Therefore, we propose initialization strategies to\nmitigate this risk:\n\u2022 Identity Initialization (Non-Disturbing Init): Here, biases\nare initialized with zero vectors, and weights are initial-\nized as identity matrices. This method ensures that the\nmodel\u2019s internal representations are not altered.\n\u2022 Suppressive Initialization: To bias the model toward\nmasking other speakers, we initialize the Wl\nS, Wl\nN\nweights as diagonal matrices with values close to zero,\ne.g., 0.1. This approach helps the model to distinguish\nbetween different types of speech, reinforcing the sepa-\nration between the STNO classes.\nIII. EXPERIMENTS\nWe primarily conducted our experiments on the new\nNOTSOFAR-1 dataset [22], which includes 280 meetings,\neach averaging 6 minutes, capturing diverse real-world acous-\ntic conditions and conversational dynamics. To assess gen-\neralization and competitiveness, we also evaluated our best\n\nTABLE I\nCOMPARISON OF THE PROPOSED SYSTEM BUILT WITH UPDATED\nWHISPER-LARGE-V3-TURBO [28] ALONGSIDE VARIOUS MULTI-TALKER\nASR SYSTEMS. THE TOP SECTION INCLUDES SYSTEMS WHERE NO\nADDITIONAL GROUND TRUTH INFORMATION ABOUT SPEAKER IDENTITY\nOR SEGMENTATION IS PROVIDED. THE BOTTOM SECTION FEATURES\nMODELS THAT DIRECTLY OR INDIRECTLY UTILIZE GROUND TRUTH\nSEGMENTATION INFORMATION. RESULTS MARKED WITH \u2020 ARE\nEVALUATED ON UTTERANCE GROUPS. PROPOSED ORC WER RESULTS\nMARKED WITH \u22c6WERE APPROXIMATED BY INCREASING THE COLLAR FOR\nTCORC WER.\nAMI-sdm\ntest\nORC WER\nNOTSOFAR-1\neval-small\ntcORC WER\nLibri2Mix\ntest-both\nORC WER\nRaj et al. [8]\n44.6\u2020\n60.9\u2020\nVinnikov et al. [22]\n35.5\nNiu et al. [29]\n17.7\nProposed\n18.0\u22c6\n22.6\n14.9\nInput masking\n79.1\n76.6\n56.7\nMa et al. [14]\n26.4\nZhang et al. [15]\n23.5\nProposed\n16.5\u22c6\n19.1\n10.9\nmodels on the synthetic Libri2Mix dataset [25] and on real-\nworld meeting dataset AMI [23]. Experiments are divided into\ntwo parts: In Section III-C, we analyze the FDDT behavior\nunder different weight structure constraints, initializations,\nnumbers of additional parameters, and provided information.\nIn Section III-D, we evaluate the framework\u2019s performance as\nmore parameters and data are added.\nSource codes and recipes2 are built on top of the trans-\nformers library [26]. All models are evaluated with the Time-\nConstrained Optimal Reference Combination Word Error Rate\n(tcORC WER) [27], referred to as WER throughout the text.\nA. Training details\nTo enhance Whisper\u2019s performance, we incorporated an\nadditional CTC (Connectionist Temporal Classification) head,\nfollowing the hybrid CTC-attention-based training scheme\nproposed in [30]. Given Whisper\u2019s large 50k vocabulary size\nand fixed sequence length, adding an extra projection layer\nposes memory challenges. To overcome this, we added two\nconvolutional layers, each with a subsampling factor of two,\nalong with an additional self-attention layer. Both the CTC\nhead and the decoder are trained with timestamp tokens, and\nthe CTC loss weight is set to 0.3.\nFor the ablation experiments, we used Whisper-medium.en,\nwhile the final model was trained with Whisper-large-v3-turbo.\nAll models are trained with an overall batch size of 64 samples\nusing bf16 precision and the AdamW optimizer [31]. The\nlearning rate is set to 2 \u00d7 10\u22126, with a weight decay of\n1 \u00d7 10\u22126, a linear decay scheduler, and 2k warm-up steps.\nThe new parameters introduced by FDDT are trained with a\nlearning rate of 2 \u00d7 10\u22124. By default, FDDT modules are\ninserted before all layers of the encoder with the diagonal\nconstraint, meaning that only the diagonal values of the weight\n2https://github.com/BUTSpeechFIT/TS-ASR-Whisper\nTABLE II\nANALYSIS OF DIFFERENT CONSTRAINTS APPLIED TO THE FDDT\nPARAMETERS AND METHODS USED TO INITIALIZE THEM EVALUATED\nWITH WHISPER-MEDIUM.EN ON NOTSOFAR-1 EVAL-SMALL. THE FDDT\nPARAMETERS COLUMN SPECIFIES WHICH PARAMETERS ARE USED TO\nCONDITION THE MODEL. Wdiag = diag(w), WHERE diag(w) IS A\nDIAGONAL MATRIX WITH ELEMENTS OF w IN THE DIAGONAL.\nInitilization Method\nFDDT parameters\nRandom\nIdentity\nSuppressive\nb\n28.4\n28.0\n28.0\nWdiag, b\n129.4\n27.3\n26.7\nW, b\n129.0\n46.1\n44.6\nmatrices are updated during training and can be non-zero.\nUnless otherwise stated, the CTC head undergoes an initial\n\u201cCTC preheating\u201d phase, where it is trained on LibriSpeech for\n10k steps, with the rest of the model being frozen. Afterwards,\nFDDT and CTC parameters are trained for a single epoch\n(FDDT preheating). Finally, the full model is trained for up to\n50k steps, with early stopping set to a patience of 5 epochs.\nMost of the models typically converge within ten epochs.\nFor the final evaluation, we always select the best-performing\ncheckpoint based on the development set WER.\nB. Comparison to Baselines\nTable I presents a comparison of the proposed method\nwith various end-to-end and modular systems. The top section\nincludes systems where no additional ground truth information\nabout speaker identity or segmentation is provided, and the\nsystem must infer this information. For the proposed system,\nwe utilized DiariZen [32]3 to condition our model. The bot-\ntom section features models that directly or indirectly utilize\nground truth segmentation information. It can be seen that our\napproach significantly outperforms the naive input masking\nbaseline across all three datasets, largely due to its fine-\ntuning capabilities and robust handling of overlapped speech.\nAlthough our method does not achieve the top performance\non the NOTSOFAR dataset, it surpasses baseline models,\nincluding the NOTSOFAR baseline [22] and the fine-tuned\nSURT model [8]. Furthermore, our approach delivers strong\nresults on the AMI and Libri2Mix datasets, underscoring its\neffectiveness across diverse scenarios.\nC. Frame-Level Diarization Dependent Transformation\nTo assess the impact of FDDT, we evaluated whether reduc-\ning the number of parameters in the additional modules affects\nthe performance and examined the importance of proper ini-\ntialization. As shown in Table II, using biases alone performs\ncomparably to the combination of diagonal transformation\nmatrices and biases, indicating that biasing frame-by-frame\nrepresentations is sufficient to effectively focus the model\non frames belonging to the same STNO class. The table\nalso highlights that randomly initializing FDDT parameters\nis suboptimal and can significantly harm model performance,\n3https://github.com/BUTSpeechFIT/DiariZen\n\nTABLE III\nEFFECT OF INCREASING THE NUMBER OF ENCODER LAYERS, WHERE\nFDDTS ARE APPLIED ON NOTSOFAR-1 EVAL-SMALL WITH\nWHISPER-MEDIUM.EN (24 ENCODER LAYERS).\nInitilization Method\nFDDT parameters\n# layers\nRandom\nIdentity\nSuppressive\nb\n1\n28.7\n30.9\n29.3\n12\n28.7\n27.6\n27.6\n24\n28.4\n28.0\n28.0\nWdiag, b\n1\n117.7\n27.8\n27.0\n12\n118.9\n27.4\n27.1\n24\n129.4\n27.3\n26.7\nsuggesting that suppressive initialization is preferable. Inter-\nestingly, using non-restricted weights, meaning full matrices\nwithout diagonal constraints leads to noticeable performance\ndegradation.\nTable III demonstrates the effect of increasing the number\nof encoder layers where FDDTs are applied. The number of\nlayers refers to how many layers, starting from the first encoder\nlayer, are modified by FDDT parameters. For example, when\nonly one layer is used, only the input to the Whisper encoder\nis modified. The results indicate that even a single layer of\nbias-only parameters can achieve performance comparable to\nthe best diagonal setup. However, using a random initialization\nwith diagonal matrices, even for just the first layer, severely\nimpacts the model\u2019s performance. Notably, the use of a diag-\nonal setup with both Wdiag and b across 24 layers yields the\nbest performance, with the suppressive initialization method\nachieving the lowest WER of 26.7.\nTable IV presents the performance of the FDDT method un-\nder different reductions of diarization information. It demon-\nstrates that while the FDDT model still significantly outper-\nforms input masking, some performance degradation is evident\nwhen comparing the STNO and T configurations. Interestingly,\nusing three classes (TNO) results in worse performance than\nusing (TN) classes only, even though the STNO and TNO\nconfigurations provide the same amount of information.\nD. Scaling System With More Data And Parameters\nTable V explores the impact of incorporating additional\ntraining data on system performance. The results show that\nadding the AMI dataset improves the performance on the\nNOTSOFAR-1, highlighting the benefits of more real-world\ndata. Furthermore, incorporating Libri2Mix data provides ad-\nTABLE IV\nTHE PERFORMANCE OF FDDT GIVEN A REDUCTION OF INFORMATION\nPROVIDED FROM DIARIZATION OUTPUT ON NOTSOFAR-1 EVAL-SMALL\nWITH WHISPER-MEDIUM.EN. WHEN EMPLOYING THE STNO MASK, ALL\nFRAMES ARE TRANSFORMED, WHILE WITH TNO, FRAMES\nCORRESPONDING TO THE SILENCE ARE LEFT UNCHANGED.\nSTNO\nTNO\nTN\nT\n26.7\n30.0\n28.7\n34.8\nTABLE V\nDIFFERENT SIZES OF TRAINING CORPORA AFFECTING THE PERFORMANCE\nOF WHISPER-MEDIUM.EN. TESTED ON NOTSOFAR-1 EVAL SMALL.\nNOTSOFAR-1\n+ AMI\n+ Libri2Mix\n26.7\n25.6\n24.8\nditional gains, raising the question of whether pretraining on\nsynthetic data could offer even greater performance gains.\nFinally, Table VI provides a performance analysis across\ndifferent model sizes, highlighting the improvements from\nemploying an additional CTC head. Notably, incorporating a\nrandomly initialized CTC head does not improve performance\nand leads to degradation. However, preheating the CTC head\nprovides some improvements, especially for the small and\nmedium models, even without joint decoding. Additionally, an\nFDDT preheating phase further enhances performance, with\nthe best results observed in the large-v3 model.\nTABLE VI\nINFLUENCE OF CTC HEAD AND SIZE OF THE MODEL EVALUATED ON\nNOTSOFAR-1 EVAL-SMALL.\nsmall.en\nmedium.en\nlarge-v3\nwithout CTC\n30.3\n28.1\n24.6\nwith CTC\n31.0\n28.9\n25.8\n+ CTC Preheating\n29.2\n26.7\n25.2\n+ FDDT Preheating\n30.3\n27.4\n24.5\nIV. CONCLUSIONS AND LIMITATIONS\nIn this study, we introduced FDDT into the Whisper model,\nmaking it a target-speaker ASR (TS-ASR) system conditioned\non diarization output. We analyzed factors such as model size,\nFDDT placement, dataset size, and parameter initialization,\nevaluating the system on both real and synthetic datasets with\nconsistently strong results. We also examined the impact of\nincorporating a CTC head, noting some improvements when\nit was preheated.\nWhile the FDDT-based TS-ASR method performed effec-\ntively across diverse datasets, further validation on varied\ndatasets, conditions, and languages is needed. We identified\nstrengths and limitations, particularly regarding robustness\nagainst diarization errors, which need further attention for bet-\nter real-world performance. Finally, the approach is extendable\nto other pre-trained ASR models, and a comparative analysis\nacross ASR architectures could offer deeper insights into its\nadaptability and benefits.\nACKNOWLEDGEMENT\nThe work was supported by Ministry of Education, Youth\nand Sports of the Czech Republic (MoE) through the OP\nJAK project \u201dLinguistics, Artificial Intelligence and Language\nand Speech Technologies: from Research to Applications\u201d\n(ID:CZ.02.01.01/00/23 020/0008518) and Brno Ph.D. Talent\nScholarship Programme. Computing on IT4I supercomputer\nwas supported by MoE through the e-INFRA CZ (ID:90254).\n\nREFERENCES\n[1] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,\nT. Yoshioka, X. Xiao et al., \u201cWavLM: Large-scale self-supervised pre-\ntraining for full stack speech processing,\u201d IEEE Journal of Selected\nTopics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\n[2] W.-N. Hsu, Y.-H. H. Tsai, B. Bolte, R. Salakhutdinov, and A. Mohamed,\n\u201cHuBERT: How much can a bad teacher benefit ASR pre-training?\u201d in\nICASSP 2021-2021 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2021, pp. 6533\u20136537.\n[3] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu,\nA. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., \u201cScaling speech\ntechnology to 1,000+ languages,\u201d Journal of Machine Learning Re-\nsearch, vol. 25, no. 97, pp. 1\u201352, 2024.\n[4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., \u201cGPT-4\ntechnical report,\u201d arXiv preprint arXiv:2303.08774, 2023.\n[5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n\u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint\narXiv:2302.13971, 2023.\n[6] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, \u201cRobust speech recognition via large-scale weak supervi-\nsion,\u201d in International conference on machine learning.\nPMLR, 2023,\npp. 28 492\u201328 518.\n[7] Y. Peng, J. Tian, B. Yan, D. Berrebbi, X. Chang, X. Li, J. Shi, S. Arora,\nW. Chen, R. Sharma, W. Zhang, Y. Sudo, M. Shakeel, J.-W. Jung,\nS. Maiti, and S. Watanabe, \u201cReproducing Whisper-style training using an\nopen-source toolkit and publicly available data,\u201d in 2023 IEEE Automatic\nSpeech Recognition and Understanding Workshop (ASRU), 2023, pp. 1\u2013\n8.\n[8] D. Raj, D. Povey, and S. Khudanpur, \u201cSurt 2.0: Advances in\ntransducer-based multi-talker speech recognition,\u201d IEEE/ACM Trans.\nAudio, Speech and Lang. Proc., vol. 31, p. 3800\u20133813, sep 2023.\n[Online]. Available: https://doi.org/10.1109/TASLP.2023.3318398\n[9] C. Li, Y. Qian, Z. Chen, N. Kanda, D. Wang, T. Yoshioka, Y. Qian, and\nM. Zeng, \u201cAdapting multi-lingual ASR models for handling multiple\ntalkers,\u201d in Interspeech 2023, 2023, pp. 1314\u20131318.\n[10] N. Kanda, Y. Gaur, X. Wang, Z. Meng, and T. Yoshioka, \u201cSerialized\noutput training for end-to-end overlapped speech recognition,\u201d in Inter-\nspeech 2020, 2020, pp. 2797\u20132801.\n[11] Y. Qian, X. Chang, and D. Yu, \u201cSingle-channel multi-talker speech\nrecognition with permutation invariant training,\u201d Speech Communica-\ntion, vol. 104, pp. 1\u201311, 2018.\n[12] N. Kanda, S. Horiguchi, R. Takashima, Y. Fujita, K. Nagamatsu, and\nS. Watanabe, \u201cAuxiliary interference speaker loss for target-speaker\nspeech recognition,\u201d in Interspeech 2019, 2019, pp. 236\u2013240.\n[13] Y. Zhang, K. C. Puvvada, V. Lavrukhin, and B. Ginsburg, \u201cConformer-\nbased target-speaker automatic speech recognition for single-channel\naudio,\u201d in ICASSP 2023 - 2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.\n[14] H. Ma, Z. Peng, M. Shao, J. Li, and J. Liu, \u201cExtending Whisper with\nprompt tuning to target-speaker ASR,\u201d in ICASSP 2024-2024 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP).\nIEEE, 2024, pp. 12 516\u201312 520.\n[15] W. Zhang and Y. Qian, \u201cWeakly-supervised speech pre-training: A case\nstudy on target speech recognition,\u201d in Interspeech 2023, 2023, pp.\n3517\u20133521.\n[16] Z. Huang, D. Raj, P. Garc\u00b4\u0131a, and S. Khudanpur, \u201cAdapting self-\nsupervised models to multi-talker speech recognition using speaker\nembeddings,\u201d in ICASSP 2023 - 2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.\n[17] L. Meng, J. Kang, Y. Wang, Z. Jin, X. Wu, X. Liu, and H. Meng,\n\u201cEmpowering Whisper as a joint multi-talker and target-talker speech\nrecognition system,\u201d in Interspeech 2024, 2024, pp. 4653\u20134657.\n[18] M. Karafi\u00b4at, L. Burget, P. Mat\u02c7ejka, O. Glembek, and J. \u02c7Cernock`y,\n\u201ciVector-based discriminative adaptation for automatic speech recog-\nnition,\u201d in 2011 IEEE Workshop on Automatic Speech Recognition &\nUnderstanding.\nIEEE, 2011, pp. 152\u2013157.\n[19] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-\nend factor analysis for speaker verification,\u201d IEEE Transactions on\nAudio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798,\n2010.\n[20] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur,\n\u201cX-vectors: Robust DNN embeddings for speaker recognition,\u201d in 2018\nIEEE international conference on acoustics, speech and signal process-\ning (ICASSP).\nIEEE, 2018, pp. 5329\u20135333.\n[21] H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng,\nand Y. Qian, \u201cWespeaker: A research and production oriented speaker\nembedding learning toolkit,\u201d in IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023, pp.\n1\u20135.\n[22] A. Vinnikov, A. Ivry, A. Hurvitz, I. Abramovski, S. Koubi, I. Gurvich,\nS. Peer, X. Xiao, B. M. Elizalde, N. Kanda, X. Wang, S. Shaer, S. Yagev,\nY. Asher, S. Sivasankaran, Y. Gong, M. Tang, H. Wang, and E. Krupka,\n\u201cNotsofar-1 challenge: New datasets, baseline, and tasks for distant\nmeeting transcription,\u201d in Interspeech 2024, 2024, pp. 5003\u20135007.\n[23] I. Mccowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn,\nM. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lath-\noud, M. Lincoln, A. Lisowska Masson, W. Post, D. Reidsma, and\nP. Wellner, \u201cThe AMI meeting corpus,\u201d Int\u2019l. Conf. on Methods and\nTechniques in Behavioral Research, 01 2005.\n[24] F. Yu, S. Zhang, Y. Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo,\nZ. Yan, B. Ma, X. Xu, and H. Bu, \u201cM2Met: The ICASSP 2022 multi-\nchannel multi-party meeting transcription challenge,\u201d ICASSP 2022 -\n2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 6167\u20136171, 2021.\n[25] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent,\n\u201cLibriMix: An open-source dataset for generalizable speech separation,\u201d\n2020. [Online]. Available: https://arxiv.org/abs/2005.11262\n[26] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,\nP. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer,\nP. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger,\nM. Drame, Q. Lhoest, and A. M. Rush, \u201cTransformers: State-of-\nthe-art natural language processing,\u201d in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing:\nSystem\nDemonstrations.\nOnline:\nAssociation\nfor\nComputational\nLinguistics,\nOct.\n2020,\npp.\n38\u201345.\n[Online].\nAvailable:\nhttps:\n//www.aclweb.org/anthology/2020.emnlp-demos.6\n[27] T. v. Neumann, C. B. Boeddeker, M. Delcroix, and R. Haeb-Umbach,\n\u201cMeetEval: A Toolkit for Computation of Word Error Rates for Meeting\nTranscription Systems,\u201d in Proceedings of the 7th International Work-\nshop on Speech Processing in Everyday Environments (CHiME 2023),\n2023, pp. 27\u201332.\n[28] A. Polok, D. Klement, M. Kocour, J. Han, F. Landini, B. Yusuf,\nM. Wiesner, S. Khudanpur, J. \u02c7Cernock\u00b4y, and L. Burget, \u201cDicow:\nDiarization-conditioned whisper for target speaker automatic speech\nrecognition,\u201d 2024, submitted to Computer Speech & Language\n(CSL) journal special issue on Multi-Speaker, Multi-Microphone,\nand Multi-Modal Distant Speech Recognition. [Online]. Available:\nhttps://arxiv.org/abs/2501.00114\n[29] S. Niu, R. Wang, J. Du, G. Yang, Y. Tu, S. Wu, S. Qian, H. Wu, H. Xu,\nX. Zhang, G. Zhong, X. Yu, J. Chen, M. Wang, D. Cai, T. Gao, G. Wan,\nF. Ma, J. Pan, and J. Gao, \u201cThe USTC-NERCSLIP Systems for the\nCHiME-8 NOTSOFAR-1 Challenge,\u201d in 8th International Workshop on\nSpeech Processing in Everyday Environments (CHiME 2024), 2024, pp.\n31\u201336.\n[30] T. Hori, S. Watanabe, and J. Hershey, \u201cJoint CTC/attention decoding\nfor end-to-end speech recognition,\u201d in Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), R. Barzilay and M.-Y. Kan, Eds.\nVancouver, Canada:\nAssociation for Computational Linguistics, Jul. 2017, pp. 518\u2013529.\n[Online]. Available: https://aclanthology.org/P17-1048\n[31] I. Loshchilov and F. Hutter, \u201cDecoupled Weight Decay Regularization,\u201d\nin\nInternational\nConference\non\nLearning\nRepresentations,\n2019.\n[Online]. Available: https://openreview.net/forum?id=Bkg6RiCqY7\n[32] J. Han, F. Landini, J. Rohdin, A. Silnova, M. Diez, and L. Burget,\n\u201cLeveraging self-supervised learning for speaker diarization,\u201d in ICASSP\n2025-2025 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2025.\n",
    "2411.00784v2.pdf": "FIRE\n: Fact-checking with Iterative Retrieval and Verification\nZhuohan Xie1 Rui Xing1, 2 Yuxia Wang1 Jiahui Geng1\nHasan Iqbal1 Dhruv Sahnan1 Iryna Gurevych1 Preslav Nakov1\n1MBZUAI, 2The University of Melbourne\n{zhuohan.xie, preslav.nakov}@mbzuai.ac.ae\nAbstract\nFact-checking long-form text is challenging,\nand it is therefore common practice to break it\ndown into multiple atomic claims. The typical\napproach to fact-checking these atomic claims\ninvolves retrieving a fixed number of pieces of\nevidence, followed by a verification step. How-\never, this method is usually not cost-effective,\nas it underutilizes the verification model\u2019s inter-\nnal knowledge of the claim and fails to replicate\nthe iterative reasoning process in human search\nstrategies. To address these limitations, we\npropose FIRE, a novel agent-based framework\nthat integrates evidence retrieval and claim\nverification in an iterative manner.\nSpecifi-\ncally, FIRE employs a unified mechanism to\ndecide whether to provide a final answer or\ngenerate a subsequent search query, based on\nits confidence in the current judgment. We\ncompare FIRE with other strong fact-checking\nframeworks and find that it achieves slightly\nbetter performance while reducing large lan-\nguage model (LLM) costs by an average of 7.6\ntimes and search costs by 16.5 times. These\nresults indicate that FIRE holds promise for\napplication in large-scale fact-checking oper-\nations.\nOur code is available at https://\ngithub.com/mbzuai-nlp/fire.git.\n1\nIntroduction\n\u201cEvery man has a right to his opinion,\nbut no man has a right to be wrong in his\nfacts.\u201d - Bernard M. Baruch\nLarge language models (LLMs) have demon-\nstrated exceptional performance across a wide\nrange of tasks, including both language comprehen-\nsion and generation (Zhao et al., 2023; Xie et al.,\n2023a). Consequently, LLMs are now widely ap-\nplied in various domains (Xie et al., 2023b), and\nmany users increasingly rely on the information\nthey provide. However, this reliance is problematic,\nas LLMs are capable of producing outputs that are\nhighly confident but factually incorrect, highlight-\ning the critical need for robust fact-checking sys-\ntems (Akhtar et al., 2023). However, fact-checking\nthe entire output of LLMs in a single step is highly\nchallenging. To address this, Min et al. (2023)\nproposed decomposing the content into multiple\natomic claims, each of which can be individually\nverified. While this approach simplifies the fact-\nchecking process, assessing the veracity of these\natomic claims remains complex, especially when\nmany require sourcing evidence from the web. In-\ndeed, identifying the most relevant evidence online\nis a key challenge in fact-checking pipelines (Wang\net al., 2024a).\nTo address this issue, conventional methods,\nsuch as FACTOOL and FACTCHECK-GPT (Chern\net al., 2023; Wang et al., 2024a), frame the prob-\nlem as a question-answering task, as illustrated on\nthe left side of Figure 1. In these approaches, an\nLLM is prompted to generate N relevant questions,\nwhich are then used as search queries by a web\nsearch tool. The search results serve as evidence\nfor LLM to determine the factuality of the claim.\nHowever, we argue that this process is inefficient in\ntwo key aspects. First, it underutilizes the internal\nknowledge already embedded in LLMs during pre-\ntraining. For claims involving common knowledge\nor widely known events, the LLM could confidently\nassess the claim without relying on external infor-\nmation. Second, generating multiple search queries\nconcurrently does not align with the typical human\nreasoning process during search (Hu et al., 2023).\nHumans tend to begin with an initial query, gather\ninformation, and then refine their perspective on\nthe claim, which often leads to the formulation of\nmore effective follow-up queries.\nTo address this gap, we introduce Fact-checking\nwith Iterative Retrieval and VErification (FIRE), an\ninnovative agent-based framework that integrates\nboth the internal knowledge of LLMs and exter-\nnal knowledge sources by unifying the verifica-\n1\narXiv:2411.00784v2  [cs.IR]  12 Feb 2025\n\nAtomic Claim: In 1980, the oldest justice on the United States Supreme Court was Justice William O. Douglas.\n\u00d7 N\nVerification\n< N?\nVerification\nYes\nNo\nWeb Search\nPrevious \nSearch \nResults\nFinal Answer or \nNext Search \nQuery?\nWeb Search\nWeb Search\n< N?\nSearch Query\nFinal \nAnswer\nPrevious \nSearch \nResults\nYes\nNo\nFinal \nVerification\nLabel: True/False\nFactCheckGPT\nFACTOOL\nSAFE\nFigure 1: Comparisons between FIRE and previous frameworks. Previous frameworks typically treat web search\nand claim verification as distinct processes. In contrast, FIRE integrates interactive retrieval and verification.\ntion process and search query generation into a\nsingle step.\nAs illustrated on the right side of\nFigure 1, FIRE employs a mechanism to decide\nwhether to produce the final answer or generate a\nnew search query, continuing the evidence-seeking\nprocess. This decision is based on the model\u2019s con-\nfidence in its judgment. The closest related work\nto us is SAFE (Wei et al., 2024), depicted in the\ncenter of Figure 1. Their method generates web\nsearch queries iteratively and subsequently verifies\nwhether the entire retrieved evidence supports the\nclaim. However, this approach lacks flexibility, as\nit treats evidence retrieval and claim verification\nas distinct processes, requiring a predetermined\nfixed number of searches regardless of the claim\u2019s\ncomplexity. In contrast, our approach integrates ev-\nidence retrieval and claim verification into an itera-\ntive framework, encouraging the language model\nto verify based on its own knowledge and conduct\nsearches only when necessary. Our experiments\ndemonstrate that our method significantly re-\nduces the computational costs of LLMs by an\naverage factor of 7.6, as well as search-related\ncosts by a factor of 16.5, all while maintaining\nfact-checking performance.\nIn summary, our contributions are as follows:\n\u2022 We present FIRE, a simple yet effective inter-\nactive framework for fact-checking. Through\nextensive experiments conducted across mul-\ntiple datasets, we demonstrate that our frame-\nwork significantly reduces the LLM compu-\ntational and search costs, making it a better\noption for large-scale production.\n\u2022 Our ablation studies demonstrate that the\nstep-by-step reasoning process enhances the\nmodel\u2019s confidence in fact-checking, partic-\nularly with GPT-4o-mini. For GPT-4o, we\nobserved a similar trend; however, the ef-\nfect was not as pronounced as that seen with\nGPT-4o-mini.\n\u2022 We conducted an error analysis and identified\nseveral quality issues in the current bench-\nmark datasets, including the presence of un-\ngrounded claims. Additionally, the strict rea-\nsoning capabilities of the LLM may incor-\nrectly classify some debatable claims as non-\nfactual.\n2\nRelated Work\nLLM Factuality\nDespite the remarkable capabil-\nities of LLMs (Brown et al., 2020; OpenAI, 2023;\nZhao et al., 2023), the auto-regressive learning\nobjective does not inherently offer strong guaran-\ntee or enforce the learning of factual accuracy in\nthe training process, making these models produce\ncontent that deviates from real-world facts (Wang\net al., 2024b).\nOn average, there are 5%-10%\nfalse claims in responses of GPT-4 (OpenAI, 2023)\nand LLaMA-2 (Touvron et al., 2023) on world-\n2\n\nknowledge questions (Iqbal et al., 2024). Retrieval-\naugmented generation (Guu et al., 2020) and post-\ngeneration fact-checking are essential for ensur-\ning accurate knowledge dissemination. Retrieving\nhighly relevant information plays a pivotal role\nin both guiding generation as a reference and de-\ntermining verification results in fact-checking sys-\ntems (Wang et al., 2024a).\nThe retriever and verifier are the most resource-\nconsuming components in fact-checking systems,\nin terms of time and cost. Even with the inexpen-\nsive APIs (e.g., Serper at 0.001 USD per request\nand GPT-3.5-turbo for verification), verifying an\natomic claim costs approximately 0.02 USD, mak-\ning extensive verification impractical for general\nusers (Iqbal et al., 2024). This high cost limits the\nability to verify large volumes of LLM responses,\npotentially contributing to the spread of misinfor-\nmation. Our framework aims to minimize the costs\nin these two steps, enabling affordable verification\nfor general users. This allows them to easily verify\nsuspicious or doubtful information, enhancing the\ndissemination of factual information.\nFact Checking with Agents\nThe recent advance-\nments in LLMs have spurred significant research\non LLM-powered agents, which are capable of rea-\nsoning about their environment and making deci-\nsions by either invoking external tools or perform-\ning internal actions (Wang et al., 2024c). These\nagent frameworks typically consist of several com-\nponents, including reasoning, tool usage, memory,\nand multi-agent debate (Masterman et al., 2024),\nmany of which can be seamlessly integrated into\nfact-checking pipelines to enhance the performance\nof traditional fact-checking systems. For exam-\nple, recent works have endowed systems with the\nability to call external tools, such as search en-\ngines (Chern et al., 2023; Wang et al., 2024a; Wei\net al., 2024; Cheng et al., 2024), recognizing that\nmany claims in the field require additional informa-\ntion for verification. During the verification stage,\nSun et al. (2024) proposed a Markov Chain-based\nmulti-agent debate approach to ensure more rigor-\nous verification by enabling collaborative decision-\nmaking among agents based on retrieved evidence.\nOur work differs from previous approaches by com-\nbining the evidence retrieval and verification stages,\nleveraging agents\u2019 reasoning and tool-use capabili-\nties to more closely simulate human cognitive pro-\ncesses in fact-checking.\n3\nFramework\nAssessing the factual accuracy of long-form text\npresents significant challenges (Min et al., 2023).\nTo address this, prior approaches have broken down\nthe text into individual checkworthy claims (Chern\net al., 2023). These sentences, referred to as atomic\nclaims, are fact-checked individually, with their\nfactuality scores aggregated to evaluate the over-\nall factual accuracy of the original text. Previous\nresearch indicates that verifying the factuality of\natomic claims is the most challenging step in this\nprocess (Wang et al., 2024a). Our work there-\nfore focuses on this critical task: determining\nthe factual accuracy of individual atomic claims,\nclassifying each as either True or False.\n3.1\nFIRE\nWe present FIRE, a simple yet effective agent-\nbased framework for interactive claim verification\nthrough web searches. As illustrated in Figure 1,\nFIRE takes an atomic claim as input and outputs a\nbinary label indicating whether the claim is factual\nor non-factual. The framework consists of three key\ncomponents: Final Answer or Next Search Query,\nWeb Search, and Final Verification, each of which\nwe will explain below.\nFinal Answer or Next Search Query\nWe in-\ntroduce a unified method, Final Answer or Next\nSearch Query f(\u00b7), which integrates claim veri-\nfication with search query generation. Given an\natomic claim c, this component decides whether to\nproduce a final answer a or generate an additional\nsearch query q. This decision is guided by both\nan external evidence set E, derived from search re-\nsults, and the internal knowledge k of the language\nmodel, acquired during pre-training. At the outset,\nno evidence has been retrieved, meaning that the\nevidence set E is initially empty. Consequently, the\ndecision relies solely on the internal knowledge k.\nAs shown in Equation 1, we incorporate confidence\nestimation into the reasoning process to determine\nthe next action. If the model\u2019s confidence is suffi-\nciently high, it outputs a final answer a; otherwise,\nit generates an additional query q.\nf(c, E, k) =\n(\na,\nif confident\nq,\nif not confident\n(1)\nThis method offers greater flexibility by eliminat-\ning the need to retrieve a fixed number of evidence\n3\n\nitems before verification, thereby largely reducing\nsearch costs. A detailed description of the prompt\nused for this component is provided in Appendix A.\nWeb search\nWhen the language model deter-\nmines that a web search is necessary and issues\na search query q, we retrieve results using Google\nSearch via the SerpAPI1, following prior work (Wei\net al., 2024). This API returns the retrieved snip-\npets as a single string, which we use as the new\nevidence e. We then append e to the existing evi-\ndence set E to form the updated evidence set E\u2032\nfor the next iteration, as shown in Equation 2.\nE\u2032 = E \u222ae, e = Search(q)\n(2)\nFinal Verification\nDue to the inherent difficulty\nof confidently verifying certain claims, even with\nsupplementary evidence, we impose an upper limit\non the number of retrieval steps. As shown in\nEquation 3, once this limit is reached, the model\nperforms a final verification f\u2032(\u00b7) based on all pre-\nviously retrieved evidence. The detailed prompt for\nthis process is provided in Appendix B.\n(\na = f\u2032(c, E, k),\nn \u2265N\ne = Search(q),\nn < N\n(3)\n3.2\nPrevention of Repetitive Search Queries\nIn our preliminary studies, we identified a recur-\nring issue with sequential search query generation\nusing language models: the tendency of these mod-\nels to generate repetitive queries. This occurs even\nwhen the models are explicitly instructed to gener-\nate queries targeting new, claim-relevant informa-\ntion. As a result, identical queries are repeatedly\nsubmitted to web search tools, leading to inefficient\nuse of search resources. To address this issue, we\ninvestigate following methods for enhancing search\nquery generation and reducing repetition.\nEarly Termination\nThe iterative process is ter-\nminated when consecutive queries or retrieved re-\nsults exhibit a high degree of similarity, indicating\ndiminishing returns.\nDiversity\nPrompt\nWe\nintroduce\nadditional\nprompts to encourage the model to generate more\ndiverse queries when consecutive similar queries\nor search results are detected.\n1https://serpapi.com\n3.3\nPrevention of Verification Overconfidence\nLLMs can exhibit strong calibration abilities across\ndiverse tasks (Kadavath et al., 2022; Geng et al.,\n2024). Consequently, they are aware of their con-\nfidence levels during the claim verification pro-\ncess. However, our preliminary analysis reveals\nthat LLMs often demonstrate excessive strictness\nand unwarranted confidence in certain cases, lead-\ning to errors. Considering this, we explore several\ntechniques to prevent overconfidence in verifica-\ntion:\nAt Least One/Two\nAt Least One requires models\nto retrieve at least one evidence during the verifica-\ntion, which increase the probability of eliminating\noverconfidence. Similarly, we also adopted a more\naggressive approach At Least Two to retrieve a\nsecond evidence to reduce the uncertainty.\nInclusive Prompt\nIn this setting, we prompt\nmodels to be \u201cless strict, open-minded and avoid\nbeing over confident\u201d to encourage models to re-\nflect on their confidence level of answers.\n4\nExperiments Setup\n4.1\nDatasets\nIn our study, we utilized four datasets from prior\nresearch that align with our experimental setup:\nFacTool (Chern et al., 2023), FELM (Chen et al.,\n2023), Factcheck-Bench (Wang et al., 2024a), and\nBingCheck (Li et al., 2024b). FacTool and FELM\nprovide factuality claims across multiple domains.\nFrom these, we selected instances requiring world\nknowledge for verification, which we refer to as\nFacTool-QA and FELM-WK, both annotated with\nbinary labels (True or False). Our selection was\nmotivated by the need to focus on claims that chal-\nlenge models to use external knowledge, a critical\naspect of factual verification.\nFor Factcheck-Bench and BingCheck, we con-\nsolidated the original four-label classification (sup-\nported, partially supported, not supported, refuted)\ninto a binary format by merging supported and par-\ntially supported into True, treating refuted as False,\nand excluding not supported. This binarization\naligns these datasets with the others and simplifies\nevaluation, focusing on clear-cut factuality deci-\nsions. We sampled a subset of BingCheck due to\nits class imbalance (3,581 True claims versus 42\nFalse claims), selecting 100 True claims for our\ntest set. This sampling was essential to create a\nmore balanced and manageable test set, ensuring\n4\n\nDataset\n#True\n#False\nTotal\nFactcheck-Bench\n472\n159\n631\nFacTool-QA\n177\n56\n233\nFELM-WK\n99\n85\n184\nBingCheck\n100\n42\n142\nTable 1: Statistics of the datasets after processing.\nFamily\nName\nGPT\nGPT-4o, GPT-4o-mini, o1-preview,\no1-mini\nClaude\nClaude-3 Haiku, Claude-3 Opus,\nClaude-3.5 Sonnet\nLLaMA\nLLaMA 3.1-Inst 8B\nMistral\nMistral-Inst 7B\nTable 2: Model families and specific model names\nused in this study.\nthat evaluation metrics reflect performance on both\nclasses without being dominated by the majority\nclass. In FELM-WK, we retained un-split claims\nto maintain contextual integrity, which is crucial\nfor accurate verification. Full dataset statistics are\nprovided in Table 1.\nIn our experiments, we first use the Factcheck-\nBench dataset as a development set to optimize\nthe settings for our framework. We then evaluate\nits performance on the remaining three datasets,\ncomparing it with other competitive fact-checking\nsystems.\n4.2\nLanguage Models\nWe investigate several state-of-the-art (SOTA) lan-\nguage models, including proprietary models from\ntwo prominent families: GPT models (OpenAI,\n2024a,b) and Claude models (Anthropic, 2024), as\ndetailed in Table 2. In addition, we assess two open-\nsource models: LLaMA 3.1-Inst 8B (Dubey et al.,\n2024) and Mistral-Inst 7B (Jiang et al., 2023).\n4.3\nCompared Fact-checking Frameworks\nWe select several SOTA fact-checking frameworks\nfor comparison. Additionally, we introduce two\nbaseline models: Random and Always True/False.\nTo further assess the impact of LLM reasoning and\nevidence retrieval in fact-checking, we include two\nablation settings: FIRE (No Reason) and FIRE (No\nSearch).\nFACTOOL\nis adaptable across domains and tasks,\nusing a tool-augmented framework that integrates\nexternal tools like Google Search and Python in-\nterpreters to assess the factuality of content from\nlarge language models. However, this can intro-\nduce complexity and depend on the accuracy of\nthese external tools.\nFACTCHECK-GPT\nexcels in fine-grained fac-\ntuality evaluation through a detailed benchmark\nwith annotations at the claim, sentence, and docu-\nment levels. While resource-intensive, it provides\nvaluable insights into specific stages of factual in-\naccuracies.\nSAFE\nuses a search-augmented approach to ver-\nify long-form content by breaking it down into indi-\nvidual facts and checking them via Google Search.\nThis method is cost-effective compared to human\nannotation but depends on the reliability of search\nengine results, which can vary and introduce bi-\nases.\nRandom\nassigns the predicted label for each\nclaim in the test set randomly, choosing between\nTrue and False with equal probability.\nAlways True/False\nis an approach that always\npredicts a single label \u2013 either True or False \u2013 for\nall claims in the test set.\nFIRE (No Reason)\nutilizes the same framework\nas FIRE; however, it is explicitly instructed not to\narticulate its reasoning process in the output. This\nmodification aims to assess the impact of explicitly\nstating the step-by-step reasoning process on the\nresults.\nFIRE (No Search)\nemploys the same framework\nas FIRE; however, it is not permitted to invoke the\nsearch tool. This configuration is designed to eval-\nuate the model\u2019s ability to perform fact-checking\nwithout retrieving any supporting evidence.\n4.4\nEvaluation Metrics\nIn this work, we investigate the trade-off between\ncomputational cost and fact-checking performance.\nPerformance\nWe evaluate precision, recall, and\nF1 scores for both positive and negative classes.\nComputational Cost\nWe report the financial\ncosts of LLM API calls for proprietary models\nand GPU rental expenses for open-source models,\nalongside an analysis of API costs from search\nengine queries and a breakdown of the total time\nspent on the fact-checking process. The experi-\nments using open-source models were conducted\non an NVIDIA RTX 6000 GPU at an estimated cost\n5\n\nLLM\nLLM+Search\nCost ($)\nLabel = True\nLabel = False\nPrec Recall\nF1\nPrec Recall\nF1\nGPT-4o-mini\n0.19+0.44\n0.91\n0.84\n0.87 0.61\n0.74\n0.67\nGPT-4o\n10.45+1.47\n0.92\n0.79\n0.85 0.56\n0.79\n0.66\no1-preview\n145.66+0.80\n0.91\n0.86\n0.88 0.64\n0.75\n0.69\no1-mini\n20.06+1.13\n0.89\n0.81\n0.85 0.56\n0.71\n0.62\nClaude-3 Haiku\n0.56+0.85\n0.9\n0.81\n0.85 0.56\n0.73\n0.64\nClaude-3 Opus\n48.64+1.43\n0.92\n0.81\n0.86 0.58\n0.79\n0.67\nClaude-3.5 Sonnet\n13.21+1.63\n0.94\n0.79\n0.86 0.58\n0.85\n0.69\nLLaMA 3.1-Inst 8B\n3.95+2.27\n0.89\n0.74\n0.8\n0.48\n0.72\n0.57\nMistral-Inst 7B\n1.84+1.22\n0.85\n0.67\n0.75\n0.4\n0.66\n0.5\nTable 3: Fact-checking performance and cost com-\nparisons between different language models within\nFIRE on Factcheck-Bench.\nof $0.79 per hour, while search queries via SerpAPI\nincurred approximately $0.00105 per search.\n5\nResults\nIn this section, we first present preliminary stud-\nies on Factcheck-Bench (\u00a7 5.1), focusing on three\nkey aspects: language models, prevention of repeti-\ntive search queries, and prevention of verification\noverconfidence. These studies aim to identify the\nmost appropriate configurations for our framework.\nSubsequently, we compare FIRE to other strong\nfact-checking frameworks across three additional\ndatasets (\u00a7 5.2) to evaluate the generalization capa-\nbilities of our approach.\n5.1\nPreliminary studies\nLanguage Models\nWe present a performance\ncomparison of various language models in Table 3.\nOverall, proprietary language models generally out-\nperform open-source models, likely due to their\nlarger size and more sophisticated training in rea-\nsoning and tool utilization. Among the propri-\netary models, the latest and most advanced of-\nferings from different organizations\u2014specifically\no1-preview from OpenAI and Claude-3.5 Sonnet\nfrom Anthropic\u2014exhibit the best performance. Al-\nthough the more economical model, GPT-4o-mini,\nperforms slightly worse than the top-performing\no1-preview, it offers a cost savings of 766 times.\nThis suggests that for fact-checking tasks, the\nmost advanced models may not be necessary;\nGPT-4o-mini can serve as a sufficiently capable al-\nternative at a significantly lower cost. We will con-\ntinue our preliminary studies using GPT-4o-mini.\nPrevention of Repetitive Search Queries\nWe\nconducted an experimental analysis to evaluate\nthe impact of Early Termination and Diversity\nPrompt on mitigating the generation of repeti-\nWindow\nSize\nDiversity\nPrompt\nLLM+Search\nCost ($)\nLabel = True\nLabel = False\nPrec Recall\nF1\nPrec Recall\nF1\n2\n\u2717\n0.17+0.29\n0.92\n0.83\n0.87 0.61\n0.77\n0.68\n\u2713\n0.16+0.29\n0.91\n0.81\n0.86 0.57\n0.76\n0.65\n3\n\u2717\n0.17+0.36\n0.91\n0.82\n0.87 0.60\n0.77\n0.67\n\u2713\n0.18+0.36\n0.91\n0.82\n0.86 0.59\n0.76\n0.66\n4\n\u2717\n0.18+0.39\n0.91\n0.81\n0.86 0.57\n0.76\n0.65\n\u2713\n0.18+0.39\n0.91\n0.82\n0.86 0.59\n0.76\n0.66\nDefault\n-\n0.19+0.44\n0.91\n0.84\n0.87 0.61\n0.74\n0.67\nTable 4: FIRE performance across various window\nsizes, with and without the use of prompts for gener-\nating diverse queries on Factcheck-Bench.\ntive search queries. To assess query similarity,\nwe employed Sentence-BERT (all-MiniLM-L6-\nv2; Reimers and Gurevych (2019)) with a simi-\nlarity threshold of 0.9, as established by Shashavali\net al. (2019). Table 4 presents experimental re-\nsults, where window size refers to the predefined\nnumber of consecutive similar queries or retrieval\nresults. Once this threshold is reached, early termi-\nnation is triggered to prevent further query genera-\ntion and retrieval. If the model generates queries or\nretrieves results exhibiting high similarity within\nthis window, the system also activates an early stop-\nping mechanism. The results indicate that optimiz-\ning the similarity window size effectively reduces\nsearch costs without compromising the model\u2019s\nperformance. However, our findings suggest that\nthe diversity prompt does not enhance performance.\nIn our optimal configuration, we selected a window\nsize of 2 without utilizing the diversity prompt.\nPrevention of Verification Overconfidence\nWe\npresent the performance and cost of various over-\nconfidence prevention approaches for verification\non Factcheck-Bench in Table 5.\nInterestingly,\nthe At Least One/Two settings, which aggres-\nsively retrieve additional evidence, result in higher\nsearch costs without improving fact-checking per-\nformance compared to the Default setting, where\nno explicit constraints are placed on web search.\nThis supports our hypothesis that most atomic\nclaims are relatively straightforward and do not\nrequire extensive external web searches for veri-\nfication. In fact, introducing additional searches\nmay introduce noise, negatively impacting perfor-\nmance. The Inclusive setting encourages models\nto be more flexible and open to alternative inter-\npretations of evidence, which reduces the need for\nqueries but also leads to lower overall performance.\nBased on these observations, we maintain the De-\nfault setting, leveraging the language model\u2019s rea-\n6\n\nApproach\nLLM+Search\nCost ($)\nLabel = True\nLabel = False\nPrec Recall\nF1\nPrec Recall\nF1\nAt Least One\n0.21+0.83\n0.92\n0.81\n0.86 0.58\n0.78\n0.67\nAt Least Two\n0.22+0.87\n0.91\n0.79\n0.84 0.55\n0.77\n0.64\nInclusive\n0.20+0.42\n0.91\n0.81\n0.86 0.58\n0.77\n0.66\nDefault\n0.19+0.44\n0.91\n0.84\n0.87 0.61\n0.74\n0.67\nTable 5: FIRE performance using different veri-\nfication overconfidence prevention approaches on\nFactcheck-Bench.\nsoning capabilities without imposing additional\nsearch constraints.\n5.2\nComparisons to Other Frameworks\nWe present a performance comparison of our frame-\nwork against other frameworks in Table 6 and a\ncost analysis in Table 7. As shown, all frame-\nworks exhibit similar performance, with a small\ngap of approximately 0.2. Our framework, us-\ning GPT-4o, performs slightly better, achieving\nsuperior results on 7 out of 18 metrics, followed\nclosely by SAFE with GPT-4o at 6 metrics. This\nsuggests that all frameworks can effectively per-\nform fact-checking for most claims, although they\nmay encounter difficulties with challenging ex-\namples, which we analyze further in \u00a7 6.\nRe-\ngarding the necessity of evidence retrieval in fact-\nchecking, we observe a relatively larger perfor-\nmance drop in FACTOOL when evidence search is\nomitted, compared to a smaller drop in FELM-WK\nand BingCheck. This suggests that FacTool-QA\ncomprises more rare knowledge than GPT-4o-mini,\nwhereas FELM-WK and BingCheck may rely pre-\ndominantly on common knowledge, for which ev-\nidence retrieval is less impactful. Overall, both\nGPT-4o and GPT-4o-mini perform reasonably well\non popular public datasets, highlighting the need\nfor datasets that incorporate more complex claims.\nIn terms of model size, GPT-4o generally outper-\nforms GPT-4o-mini across most frameworks, in-\ndicating that larger models are more effective in\ndetecting misinformation. However, the perfor-\nmance improvement is limited, and the associated\ncosts result in an average increase of 16.7 times in\nLLM expenses and a three-fold increase in search\ncosts when using FIRE. Therefore, we argue that\ncheaper models, such as GPT-4o-mini, are a viable\noption for performing fact-checking tasks. Fur-\nthermore, when considering all frameworks with\nGPT-4o-mini, FIRE achieves additional cost sav-\nings, reducing LLM expenses by 7.6 times and\nsearch costs by 16.5 times compared to other frame-\n0\n1\n2\n3\nNumber of Google Searches\n0\n20\n40\n60\n80\n100\n120\n140\nTotal Count of Searches\n107\n33\n2\n0\n0\n137\n5\n0\n25\n83\n25\n9\n1\n118\n17\n6\nGPT-4o-mini\nGPT-4o-mini (No Reason)\nGPT-4o\nGPT-4o (No Reason)\nFigure 2: The effect of reasoning on the number\nof searches using GPT-4o and GPT-4o-mini within\nFIRE on BingCheck. The shaded area indicates the\nnumber of misclassified cases. The x-axis shows the\nnumber of web searches, while the y-axis denotes the\nnumber of instances.\nworks. Thus, we contend that FIRE, when paired\nwith GPT-4o-mini, offers a compelling solution for\nthe large-scale deployment of fact-checking sys-\ntems.\nFigure 2 illustrates the impact of reasoning on\nthe number of web searches conducted by GPT-4o\nand GPT-4o-mini tested on BingCheck. Notably,\nGPT-4o-mini demonstrates a high level of confi-\ndence in making verifications when it is allowed to\narticulate its reasoning process, resulting in the\nmajority of judgments being made without any\nsearches. Conversely, when not permitted to ex-\npress its reasoning, there is a significant decrease\nin the number of instances with zero searches; most\ncases now involve at least one search, indicating\na marked reduction in GPT-4o-mini\u2019s confidence\nin its judgments. This observation aligns with pre-\nvious findings that the presence of CoT reasoning\ncorrelates with increased confidence in the model\u2019s\nanswers (Wang and Zhou, 2024). While GPT-4o\nalso shows a decline in confidence when it is not\nallowed to search, the decrease is less pronounced\nthan that observed in GPT-4o-mini.\nBy combining the performance and cost results\npresented in Table 6 and Table 7, we find that, in\nthe absence of a reasoning process, the costs asso-\nciated with LLMs can be reduced through fewer\ncompletion tokens. However, this reduction leads\nto increased search costs, resulting in overall per-\nformance that is inferior to scenarios in which the\nmodels are permitted to engage in step-by-step rea-\nsoning. Furthermore, the step-by-step reasoning\napproach facilitates more effective error analysis.\n7\n\nFramework\nLLM\nFacTool-QA\nFELM-WK\nBingCheck\nLabel = True\nLabel = False\nLabel = True\nLabel = False\nLabel = True\nLabel = False\nPrec Recall\nF1\nPrec Recall\nF1\nPrec Recall\nF1\nPrec Recall\nF1\nPrec Recall\nF1\nPrec Recall\nF1\nRandom\n-\n0.81\n0.47\n0.59 0.28\n0.64\n0.39 0.75\n0.49\n0.59 0.30\n0.57\n0.39 0.77\n0.67\n0.72 0.40\n0.52\n0.45\nAlways True\n-\n0.76\n1.0\n0.86\n0\n0\n0\n0.72\n1.0\n0.84\n0\n0\n0\n0.70\n1.0\n0.83\n0\n0\n0\nAlways False\n-\n0\n0\n0\n0.24\n1.0\n0.39\n0\n0\n0\n0.28\n1.0\n0.44\n0\n0\n0\n0.30\n1.0\n0.46\nFACTOOL\nGPT-4o\n0.88\n0.81\n0.84 0.52\n0.66\n0.58 0.69\n0.53\n0.60 0.57\n0.73\n0.64 0.86\n0.57\n0.68 0.43\n0.79\n0.56\nGPT-4o-mini 0.92\n0.68\n0.78 0.45\n0.82\n0.58 0.67\n0.37\n0.48 0.51\n0.78\n0.62 0.92\n0.55\n0.69 0.45\n0.88\n0.60\nFACTCHECK-GPT\nGPT-4o\n0.90\n0.79\n0.84 0.52\n0.71\n0.60 0.67\n0.68\n0.67 0.61\n0.61\n0.61 0.85\n0.70\n0.77 0.50\n0.71\n0.59\nGPT-4o-mini 0.85\n0.80\n0.82 0.47\n0.56\n0.51 0.61\n0.50\n0.55 0.51\n0.62\n0.56 0.88\n0.78\n0.83 0.60\n0.76\n0.67\nSAFE\nGPT-4o\n0.92\n0.88\n0.90 0.66\n0.77\n0.71 0.70\n0.80\n0.75 0.72\n0.60\n0.65 0.84\n0.90\n0.87 0.71\n0.60\n0.65\nGPT-4o-mini 0.92\n0.82\n0.87 0.58\n0.79\n0.67 0.61\n0.76\n0.68 0.61\n0.44\n0.51 0.86\n0.81\n0.84 0.60\n0.69\n0.64\nFIRE\nGPT-4o\n0.92\n0.88\n0.90 0.65\n0.71\n0.68 0.70\n0.86\n0.77 0.77\n0.54\n0.63 0.86\n0.88\n0.87 0.70\n0.67\n0.68\nGPT-4o-mini 0.87\n0.88\n0.87 0.60\n0.59\n0.59 0.63\n0.82\n0.71 0.67\n0.44\n0.53 0.87\n0.91\n0.88 0.74\n0.67\n0.70\nFIRE (No Reason)\nGPT-4o\n0.88\n0.86\n0.87 0.60\n0.64\n0.62 0.70\n0.85\n0.77 0.77\n0.58\n0.66 0.85\n0.89\n0.87 0.70\n0.62\n0.66\nGPT-4o-mini 0.87\n0.84\n0.86 0.55\n0.61\n0.58 0.65\n0.84\n0.73 0.71\n0.47\n0.57 0.84\n0.87\n0.85 0.66\n0.6\n0.62\nFIRE (No Search)\nGPT-4o\n0.86\n0.87\n0.88 0.61\n0.54\n0.57 0.69\n0.86\n0.77 0.77\n0.55\n0.65 0.86\n0.91\n0.88 0.79\n0.64\n0.71\nGPT-4o-mini 0.84\n0.84\n0.84 0.49\n0.48\n0.49 0.61\n0.86\n0.72\n0.7\n0.36\n0.48 0.83\n0.9\n0.87 0.71\n0.57\n0.63\nTable 6: Performance comparisons between different frameworks across multiple datasets.\nFramework\nLLM\nLLM Search Time\nFACTOOL\nGPT-4o\n24.76\n3.67\n2.92\nGPT-4o-mini\n1.49\n3.67\n2.34\nFACTCHECK-GPT\nGPT-4o\n21.41\n-\n4.25\nGPT-4o-mini\n1.28\n-\n4.09\nSAFE\nGPT-4o\n6.34\n2.93\n4.62\nGPT-4o-mini\n0.43\n2.93\n4.25\nFIRE\nGPT-4o\n3.35\n0.60\n1.31\nGPT-4o-mini\n0.14\n0.20\n1.25\nFIRE (No Reason)\nGPT-4o\n1.65\n0.68\n0.57\nGPT-4o-mini\n0.07\n0.59\n0.54\nFIRE (No Search)\nGPT-4o\n1.70\n-\n1.03\nGPT-4o-mini\n0.11\n-\n1.34\nTable 7: LLM/Search cost (USD) and time (hrs)\nfor evaluating the total 559 atomic claims in\nFacTool-QA, FELM-WK, and BingCheck. We use\nSerperAPI for FACTOOL, SAFE and FIRE for search,\nwhile FACTCHECK-GPT has its own implemented\nscrapping technique.\n6\nError Analysis\nTo identify weaknesses in our fact-checking system,\nwe manually examine failed cases of three datasets:\nFELM-WK, FacTool-QA, and BingCheck, analyz-\ning whether the majority of failures is attributed\nto inadequate retrieved evidence or to flaws in the\nLLM verification process, despite the availability\nof reliable evidence.\nWe summarized errors into four major issues\nand nine error types. Among the total number of\n135 failed claims, there are 44 cases falling into\nchallenges of (I) inaccurate identification of check-\nworthy claims and false gold labels in the original\ndatasets, 50 claims are due to (II) inaccurate or in-\nsufficient knowledge applied to verification, either\ninternally extracted from LLM parameters or exter-\nnally collected from web pages. The rest 26 and\n15 cases result from LLM reasoning ability and\ndebatable opinions over some topics, respectively,\nas shown in Table 8.\nThe major issue lies in collecting sufficient evi-\ndence, especially for long claims containing many\naspects to verify.\nThis can be approached by\ndecomposing \u201catomic claims\u201d from the original\ndataset into the real granularity of \u201catomic\u201d, each\ncontaining only 1-3 pieces of information. The\nsecond problem focus on the quality of benchmark-\ning datasets, particularly FELM-WK that includes\nmany ungrounded claims and labels (Li et al.,\n2024a), which may lead to ineffective comparisons\nbetween fact-checking systems. Interestingly, be-\nyond incorrect reasoning, overly-strict reasoning\nby exact matching between the claim and collected\nevidence can also lead to verification errors. For\nexample, LLMs label a claim as false when the\nclaim states FUN Word-Cross Puzzle while evi-\ndence mentions Word-Cross Puzzle. Additionally,\nsome claims can be viewed as true from one per-\nspective but false from another, as seen in debates\nover the origins of fortune cookies, where the truth\nof related claims is debatable.\nConsidering above, to further advance the field\nof fact-checking, we highlight the need for im-\nproved benchmarking datasets, a stronger focus\non verifying fine-grained claims, and strategies\nto guide LLMs in performing verification under\nmore flexible reasoning conditions, such as seman-\ntic alignment, rather than relying exclusively on\nexact matches.\n8\n\nMajor Issue\nError Type Description\nFELM FacTool BingCheck #Total\nI. Dataset Issue\n1. Not a claim, e.g. a claim only has a name Elvis Presley.\n12\n1\n0\n13\n2. Unclear, ambiguous or subjective claim e.g. there is no record of how many sons he had.\n11\n7\n2\n20\n3. False gold labels, i.e., the original annotated label might be wrong. For example, claim\nchoosing organic and local foods that are in season can reduce emissions from transporting food\nfrom far away is labeled as false\n10\n0\n1\n11\nII. Knowledge Issue\n4. Complicated science domain expert knowledge is needed to judge, like astronomy.\n3\n0\n1\n4\n5. Inaccurate parametric knowledge. LLM-based verifiers make wrong verification due to the\nincorrect parametric knowledge stored in LLMs.\n3\n6\n7\n16\n6. Insufficient or inaccurate externally collected knowledge (evidence), involving three\nscenarios: (i) no external evidence, model makes wrong reasoning by itself; (ii) collected\nevidence is insufficient to cover all aspects mentioned by a long claim; (iii) collected evidence is\ninaccurate (e.g., evidence contain statement More than 430 species of mammal are found in the\nAmazon when the correct number is 427).\n9\n15\n6\n30\nIII. LLM Reasoning\n7. Incorrect reasoning, e.g., the claim mentioned A while the model dismissed in the reasoning\nprocess, or the claim did not mention A while the model hallucinated A\n6\n8\n4\n18\n8. Strict reasoning includes two situations: (i) strictly depending on the collect evidence to\nmake decision leads to wrong verification, while if it combines commonsense and collected\nevidence to analyze, it can verify correctly; (2) strict reasoning based on parametric knowledge.\nE.g. regarding Word-Cross Puzzle and FUN Word-Cross Puzzle are different.\n5\n0\n3\n8\nIV. Debatable Opinion\n9. Debatable Opinions on controversial topics, e.g. actual origins are debated for claim Fortune\ncookies made their way to San Francisco in the late 1800s and early 1900s through Japanese\nimmigrants.\n7\n8\n0\n15\nTotal\n66\n45\n24\n135\nTable 8: Datasets Error distribution, grouped into nine fine-grained types under four major issues.\n7\nConclusions and Future Work\nConventional fact-checking systems typically sep-\narate the steps of evidence retrieval and claim ver-\nification, leading to suboptimal utilization of the\nverification models\u2019 internal knowledge. To ad-\ndress this, we propose FIRE, a novel framework\nthat integrates evidence retrieval and claim verifi-\ncation in an iterative process. FIRE enables LLMs\nto leverage their internal knowledge for judgment\nand only rely on external evidence retrieval when\nuncertain. Our experiments on multiple datasets\ndemonstrate that FIRE not only slightly improves\naccuracy but also reduces LLM computation costs\nby an average of 7.6 times and search costs by 16.5\ntimes, making it highly efficient for production use.\nAdditionally, we performed a detailed error analy-\nsis, which revealed issues with the benchmarking\ndatasets quality. These findings highlight the need\nfor further research into edge cases, rather than\nrelying solely on automatic metrics for evaluation.\nWe identify several promising directions for fu-\nture work, which include: (1) Integrating memory\nbanks to store verification results, allowing the sys-\ntem to reuse previous results instead of repeatedly\nexecuting the entire process; (2) Expanding the\nsystem to support additional modalities, such as\ncode and images; and (3) Revisiting existing pub-\nlic fact-checking datasets, incorporating personal\nopinions when addressing ambiguous cases, and\nadding claims that require rarer and more complex\nknowledge, where evidence retrieval is essential.\nLimitations\nWe acknowledge several limitations in this work\nthat we plan to address in future research. First, to\nmaintain the efficiency of our framework, we im-\nplement the \u201cFinal Answer or Next Search Query\u201d\nmechanism in a compact manner, allowing it to\nretrieve evidence, assess confidence in knowledge,\nand verify the final answer within a single step.\nIdeally, this process could be separated to include\na standalone confidence estimation step, which\nwould enhance both flexibility and interpretability.\nWe leave this exploration to future work. Second,\nto ensure a fair comparison across multiple fact-\nchecking datasets, our system adopts a binary la-\nbeling scheme (\u201cTrue\u201d or \u201cFalse\u201d) and standardizes\nlabels across datasets. However, this approach may\nnot fully capture the complexity of factual labels\nin real-world settings. We intend to incorporate\nfine-grained labeling schemes in future research.\nFinally, in this study, we rely on SerpAPI with its\ndefault settings. While we did not investigate in\ndetail how evidence is retrieved, we believe future\nwork could explore this aspect further to optimize\nthe selection of the most relevant evidence for a\ngiven claim.\nEthical Statement and Broad Impact\nData License\nA primary ethical consideration is\nthe data license. We reused pre-existing dataset,\nFactBench, FACTOOL, FELM-WK, BingCheck,\nwhich have been publicly released and approved\nfor research purposes. We adhere to the intended\n9\n\nusage of all these dataset licenses.\nEthical Statement\nWe acknowledge that our sys-\ntem relies on LLMs, which can sometimes produce\nbiased or incorrect judgments due to the data used\nin their pre-training or biases present in external\nsources. Additionally, there is the risk of over-\nreliance on the system for making critical factual\njudgments without human oversight. To mitigate\nthese risks, we strongly encourage human review-\ners to be involved in decision-making, especially\nin high-stakes domains such as legal, political, or\nmedical contexts.\nBroad Impact\nFIRE has the potential to advance\nthe field of automated fact-checking by enhancing\nits efficiency and accessibility. Its capability to\niteratively retrieve evidence while minimizing com-\nputational costs will empower a broader range of\nusers\u2014including journalists, researchers, and the\ngeneral public\u2014to verify factual information with\ngreater ease. Furthermore, FIRE can be applied\nto large-scale implementations, such as integration\ninto search engines and social media platforms,\nthereby contributing to efforts to combat the spread\nof misinformation.\nAcknowledgments\nWe thank our reviewers for their valuable reviews\nand feedback, which significantly contributed to\nthe improvement of our paper.\nReferences\nMubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo,\nOana Cocarascu, Elena Simperl, and Andreas Vla-\nchos. 2023. Multimodal automated fact-checking: A\nsurvey. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pages 5430\u20135448,\nSingapore. Association for Computational Linguis-\ntics.\nAnthropic. 2024. Introducing the next generation of\nclaude.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern,\nSiyang Gao, Pengfei Liu, and Junxian He. 2023.\nFELM: benchmarking factuality evaluation of large\nlanguage models. In Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neu-\nral Information Processing Systems 2023, NeurIPS\n2023, New Orleans, LA, USA, December 10 - 16,\n2023.\nXiaoxue Cheng, Junyi Li, Xin Zhao, Hongzhi Zhang,\nFuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong\nWen. 2024. Small agent can also rock! empowering\nsmall language models as hallucination detector. In\nProceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2024, Miami, FL, USA, November 12-16, 2024, pages\n14600\u201314615. Association for Computational Lin-\nguistics.\nI-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan,\nKehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, and Pengfei Liu. 2023. Factool: Factual-\nity detection in generative AI - A tool augmented\nframework for multi-task and multi-domain scenar-\nios. ArXiv preprint, abs/2307.13528.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien\nRodriguez, Austen Gregerson, Ava Spataru, Baptiste\nRozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern,\nand et al. 2024. The llama 3 herd of models. ArXiv\npreprint, abs/2407.21783.\nJiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl,\nPreslav Nakov, and Iryna Gurevych. 2024. A sur-\nvey of confidence estimation and calibration in large\nlanguage models. In Proceedings of the 2024 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies (Volume 1: Long Papers), pages\n6577\u20136595, Mexico City, Mexico. Association for\nComputational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nZiniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang,\nYizhou Sun, David Ross, Cordelia Schmid, and\nAlireza Fathi. 2023. AVIS: autonomous visual in-\nformation seeking with large language model agent.\nIn Advances in Neural Information Processing Sys-\ntems 36: Annual Conference on Neural Information\nProcessing Systems 2023, NeurIPS 2023, New Or-\nleans, LA, USA, December 10 - 16, 2023.\nHasan Iqbal, Yuxia Wang, Minghan Wang, Georgi\nGeorgiev, Jiahui Geng, Iryna Gurevych, and Preslav\n10\n\nNakov. 2024. Openfactcheck: A unified framework\nfor factuality evaluation of llms.\nArXiv preprint,\nabs/2408.11832.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nth\u00e9e Lacroix, and William El Sayed. 2023. Mistral\n7b. ArXiv preprint, abs/2310.06825.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jack-\nson Kernion, Shauna Kravec, Liane Lovitt, Ka-\nmal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\nKaplan. 2022. Language models (mostly) know what\nthey know. ArXiv preprint, abs/2207.05221.\nHaonan Li, Xudong Han, Hao Wang, Yuxia Wang,\nMinghan Wang, Rui Xing, Yilin Geng, Zenan Zhai,\nPreslav Nakov, and Timothy Baldwin. 2024a. Loki:\nAn open-source tool for fact verification.\nMiaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao,\nand Zhu Zhang. 2024b. Self-checker: Plug-and-play\nmodules for fact-checking with large language mod-\nels. In Findings of the Association for Computational\nLinguistics: NAACL 2024, pages 163\u2013181, Mexico\nCity, Mexico. Association for Computational Lin-\nguistics.\nTula Masterman, Sandi Besen, Mason Sawtell, and Alex\nChao. 2024. The landscape of emerging AI agent\narchitectures for reasoning, planning, and tool calling:\nA survey. ArXiv preprint, abs/2404.11584.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12076\u201312100, Singa-\npore. Association for Computational Linguistics.\nOpenAI. 2023. GPT-4 technical report. ArXiv preprint,\nabs/2303.08774.\nOpenAI. 2024a. Hello gpt-4o.\nOpenAI. 2024b. Introducing openai o1-preview.\nNils Reimers and Iryna Gurevych. 2019.\nSentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982\u20133992, Hong Kong, China. Association for Com-\nputational Linguistics.\nD. Shashavali, V. Vishwjeet, Rahul Kumar, Gaurav\nMathur, Nikhil Nihal, Siddhartha Mukherjee, and\nSuresh Venkanagouda Patil. 2019. Sentence similar-\nity techniques for short vs variable length text using\nword embeddings. Computaci\u00f3n y Sistemas, 23(3).\nXiaoxi Sun, Jinpeng Li, Yan Zhong, Dongyan Zhao,\nand Rui Yan. 2024. Towards detecting llms hallu-\ncination via markov chain-based multi-agent debate\nframework. ArXiv preprint, abs/2406.03075.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. ArXiv preprint, abs/2307.09288.\nXuezhi Wang and Denny Zhou. 2024. Chain-of-thought\nreasoning without prompting.\nArXiv preprint,\nabs/2402.10200.\nYuxia Wang, Revanth Gangi Reddy, Zain Muhammad\nMujahid, Arnav Arora, Aleksandr Rubashevskii, Ji-\nahui Geng, Osama Mohammed Afzal, Liangming\nPan, Nadav Borenstein, Aditya Pillai, Isabelle Au-\ngenstein, Iryna Gurevych, and Preslav Nakov. 2024a.\nFactcheck-bench: Fine-grained evaluation bench-\nmark for automatic fact-checkers. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2024, Miami, Florida, USA, November 12-16, 2024,\npages 14199\u201314230. Association for Computational\nLinguistics.\nYuxia Wang, Minghan Wang, Muhammad Arslan Man-\nzoor, Fei Liu, Georgi N. Georgiev, Rocktim Jyoti Das,\nand Preslav Nakov. 2024b. Factuality of large lan-\nguage models: A survey. In Proceedings of the 2024\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2024, Miami, FL, USA,\nNovember 12-16, 2024, pages 19519\u201319529. Associ-\nation for Computational Linguistics.\n11\n\nZhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried,\nand Graham Neubig. 2024c. What are tools anyway?\nA survey from the language model perspective. ArXiv\npreprint, abs/2403.15452.\nJerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu,\nNathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu,\nDa Huang, Cosmo Du, and Quoc V. Le. 2024. Long-\nform factuality in large language models.\nArXiv\npreprint, abs/2403.18802.\nZhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023a.\nThe next chapter: A study of large language models\nin storytelling.\nIn Proceedings of the 16th Inter-\nnational Natural Language Generation Conference,\npages 323\u2013351, Prague, Czechia. Association for\nComputational Linguistics.\nZhuohan Xie, Miao Li, Trevor Cohn, and Jey Lau.\n2023b. DeltaScore: Fine-grained story evaluation\nwith perturbations. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n5317\u20135331, Singapore. Association for Computa-\ntional Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.\nA survey of large language models. ArXiv preprint,\nabs/2303.18223.\nA\nPrompts for Verification\nDefault prompt\nWe use the following prompt to\nguide the language model in verifying the atomic\nclaim, determining whether to provide a final judg-\nment or issue an additional Google search query\nbased on the current status. The prompt will output\nreason or explanation for the verification process.\n_FINAL_ANSWER_OR_NEXT_SEARCH_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your conclusion, think\nthrough the process step-by-step. Include\na summary of the key points from the\nKNOWLEDGE as part of your reasoning.\n4. If the KNOWLEDGE allows you to confidently\nmake a decision, output the final answer\nas a JSON object in the following format:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n5. If the KNOWLEDGE is insufficient to make a\njudgment, issue ONE Google Search query\nthat could provide additional evidence.\nOutput the search query in JSON format, as\nfollows:\n{{\n\"search_query\": \"Your Google search query\nhere\"\n}}\n6. The query should aim to obtain new\ninformation not already present in the\nKNOWLEDGE, specifically helpful for\nverifying the STATEMENT\u2019s accuracy.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nNo Reason prompt\nTo improve efficiency, we\nopted for this setting to output only the label.\n_FINAL_ANSWER_OR_NEXT_SEARCH_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your conclusion, think\nthrough the process step-by-step. Include\na summary of the key points from the\nKNOWLEDGE as part of your reasoning.\n4. If the KNOWLEDGE allows you to confidently\nmake a decision, output the final answer\nas a JSON object in the following format:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n12\n\n5. If the KNOWLEDGE is insufficient to make a\njudgment, issue ONE Google Search query\nthat could provide additional evidence.\nOutput the search query in JSON format, as\nfollows:\n{{\n\"search_query\": \"Your Google search query\nhere\"\n}}\n6. The query should aim to obtain new\ninformation not already present in the\nKNOWLEDGE, specifically helpful for\nverifying the STATEMENT\u2019s accuracy.\n7. Do not provide any additional information\nor reasoning in the output. Only output\nthe JSON object.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nAt Least One prompt\nAt Least One prompt re-\nquires models to retrieve at least one evidence dur-\ning the verification.\n_FINAL_ANSWER_OR_NEXT_SEARCH_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your conclusion, think\nthrough the process step-by-step. Include\na summary of the key points from the\nKNOWLEDGE as part of your reasoning.\n4. If the KNOWLEDGE allows you to confidently\nmake a decision, output the final answer\nas a JSON object in the following format:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n5. If the KNOWLEDGE is insufficient to make a\njudgment, issue ONE Google Search query\nthat could provide additional evidence.\nOutput the search query in JSON format, as\nfollows:\n{{\n\"search_query\": \"Your Google search query\nhere\"\n}}\n6. The query should aim to obtain new\ninformation not already present in the\nKNOWLEDGE, specifically helpful for\nverifying the STATEMENT\u2019s accuracy.\n7. If the KNOWLEDGE is empty, please issue ONE\nGoogle Search query immediately.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nAt Least Two prompt\nAt Least Two prompt is\na more aggressive approach to retrieve minimum\ntwo evidence before verification.\n_FINAL_ANSWER_OR_NEXT_SEARCH_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your conclusion, think\nthrough the process step-by-step. Include\na summary of the key points from the\nKNOWLEDGE as part of your reasoning.\n4. If the KNOWLEDGE allows you to confidently\nmake a decision, output the final answer\nas a JSON object in the following format:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n5. If the KNOWLEDGE is insufficient to make a\njudgment, issue ONE Google Search query\nthat could provide additional evidence.\nOutput the search query in JSON format, as\nfollows:\n{{\n\"search_query\": \"Your Google search query\nhere\"\n}}\n6. The query should aim to obtain new\ninformation not already present in the\nKNOWLEDGE, specifically helpful for\nverifying the STATEMENT\u2019s accuracy.\n7. If the KNOWLEDGE is empty or there is only\nONE evidence in the KNOWLEDGE, please\nissue ONE Google Search query immediately.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nInclusive\nIn this setting, we prompt models to\nbe \u201cless strict, open-minded and avoid being over\nconfident\u201d to encourage models to reflect on their\nconfidence level of answers.\n_FINAL_ANSWER_OR_NEXT_SEARCH_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your conclusion, think\nthrough the process step-by-step. Include\na summary of the key points from the\nKNOWLEDGE as part of your reasoning.\n4. If the KNOWLEDGE allows you to confidently\nmake a decision, output the final answer\nas a JSON object in the following format:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n13\n\n5. If the KNOWLEDGE is insufficient to make a\njudgment, issue ONE Google Search query\nthat could provide additional evidence.\nOutput the search query in JSON format, as\nfollows:\n{{\n\"search_query\": \"Your Google search query\nhere\"\n}}\n6. The query should aim to obtain new\ninformation not already present in the\nKNOWLEDGE, specifically helpful for\nverifying the STATEMENT\u2019s accuracy.\n7. Please be more open-minded and less strict\nin your evaluation. Avoid being overly\nconfident, and consider the possibility of\nalternative interpretations or\nuncertainties in the evidence.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nB\nPrompt for Final Verification\nUpon reaching the maximum number of steps, we\nissue the following prompt to compel the language\nmodel to make a final judgment based on the accu-\nmulated information.\n_MUST_HAVE_FINAL_ANSWER_FORMAT = f\"\"\"\nInstructions:\n1. You are provided with a STATEMENT and\nrelevant KNOWLEDGE points.\n2. Based on the KNOWLEDGE, assess the factual\naccuracy of the STATEMENT.\n3. Before presenting your final answer, think\nstep-by-step and show your reasoning.\nInclude a summary of the key points from\nthe KNOWLEDGE as part of your reasoning.\n4. Your final answer should be either \"{\n_Factual_LABEL}\" or \"{_Non_Factual_LABEL\n}\".\n5. Format your final answer as a JSON object\nin the following structure:\n{{\n\"final_answer\": \"{_Factual_LABEL}\" or \"{\n_Non_Factual_LABEL}\"\n}}\n6. Do not include any other information or\nreasoning in the output. Only provide the\nJSON object.\nKNOWLEDGE:\n{_KNOWLEDGE_PLACEHOLDER}\nSTATEMENT:\n{_STATEMENT_PLACEHOLDER}\n\"\"\"\nC\nEffect of Reasoning\nWe additionally include figures to illustrate\nthe effect of reasoning on two other datasets:\n0\n1\n2\n3\nNumber of Google Searches\n0\n50\n100\n150\n200\nTotal Count of Searches\n158\n69\n4\n2\n4\n224\n4\n1\n24\n190\n18\n1\n4\n214\n8\n7\nGPT-4o-mini\nGPT-4o-mini (No Reason)\nGPT-4o\nGPT-4o (No Reason)\nFigure 3: The effect of reasoning on the number\nof searches using GPT-4o and GPT-4o-mini within\nFIRE on FacTool-QA.\n0\n1\n2\n3\nNumber of Google Searches\n0\n20\n40\n60\n80\n100\n120\n140\nTotal Count of Searches\n120\n59\n5\n0\n24\n150\n7\n3\n48\n104\n25\n7\n14\n136\n25\n9\nGPT-4o-mini\nGPT-4o-mini (No Reason)\nGPT-4o\nGPT-4o (No Reason)\nFigure 4: The effect of reasoning on the number\nof searches using GPT-4o and GPT-4o-mini within\nFIRE on FELM-WK.\nFacTool-QA (Figure 3) and FELM-WK (Figure 4),\nsupplementing Figure 2. These figures demonstrate\nthat both GPT-4o and GPT-4o-mini are influenced\nby explicitly stating their reasoning process, with\nGPT-4o-mini showing a consistent impact across\nall datasets, not just BingCheck.\nFurthermore,\nwhen comparing these datasets, we observe that\nthe models appear most confident on FELM-WK\ncompared to the other two datasets. As a result,\neven in the absence of explicit reasoning, they do\nnot perform any searches to verify the claims.\n14\n",
    "2405.06258v2.pdf": "Automatic Generation of Model and Data Cards:\nA Step Towards Responsible AI\nJiarui Liu\nCMU\njiaruil5@andrew.cmu.edu\nWenkai Li\nCMU\nwenkail@andrew.cmu.edu\nZhijing Jin\nMPI & ETH Z\u00fcrich\njinzhi@ethz.ch\nMona Diab\nCMU\nmdiab@andrew.cmu.edu\nAbstract\nIn an era of model and data proliferation in\nmachine learning/AI especially marked by the\nrapid advancement of open-sourced technolo-\ngies, there arises a critical need for standard-\nized consistent documentation. Our work ad-\ndresses the information incompleteness in cur-\nrent human-generated model and data cards.\nWe propose an automated generation approach\nusing Large Language Models (LLMs). Our\nkey contributions include the establishment of\nCARDBENCH, a comprehensive dataset aggre-\ngated from over 4.8k model cards and 1.4k\ndata cards, coupled with the development of\nthe CARDGEN pipeline comprising a two-step\nretrieval process. Our approach exhibits en-\nhanced completeness, objectivity, and faithful-\nness in generated model and data cards, a sig-\nnificant step in responsible AI documentation\npractices ensuring better accountability and\ntraceability.1\n1\nIntroduction\nThe landscape of artificial intelligence (AI) has un-\ndergone a profound transformation with the recent\nsurge in open-sourced models (Villalobos et al.,\n2022; Sevilla et al., 2022) and datasets (Northcutt\net al., 2021; Sevilla et al., 2022). The trend has\nbeen significantly accelerated by the advent of dis-\nruptive technologies such as transformers (Gruet-\nzemacher and Whittlestone, 2022; Vaswani et al.,\n2017). Since this proliferation of accessible mod-\nels and datasets can have their applications signif-\nicantly influence various aspects of society, it be-\ncomes increasingly important to underscore the ne-\ncessity for standardized consistent documentation\nto communicate their performance characteristics\naccurately (Liang et al., 2022).\nIn this context, model cards proposed by Mitchell\net al. (2019) and data cards proposed by Pushkarna\n1Our code and data is available at https://github.com/\njiarui-liu/AutomatedModelCardGeneration.\nHuman Written\nLLM Generated\nLack import information: \ntraining details,\nevaluation metrics, and \nresults, etc.\nUnclear: The description \nof intended use and \nlimitations is unclear\nIncomprehensible: Lacking \nessential technical \ninformation and \nexplanation\n## Training Details\nN/A\n## Intended uses & \nlimitations\nYou can use the raw model \nfor optical character \nrecognition on single \ntext-line images.\n## Model Description\nImages are presented to \nthe model as a sequence \nof \ufb01xed-size patches \n(resolution 16x16).\n## Training Details\nThe model is trained using \nlarge-scale synthetic data \nand \ufb01ne-tuned with \nhuman-labeled datasets.\n## Model Description\nIt resizes the input text \nimage into 384x384 and \nsplits it into a sequence of \n16x16 patches.\n## Intended uses & \nlimitations\nThe model is speci\ufb01cally \ntrained and optimized for \nprinted text recognition \nand may not perform as \nwell on other types of text, \nsuch as handwritten or \nscene text.\nFigure 1: Common problems with manually generated\nmodel cards and data cards.\net al. (2022), emerge as necessary documenta-\ntion tools. These cards bridge the communica-\ntion gap between model/data creators and prod-\nuct developers, thereby ensuring a comprehensive\nunderstanding of the model\u2019s/data\u2019s capabilities\nand limitations in both academic and industrial ap-\nplications (Pushkarna et al., 2022; Sevilla et al.,\n2022; Vaswani et al., 2017; Sevilla et al., 2022).\nModel/data cards are instrumental in research, of-\nfering detailed insights such as data characteristics,\nsources, etc, as well as model architecture, training\nprocedures, and potential biases and limitations,\nwhich accelerates development and reduces error\npropagation in subsequent models (Swayamdipta\net al., 2020).\nInspired by these concepts, HuggingFace (HF) de-\nveloped card specifications for models and datasets\nhosted on its website. Despite the release of some\narXiv:2405.06258v2  [cs.CL]  19 Jun 2024\n\nGPT\nLLAMA\nMISTRAL\nParagraph\nStore\nProvide a 1-2 sentence \nsummary of what the model \n{model} is.\nWhat are the known or \nforeseeable issues stemming \nfrom this model\nWhat metrics will be used \nfor evaluation in light of \ntradeoffs between different \nerrors about the model\nQuestions\nGenerated Result\n## Summary\n{model} is an end-to-end \nTransformer-based OCR \nmodel that leverages \u2026\n### Metrics\nWord-level precision, recall, \nand F1-score: \u2026\nCharacter-level accuracy: \u2026\n### Limitations and Biases\nMay not perform as well on \nother types of text, such as \nhandwritten or scene text.\n\u25cfAbstract\n\u25cfIntroduction\n\u25cfMethods\n\u25cfExperiment\n\u25cfResult\n\u25cfAnalysis\nPseudo \nAnswer\nRetrieval Chain\nPseudo Answer \nChain\nInfer Relevant Paragraph\nGeneration \nChain\nAbstract\nFigure 2: Overview of the CARDGEN pipeline to generate a full model card or a full data card.\navailable tools to assist model card writing2, HF\nleaves the decision of what to report up to devel-\nopers. This raises several problems: First, this\napproach relies heavily on the developers\u2019 under-\nstanding and interpretation of what should be re-\nported, leading to inconsistencies and potential\nomissions of critical information (Shukla et al.,\n2021). Second, there is a tendency among card cre-\nators to use existing cards as templates rather than\nstarting from the standardized template provided\n(Pushkarna et al., 2022). Such variability compro-\nmises the comprehensiveness and reliability of the\ncards.\nWith the power of state-of-the-art LLMs (Touvron\net al., 2023; Brown et al., 2020; Ouyang et al., 2022;\nJiang et al., 2023; Touvron et al., 2023), the auto-\nmatic generation of model and data cards presents\na method to ensure uniformity, consistency, and\nthoroughness across various model/dataset docu-\nmentation. To this end, we contribute the follow-\ning: (1) A novel pioneering initiative to system-\natically utilize LLMs for automatically generat-\ning model/data cards; (2) CARDBENCH, a curated\ndataset that encompasses all the associated papers\nand GitHub READMEs referenced in 4.8k model\ncards and 1.4k data cards; (3) A novel approach\nthat decomposes the card generation task into mul-\ntiple sub-tasks, proposing a CARDGEN pipeline\nincluding a two-step retrieval process; (4) A novel\nset of quantitative and qualitative evaluation met-\nrics. We demonstrate that using our pipeline with\nGPT3.5, we achieve higher scores than human gen-\nerated cards on completeness, objectivity, and un-\nderstandability, demonstrating the effectiveness of\nthe CARDGEN pipeline.\n2https://huggingface.co/spaces/huggingface/\nModel_Cards_Writing_Tool\n2\nRelated Work\n2.1\nAccountability and Traceability for AI\nSystems Through Documentation\nThe increasing complexity of AI systems has raised\nsignificant concerns regarding their potential biases\nand lack of transparency, which in turn poses nega-\ntive implications for users and society (Jacovi et al.,\n2021; Barocas and Selbst, 2016; Panch et al., 2019;\nDaneshjou et al., 2021; Huang et al., 2023). This\nhas motivated the emergence of various documen-\ntation frameworks for ML models and datasets:\nModel Cards\nMitchell et al. (2019) introduced\nthe concept of model cards as a framework for\nthe transparent documentation of machine learn-\ning (ML) models and provided detailed evalua-\ntions across diverse demographic groups and con-\nditions. Subsequent advancements in model card\ndesign have included advocating for the generation\nof consumer labels for ML models (Seifert et al.,\n2019), introducing principles for explainable mod-\nels (Phillips et al., 2020), suggesting other cards as\ncomplements to model cards (Adkins et al., 2022;\nShen et al., 2021), environmental and financial im-\npact considerations (Strubell et al., 2019), and some\ntoolkits that help to track and report specific infor-\nmation in ML models (Arya et al., 2019; Shukla\net al., 2021).\nData Cards\nIn the domain of ML dataset docu-\nmentation, Gebru et al. (2021) pioneered the con-\ncept of datasheets for datasets, followed by the\nintroduction of data statements for NLP data (Ben-\nder and Friedman, 2018; Bender et al., 2021),\nand the concept of data nutrition labels to aid\nin better decision-making (Holland et al., 2020).\n\nMcMillan-Major et al. (2021); Hutchinson et al.\n(2021) provided comprehensive data card tem-\nplates. Pushkarna et al. (2022) proposed data cards\nfor responsible AI development. D\u00edaz et al. (2022)\nintroduced CrowdWorkSheets for the transparent\ndocumentation of crowdsourced data. Our work\nbuilds upon the existing model and data card docu-\nmentation templates released by HF.\n2.2\nKnowledge-Enhanced Text Generation\nLLMs can be augmented with external knowledge\nsources to improve their reasoning capabilities\n(Lewis et al., 2020; Li et al., 2022). Retriever,\ngenerator, and evaluator are the key components\nin a standard RAG system. With the advancement\nof powerful pretrained seq2seq models as gener-\nators, numerous studies have concentrated on the\nevaluation performance:\nRAG Text Generation Evaluation\nDue to vari-\nations in retrieved content, customized generation\npipelines, and user intentions, evaluating the ef-\nfectiveness of LLM generated texts in a Retrieval-\nAugmented Generation (RAG) system becomes\nchallenging (Huang et al., 2023; Mialon et al.,\n2023).\nTraditional n-gram based metrics like\nBLEU (Papineni et al., 2002), ROUGE (Lin, 2004),\nand PARENT-T (Wang et al., 2020b) are used\nfor assessing the overlap between generated texts\nand references, but cannot fully grasp the quality\nnuances of human expectations (Honovich et al.,\n2021; Maynez et al., 2020). Some model-based\nmetrics have later been invented to better align with\nhuman judgments without requiring supervision,\nsuch as BERTScore (Zhang et al., 2019), Mover-\nScore (Zhao et al., 2019), and BARTScore (Yuan\net al., 2021). Research has primarily focused on\nfactuality (Gou et al., 2023; Chen et al., 2023; Gal-\nitsky, 2023; Min et al., 2023), and faithfulness (Bar-\nrantes et al., 2020; Fabbri et al., 2022; Santhanam\net al., 2021; Laban et al., 2023; Durmus et al., 2020)\nof generatd content. Some frameworks have been\ndesigned to automate the assessment pipeline uti-\nlizing the capabilities of LLMs (Es et al., 2023;\nPietsch et al., 2020; Liu et al., 2023; Fu et al., 2023;\nManakul et al., 2023). In this work, we present a\ncomprehensive evaluation of our approach using\nboth traditional metrics and LLM-based automatic\nmetrics. Additionally, we offer a detailed human\nevaluation of multiple performance aspects, includ-\ning faithfulness.\n3\nDefining the Model/Data Card\nGeneration Task\n3.1\nTask Formulation\nDenote our test set as D := {(mi, pi, gi)}N\ni=1 con-\nsisting of N triples, each with a human-generated\nmodel card mi, a direct paper document pi, and\na direct GitHub README document gi.\nFor\neach question qj from the question template set\nQ := {qj}M\nj=1, we define a two-stage retrieve-and-\ngenerate task f1 and f2.\nThe retrieval task f1 : P \u00d7 G \u00d7 Q \u2192R maps\nsource paper and GitHub documents according to\nthe question to a set of retrieved chunks R.\nThe generation task f2 : R \u00d7 Q \u2192A maps the\nretrieved chunk set and questions to a space A that\ncontains generated answers for all questions.\n3.2\nStructured Generation\nInspired by the model card design from Mitchell\net al. (2019), HF provides its guidelines about how\nto fully fill out a model card.3 It suggests a detailed\ndisclosure of the model features and limitations\nin a published model card. Following the guide-\nlines, we define seven sections including 31 indi-\nvidual questions for generating a complete model\ncard. These sections are model summary, model\ndetails, uses, bias and risks, training details, evalua-\ntion, and additional information about the proposed\nmodel. We have made our full question template\nfor both model cards and data cards accessible in\nAppendix A. Table 1 highlights the most important\nquestions for each section of the full template.\n4\nCARDBENCH Dataset\nCARDBENCH contains 4,829 human-generated\nmodel cards and 328 data cards with paper and\nGitHub references.\n4.1\nDataset Collection\nData Source and Preprocessing\nWe identify\nthe model page4 and the dataset page5 on HF\nas data sources. We crawl the model cards and\ndata cards (READMEs) associated with the 10,000\nmost downloaded models and datasets, respectively,\nfrom the HF page as of October 1, 2023. For each\ncollected model card, we use regular expressions\n3https://huggingface.co/docs/hub/\nmodel-card-annotated\n4https://huggingface.co/models\n5https://huggingface.co/datasets\n\nQuestion\nRole\nPrompt\nSummary\nProject organizer\nProvide a 1-2 sentence summary of what the model is.\nDescription\nProject organizer\nProvide basic details about the model. This includes the model architecture,\ntraining procedures, parameters, and important disclaimers.\nDirect use\nProject organizer\nExplain how the model can be used without fine-tuning, post-processing, or\nplugging into a pipeline. Provide a code snippet if necessary.\nBias, risks, limitations\nPractical Ethicist\nWhat are the known or foreseeable issues stemming from this model? These\ninclude foreseeable harms, misunderstandings, and technical and sociotechni-\ncal limitations.\nResults summary\nDeveloper\nSummarize the model evaluation results.\nTable 1: Template of the most important questions for each section. \u201cRoles\u201d are provided as role specifications in\nAppendix Figure 8 for LLMs, and \u201cprompts\u201d are provided as queries.\nto find all valid paper URLs and GitHub reposi-\ntory URLs. We leverage the SciPDF Parser6 to\nparse downloaded paper PDFs into a JSON for-\nmatted data structure, capturing the paper sections.\nWe further use the GitHub REST API7 to obtain\nREADME files from each repository. For each col-\nlected data card, we devise regular expressions to\nlocate all data cards with the \u201cDataset Description\u201d\nsection, which should contain information such\nas the dataset homepage, paper link, and GitHub\nrepository. Then, based on the information ob-\ntained from the data card, we retrieve and process\npaper documents and GitHub READMEs as done\nfor model cards.\nEvaluation Set Construction\nIn the absence of\nstandardized and strict content requirements by HF,\ncollected model cards are mostly incomplete, and\nsome examples are even minimally modified copies\nof existing ones. This variability undermines the\nreliability of our comparative evaluation against\nhuman-generated model cards as a reference met-\nric. In an attempt to mitigate this shortcoming, we\ncurate the highest quality human generated model\ncards to serve as our evaluation data set. This set\ncomprised a selection of 350 examples that are\nrewritten by the HF team with their unique dis-\nclaimers. Also, for data cards, the majority of those\ncollected are incomplete and lack content readabil-\nity. In order to have a sufficient number of evalu-\nation sets, we first selected all the data cards with\na \u201cDataset Description\u201d section. We then wrote\nmarkdown matching logic to obtain 300 examples\nas our evaluation set based on the word count and\nthe number of sections in the data cards. See Ap-\npendix B for more details on data collection.\n6https://github.com/titipata/scipdf_parser\n7https://docs.github.com/en/rest?api\nSplit\nPaper\nGitHub\n# Sections\n# Words\n# Sections\n# Words\nModelCard\nall\n29\n6810\n22\n2495\ntest\n30\n6674\n17\n1855\nDataCard\nall\n25\n5741\n9\n975\ntest\n25\n5784\n8\n816\nTable 2: Statistics for direct paper documents and reposi-\ntory READMEs for crawled model cards and data cards,\nin terms of the average number of sections and the aver-\nage number of words of documents.\n4.2\nData Annotation\nIn our methodology for generating model cards, we\npredominantly focus on the model\u2019s design detail\nitself rather than referencing external methodolo-\ngies cited in human-generated model cards. It ne-\ncessitates the identification of the primary paper\nproposing the model, along with the direct repos-\nitory reflecting model implementation. The eval-\nuation set is annotated by two ML Master\u2019s stu-\ndent researchers who know HF models well and\nare proficient in English. The process resulted in\n294 evaluation examples having both direct paper\nand repository links. Additionally, to annotate the\nwhole dataset, we prompt GPT-3.5-Turbo (Brown\net al., 2020) to validate direct source document\nlinks, given the context wherein each URL is situ-\nated in the model card. We finally obtained 4,829\nnon-empty ones with either direct paper links or\nrepository links. GPT\u2019s annotation reached 98.01%\naccuracy according to human validation results on\nthe test set. For data cards, their primary paper link\nand direct repository responsible for the dataset\nis within the \u201cDataset Description\u201d section. We\nfinally obtained 865 data cards with either direct\npaper links or repository links. This gain resulted in\n99.7% accuracy according to human validation re-\nsults on the 300 data cards test set. See Appendix C\nfor human annotation guidelines and prompts for\nGPT validation.\n\n4.3\nData Statistics\nWe show the overall statistics in Table 2 and Ap-\npendix Table 13. We can observe that our test set,\nconsisting of the set of model cards rewritten by the\nHF team, are more concise than other developer-\nwritten ones. Their corresponding source docu-\nments have similar sizes in terms of the number of\nsections and words.\nTo explore whether our test set represents the whole\ndataset well, we look into some model card fea-\ntures obtained with the HF API. Appendix Figure 6\nshows that test set examples are nearly uniformly\ndistributed compared to the overall dataset in terms\nof the number of downloads, and task distributions\nof models/datasets. A comparison of the test set to\nthe whole set is shown in Appendix Figure 5. See\nAppendix D for additional dataset analyses.\n5\nMethod: the CARDGEN Pipeline\n5.1\nOverview\nFigure 2 shows our CARDGEN pipeline. For each\nqj in Q, we first prompt LLMs to split qj into a sub-\nquestion set. Next, we use LLMs to infer relevant\nsections as potential knowledge sources, and gener-\nate pseudo answers for each sub-question leverag-\ning LLM\u2019s own knowledge (Gao et al., 2023). The\npseudo answer is used as a query to get the set R\nof relevant document chunks. We use an LLM to\ngenerate answers for the question prepended with\nhighest-ranked document chunks.\n5.2\nDesigning the Retriever\nAs the process of supervised retrieval necessitates\nthe acquisition of additional crowd-sourced anno-\ntations for establishing ground truth sentences for\neach query, it constitutes a substantial amount of\nlabor. Consequently, we choose to modify the stan-\ndard RAG retrieval baselines (Lewis et al., 2020),\nwhere source documents are ranked based on the\ninner product similarity with a query question. We\ndevelop a two-step retrieval method to improve the\nretrieval precision: (1) Given all section names\nof a model\u2019s paper and README documents, we\nprompt the LLM to infer the top-k most plausibly\nrelevant sections. (2) We query the pseudo answer\nfrom chunks in the inferred section contents after\nfeeding it into an embedding model. We use the em-\nbedding model jina-embeddings-v2-base-en\ndeveloped by G\u00fcnther et al. (2023). This choice is\nfurther verified in Sections 8.2 and 8.3.\n5.3\nDesigning the Generator\nFor our CARDGEN pipeline, we test Claude\n3 Opus (Anthropic, 2024), GPT-4-Turbo (Ope-\nnAI, 2023), GPT-3.5-Turbo (Brown et al., 2020),\nLlama2 70B Chat (Touvron et al., 2023), Vicuna\n13B V1.5 (Zheng et al., 2024), Llama2 7B Chat\n(Touvron et al., 2023), Mistral 7B Instruct\n(Jiang et al., 2023) as backbone LLMs. We gener-\nate an answer tj for each question qj based on R,\nand concatenate all answers in sequence to form the\nfinal model card. To leverage the LLM\u2019s strengths\nin effectively responding to varied questions, we\nassign specific roles to the LLM tailored to dif-\nferent questions, and outline its expected areas of\nexpertise. The pre-defined roles, such as project\norganizer, sociotechnical practical ethicist, and de-\nveloper, are outlined in Table 1 and Appendix A,\nas noted by Raw et al. (2022). See Appendix F for\nLLM inference details.\n6\nBaselines\nWe evaluate our two-step retrieval and generation\nprocesses in CARDGEN against two baselines:\n(1) One-step retrieval: By keeping all other com-\nponents of our pipeline unchanged, we reduce the\ncurrent two-step retrieval method to a one-step\npipeline by directly retrieving the top-12 chunks\nfrom the entire paper and GitHub documents with-\nout first inferring relevant paragraphs. Although\nintuitively the nature of our question template set\ncorrelates closely with the sectional structure of\npapers and GitHub repositories, this baseline could\nprovide further support of using a paragraph-level\nretriever.\n(2) Retrieval only: Upon completing the two-step\nretrieval and obtaining relevant chunks, the method\ndirectly use the retrieved chunk as the final out-\nput. This is used to assess the advantages of the\nsummary generation step over merely using the\nauthor\u2019s original text.\nWe compare our CARDGEN pipeline with these\ntwo baselines in Section 8.2.\n7\nEvaluation Setup\nWe evaluate CARDGEN on various standard as well\nas state-of-the-art metrics to measure the faithful-\nness, relevance, and other aspects of the generation\nquality. Additionally, we incorporate human eval-\nuation for the pipeline to address three key chal-\n\nMetric\nInput\nDescription\nFactual consistency\nR, A\nHow much the generated answer is supported by retrieved contexts.\nFaithfulness\nQ, R, A\nHow much the statements created from the question-answer pair are supported by the\nretrieved context.\nAnswer relevance\nQ, A\nRelevance score of the answer according to the given question.\nContext precision\nQ, R\nHow much the given context is useful in answering the question.\nContext relevance\nQ, R\nWhether the question can be answered by relevant sentences extracted from the given\ncontext.\nTable 3: Illustration of the input, along with a description of standard metrics and GPT-based metrics being used.\nHere Q, R, A represent the questions, retrieved texts, and generated texts, respectively.\nMetric\nHuman\nClaude3 Opus\nGPT4\nGPT3.5\nLlama2 70B\nVicuna 13B\nLlama2 7B\nMistral 7B\nCompleteness\n1.92\n7.28\n5.99\n5.24\n4.76\n4.24\n2.50\n4.07\nAccuracy\n6.66\n6.56\n6.04\n4.51\n3.61\n3.11\n1.84\n3.67\nObjectivity\n2.03\n7.16\n6.33\n5.23\n4.72\n4.25\n2.12\n4.16\nUnderstandability\n2.49\n7.11\n6.21\n4.99\n4.80\n3.80\n2.09\n4.51\nReference quality\n6.13\n6.75\n5.40\n4.28\n4.15\n3.73\n1.63\n3.93\nTable 4: Human evaluation results on LLM generated and human-generated model cards.\nlenges that can\u2019t be solved by automatic metrics\nalone: First, there is an absence of ground truth\nlabels of generated model cards by CARDGEN. To\nmitigate this, we have to develop specific manual\nevaluations to assess performance. Second, current\nmodel cards created by human developers are of-\nten incomplete and deviate from the recommended\ntemplate provided by HF. Third, the LLM gener-\nated model card is typically long with over 4000\nwords, and brings challenges to both open-source\nstandard evaluations with limited context size and\ncostly GPT-based metrics.\nStandard Metrics\nWe follow Honovich et al.\n(2022) and use ROUGE (Lin, 2004), BERTScore\n(Zhang et al., 2019), BARTScore (Yuan et al.,\n2021), and NLI-finetuned models (Williams et al.,\n2018; MacCartney and Manning, 2008) to mea-\nsure the factual consistency of retrieved chunks set\nR and the generated answer A. Due to the large\nsize of retrieved texts, we use deberta-v3-base\nas the base model for BERTScore, and use\nnli-deberta-v3-large as the NLI-finetuned\nmodel scorer (Reimers and Gurevych, 2019a; He\net al., 2021). More details in Appendix H.\nGPT Metrics\nFollowing Es et al. (2023), we con-\nsider the measurement of faithfulness, answer rele-\nvance, context precision, and context relevance us-\ning GPT4. Table 3 provides a description of these\nmetrics. As different combinations of inputs are\ntaken into consideration, these metrics are neces-\nsary supplements to standard metrics. Full prompt\ndetails are explained in Appendix H.\nHuman Evaluation Metrics\nPutting together\nLLM generated cards with the human-generated\ncards as a sample, we devise the following manual\nevaluation metrics: completeness, accuracy, objec-\ntivity, understandability, and reference quality. We\ndesign a simple Gradio annotation interface (Abid\net al., 2019), and more details are in Appendix I.\n8\nResults\n8.1\nPerformance Summary\nOur human evaluation results are shown in Table 4\nand automatic evaluation results are shown in Ta-\nbles 5 and 6 for model cards. The only difference\nfor the data card generation pipeline is the substi-\ntution with data card question templates. In this\nsubsection, we mainly answer two questions below:\nAre our generated model cards better than\nhuman-generated ones?\nWe conduct a random\nsampling of 50 model cards from the test set and\ncompute the average metric scores across all the\nannotated samples, as shown in Table 4. GPT3.5,\nGPT4, and Claude3 Opus demonstrates superior\nperformance over other open-sourced LLMs and\nhuman-generated content in terms of completeness,\nobjectivity, and understandability. This finding\naligns with the observations presented below for\nTables 5 and 6.\nConversely, the human-generated model cards of-\nten received higher scores in accuracy and refer-\nence quality. This disparity suggests that all LLMs\nexhibit some degree of hallucination for factual\ncontent and reference links in their generation. It is\n\nMetric\nModel\nSummary\nModel details\nUses\nBias\nTraining details\nEvaluation\nMore info\nAll\nROUGE-L\nClaude3 Opus\n8.91\n11.26\n14.39\n14.21\n14.11\n14.99\n12.78\n13.04\nGPT4\n8.80\n9.35\n15.38\n18.20\n17.59\n19.40\n9.73\n13.27\nGPT3.5\n9.90\n10.70\n16.51\n20.21\n14.46\n15.75\n10.73\n13.16\nLlama2 70b chat\n12.71\n14.35\n12.85\n17.20\n18.74\n18.03\n16.21\n15.98\nVicuna 13b v1.5\n10.78\n11.35\n13.54\n17.10\n16.06\n16.75\n10.29\n13.12\nLlama2 7b chat\n11.91\n12.84\n13.89\n15.85\n14.63\n16.21\n13.61\n14.08\nMistral 7b inst\n12.19\n11.01\n13.02\n15.07\n16.79\n16.23\n9.47\n12.70\nBERTScore\nClaude3 Opus\n54.78\n53.73\n58.42\n56.32\n57.83\n58.80\n55.17\n56.10\nGPT4\n54.06\n50.44\n57.81\n58.81\n59.50\n61.24\n47.48\n53.96\nGPT3.5\n54.86\n53.17\n58.62\n59.29\n56.61\n57.42\n52.47\n55.09\nLlama2 70b chat\n57.21\n56.15\n53.97\n56.55\n59.69\n59.46\n56.99\n57.21\nVicuna 13b v1.5\n55.15\n52.97\n54.99\n57.24\n57.61\n58.83\n52.10\n54.83\nLlama2 7b chat\n55.76\n54.51\n53.93\n55.48\n56.30\n57.13\n54.72\n55.26\nMistral 7b inst\n55.69\n52.80\n54.12\n53.76\n57.10\n57.63\n49.12\n53.47\nBARTScore\nClaude3 Opus\n13.92\n5.60\n2.56\n1.59\n4.10\n2.87\n4.33\n4.36\nGPT4\n9.69\n7.63\n1.43\n1.98\n4.02\n4.29\n6.11\n5.34\nGPT3.5\n17.09\n9.58\n2.04\n3.52\n5.75\n6.65\n9.10\n7.61\nLlama2 70b chat\n14.17\n5.41\n1.45\n3.10\n5.30\n4.60\n5.91\n5.15\nVicuna 13b v1.5\n13.53\n5.67\n1.90\n3.76\n5.63\n6.81\n6.77\n5.90\nLlama2 7b chat\n14.04\n3.49\n2.11\n3.61\n4.70\n3.68\n4.01\n4.03\nMistral 7b inst\n16.52\n9.65\n2.00\n3.55\n7.00\n8.75\n8.31\n7.90\nNLI\nClaude3 Opus\n58.00\n54.62\n56.33\n59.00\n62.25\n61.40\n60.12\n58.68\nGPT4\n61.00\n52.88\n53.00\n56.00\n64.50\n65.60\n62.62\n59.42\nGPT3.5\n65.14\n49.83\n57.54\n62.41\n59.14\n60.14\n56.80\n56.54\nLlama2 70b chat\n56.46\n51.70\n55.22\n58.42\n57.70\n62.04\n59.74\n57.14\nVicuna 13b v1.5\n60.20\n51.40\n58.05\n55.10\n58.29\n63.33\n55.00\n56.31\nLlama2 7b chat\n56.46\n50.19\n54.31\n57.23\n57.82\n62.11\n56.44\n55.77\nMistral 7b inst\n58.67\n50.36\n54.25\n54.59\n59.06\n58.91\n55.17\n55.02\nTable 5: Factual consistency evaluation results per section on our retrieve-and-generate pipeline using ROUGE-L,\nBERTScore, BARTScore, and NLI pretrained scorers.\nimportant to note that the human-generated model\ncards\u2019 incompleteness precludes a direct compari-\nson of human evaluation metrics with the metrics\nused in Tables 5 and 6. Moreover, the insights\nderived from Table 4 are not obtainable through\nautomatic metrics. We thus conclude that human\nevaluation metrics are indispensable components\nof our overall evaluation framework.\nHow does GPT3.5 perform compared with open\nsourced LLMs?\nFrom Table 5, we can\u2019t ob-\nserve a uniform trend for factual consistency across\nall sub-tasks. GPT3.5 outperforms open-sourced\nLLMs on \u201cUses\u201d and \u201cBias\u201d question sets in 3 over\n4 standard metrics, while Llama2 70b generates\nmore factual consistent answers on other sub-tasks\naccording to ROUGE-L and BERTScore.\nAccording to Table 6, GPT3.5 beats other LLMs\non faithfulness and answer relevance across nearly\nall sub-tasks, and shows its strong instruction-\nfollowing capabilities for question-answering.\nHowever, we have an interesting observation that\nthough GPT3.5 has higher context relevance scores,\nit is outperformed by Mistral 7B on context pre-\ncision. A higher context relevance indicates that\nthe question can be better answered from the given\ncontext, while a lower context precision means that\nthe context may contain other unnecessary informa-\ntion for answering the question. The discrepancy\nbetween results by these two metrics suggests that\nretrieved texts from the GPT CARDGEN pipeline\nare more informative but less concise. Addition-\nally, since we use LLM generated pseudo answers\nas queries for similar paragraphs, pseudo answers\nwith more possibly unrelated contents will lead to\nmore irrelevant chunks from retrieval. Along with\nthe illustration in Appendix Figure 7, we draw the\nconclusion that GPT3.5 generates pseudo answers\nwith potentially more unrelated details.\n8.2\nBaseline Results\nTo assess the effectiveness of CARDGEN\u2019s retriever\nand generator, we first compare it to the baseline\nmethods outlined in Section 6. To manage the\nexpenses associated with OpenAI AI calling, we\nemploy GPT3.5 for subsequent studies. We obtain\nKrippendorff\u2019s \u03b1 (mean=0.83, std=0.14, min=0.56,\nmax=0.99) for the agreements on Table 6 by GPT4\nand GPT3.5 to validate our evaluation model sub-\nstitution (Castro, 2017).\n\nMetric\nModel\nSummary\nDescription\nDirect use\nBias, risks, limitation\nResults summary\nFaithfulness\nClaude3 Opus\n74.97\n49.77\n78.23\n71.28\n84.89\nGPT4\n68.87\n85.58\n62.99\n64.20\n86.44\nGPT3.5\n71.23\n83.21\n48.71\n55.17\n82.99\nLlama2 70b chat\n70.03\n76.39\n43.20\n32.14\n63.87\nVicuna 13b v1.5\n78.46\n81.74\n45.94\n46.64\n78.22\nLlama2 7b chat\n72.41\n71.35\n48.43\n44.23\n65.56\nMistral 7b inst\n76.75\n75.03\n38.28\n41.77\n73.61\nAnswer relevance\nClaude3 Opus\n90.42\n91.10\n89.12\n91.39\n93.15\nGPT4\n90.83\n93.12\n89.69\n92.03\n91.36\nGPT3.5\n91.18\n93.26\n90.70\n93.75\n93.24\nLlama2 70b chat\n90.76\n92.27\n91.25\n92.23\n91.63\nVicuna 13b v1.5\n89.00\n91.22\n90.17\n92.99\n90.38\nLlama2 7b chat\n90.44\n90.95\n92.55\n92.69\n92.81\nMistral 7b inst\n90.46\n91.77\n90.36\n91.56\n90.43\nContext precision\nClaude3 Opus\n33.25\n51.73\n26.17\n20.99\n42.58\nGPT4\n35.01\n51.25\n29.29\n22.76\n40.23\nGPT3.5\n29.07\n51.80\n25.71\n18.77\n37.88\nLlama2 70b chat\n21.05\n50.00\n25.35\n20.03\n40.82\nVicuna 13b v1.5\n24.91\n51.22\n24.00\n8.93\n39.00\nLlama2 7b chat\n32.46\n50.79\n25.52\n14.27\n40.04\nMistral 7b inst\n31.10\n52.22\n28.45\n21.36\n44.45\nContext relevance\nClaude3 Opus\n13.32\n48.82\n28.90\n21.32\n23.01\nGPT4\n12.86\n52.39\n26.63\n18.89\n23.47\nGPT3.5\n13.27\n51.03\n29.82\n18.97\n26.44\nLlama2 70b chat\n13.32\n49.62\n27.22\n18.37\n24.31\nVicuna 13b v1.5\n13.83\n51.32\n27.00\n14.03\n23.08\nLlama2 7b chat\n13.87\n50.78\n28.07\n17.57\n26.23\nMistral 7b inst\n13.22\n47.05\n28.40\n18.75\n23.52\nTable 6: GPT4 evaluation results on five most important questions based on faithfulness (Faith), answer relevance\n(AR), context precision (CP), and context relevance (CR).\nModel\nMethod\nCP\nCR\nGPT3.5\nOne-step retrieval\n44.03\n27.82\nCARDGEN\n44.67 (+0.64)\n28.24 (+0.42)\nLlama2 70B\nOne-step retrieval\n42.94\n28.10\nCARDGEN\n44.03 (+1.09)\n28.83 (+0.73)\nLlama2 7B\nOne-step retrieval\n43.47\n27.35\nCARDGEN\n41.91 (-1.56)\n28.00 (+0.65)\nMistral 7B\nOne-step retrieval\n43.75\n27.80\nCARDGEN\n45.24 (+1.49)\n27.97 (+0.17)\nTable 7: GPT3.5 evaluation results of the one-step re-\ntrieval baseline and CARDGEN in terms of context pre-\ncision and context relevance.\nOne-step retrieval\nSince the change is only in\nthe retrieval process in comparison to CARDGEN,\nwe focus exclusively on context precision and con-\ntext relevance as metrics. These metrics evaluate\nthe quality of the retrieved text ri in response to a\ngiven question qi. We evaluate across four LLMs,\nand report results based on the averaged score of\nthe most important questions. According to Table 7,\nthe two-step retrieval process achieves marginally\nyet consistently higher scores than the one-step re-\ntrieval across nearly all models. These findings\nindicate that a paragraph-level retrieval model con-\nstitutes a more appropriate method for this study.\nModel\nMethod\nAR\nUnderstandability\nGPT3.5\nRetrieval only\n81.28\n5.60%\nCARDGEN\n90.84 (+9.56)\n94.40%\nLlama2 70B\nRetrieval only\n81.61\n1.60%\nCARDGEN\n90.32 (+8.71)\n98.40%\nLlama2 7B\nRetrieval only\n81.32\n4.40%\nCARDGEN\n90.78 (+9.46)\n95.60%\nMistral 7B\nRetrieval only\n81.49\n2.40%\nCARDGEN\n89.83 (+8.34)\n97.60%\nTable 8: GPT3.5 evaluation results of the retrieval-only\nbaseline and CARDGEN in terms of answer relevance\nand understandability. Full results including assess-\nments of brevity can be found in Appendix Table 15.\nRetrieval only\nFollowing the same evaluation\nsetup as above, we consider answer relevance of\ngenerated text gi according to a given question qi.\nTo further compare which method produces more\nunderstandable and concise outputs, we also in-\ncorporate understandability and brevity into our\nevaluation as GPT metrics for pairwise comparison\n(Liusie et al., 2024; Fu et al., 2023). As illustrated\nin Table 8, CARDGEN significantly outperforms\nthe retrieval-only baseline across all metrics, high-\nlighting the importance of the generation step in\nsummarizing and restating sentences from source\n\nMetric\nModel\nSummary\nDescription\nDirect use\nBias, risks, limitation\nResults summary\nNLI\nGPT3.5\n65.14 (+2.14)\n51.53 (+0.53)\n50.51 (+0.51)\n64.12 (+1.12)\n58.50 (+0.50)\nw/o pseudo\n63.00\n51.00\n50.00\n63.00\n58.00\nFaith\nGPT3.5\n81.93 (+6.75)\n79.30 (+4.30)\n41.23 (+0.62)\n46.42 (-2.53)\n72.66 (+1.21)\nw/o pseudo\n75.18\n75.00\n40.61\n48.95\n71.45\nAR\nGPT3.5\n86.94 (+0.06)\n89.56 (-0.65)\n88.95 (+0.78)\n93.55 (+0.40)\n95.20 (+0.02)\nw/o pseudo\n86.88\n90.21\n88.17\n93.15\n95.18\nCP\nGPT3.5\n47.53 (+7.49)\n19.61 (+1.01)\n13.44 (+3.20)\n13.03 (-0.26)\n64.15 (+0.24)\nw/o pseudo\n40.04\n18.60\n10.24\n13.29\n63.91\nCR\nGPT3.5\n11.85 (+2.32)\n23.24 (-2.21)\n8.70 (+1.19)\n4.35 (+0.69)\n24.04 (+5.79)\nw/o pseudo\n9.53\n25.45\n7.51\n3.66\n18.25\nFaith\nGPT3.5\n81.93 (+8.09)\n79.30 (+15.31)\n41.23 (+26.62)\n46.42 (+22.14)\n72.66 (+25.16)\nLlama2 70B\n73.84\n63.99\n14.61\n24.28\n47.50\nAR\nGPT3.5\n86.94 (-1.56)\n89.56 (+0.63)\n88.95 (+6.58)\n93.55 (+9.53)\n95.20 (+7.21)\nLlama2 70B\n88.50\n88.93\n82.37\n84.02\n87.99\nTable 9: GPT3.5 evaluation results on five most important questions for pseudo answer chain ablation in top five\nrows and generation chain ablation in bottom two rows. For the generation chain ablation, we keep all previous\nchains unchanged with GPT-3.5-turbo as the backbone, and only vary the choice of LLMs for the final generation\nchain, including GPT-3.5-turbo and Llama2-70B-Chat-HF.\ndocuments to enhance their understandability and\nconciseness. Further details are in Appendix J.\n8.3\nAblation Study\nWe also conducted the following ablation studies\nand explored model architecture variations to fur-\nther validate CARDGEN\u2019s components: (1) Re-\nmove the pseudo answer chain and use original\nquestions for embedding similarity matching. (2)\nVary the final generation chain only with different\nLLMs, and maintain all preceding reasoning chains\nas generated by GPT3.5. (3) Employ different em-\nbedding models for dense retrieval.\nPseudo Answer Chain\nWe compare the GPT\nevaluation scores and factual consistency using\nNLI of CARDGEN + GPT3.5 pipeline with or with-\nout the pseudo answer chain, as illustrated in Ta-\nble 9. CARDGEN with the pseudo answer chain\noutperforms the other across nearly all important\nquestions and metrics being tested. Our results\ndemonstrate the necessity of the pseudo answer\nchain in our pipeline. Some lower scores may be\nbecause of more unrelated texts from the generated\npseudo answers for specific questions.\nGeneration Chain\nIn bottom two rows of Ta-\nble 9, we show the comparison results by only\nsubstituting GPT3.5 in the generation chain with\nLlama2 70B based on faithfulness and answer rele-\nvance. Context precision and context relevance are\nthe same since retrieved texts remain unchanged.\nWe observe a large drop for the faithfulness score\nand a moderate drop for the answer relevance score,\nindicating the stronger instruction following capa-\nbility of GPT3.5 in the generation stage compared\nto Llama2 70B.\nEmbedding Models\nWe compare the embed-\nding model jina-embeddings-v2-base-en that\nwe use with two other commonly used sen-\ntence transformer models:\nall-MiniLM-L6-v2\nand all-mpnet-base-v2 (G\u00fcnther et al., 2023;\nWang et al., 2020a; Reimers and Gurevych, 2019b,\n2020). We justify our choice of embedding models\nin Appendix Figure 12, where CARDGEN with\njina-embeddings-v2-base-en performs better\nthan others according to all three metrics related to\nthe retrieved texts.\n8.4\nLLM Generated Model Card Statistics\nAppendix G provides related statistics. Compared\nwith statistics in Table 13, LLM generates longer\nand more informative than human.\n9\nConclusion\nIn this study, we introduce a novel task focused\non the automatic generation of model cards and\ndata cards. This task is facilitated by the creation\nof the CARDBENCH dataset, and the development\nof the CARDGEN pipeline leveraging state-of-the-\nart LLMs. The system is designed to assist in the\ngeneration of understandable, comprehensive, and\nconsistent models and data cards, thereby providing\na valuable contribution to the field of responsible\nAI.\n\nLimitations\nOne limitation of our method is that, despite the\nadoption of the RAG pipeline and explicit instruc-\ntions for LLMs to adhere closely to the retrieved\ntext, there remains the potential for hallucinations\nin the generated text. To mitigate this, future work\nmay integrate specific strategies into our CARD-\nGEN pipeline for hallucination reduction by care-\nfully balancing generation speed with quality.\nOur current approach employs a single-step gen-\neration process and a two-step retrieval process\nthat first infers relevant section contents. Future\nwork could incorporate more advanced chain-of-\nthought prompting techniques and compare with\nour CARDGEN pipeline. For complex questions re-\nquiring multistep reasoning, after decomposed into\nmanageable sub-questions, we can address each\nsub-question through multiple reasoning steps, as\nsuggested by recent research (Yao et al., 2022; Khot\net al., 2022; Press et al., 2022; He et al., 2022).\nAdditionally, an iterative retrieval-generation col-\nlaborative framework can also be used to refine re-\nsponses in each iteration based on newly retrieved\ncontexts, following recent advancements in itera-\ntive retrieval and generation frameworks for com-\nplex tasks (Shao et al., 2023; Feng et al., 2023).\nEthical Considerations\nThis work aims to provide insights about the au-\ntomatic generation of model cards and data cards.\nSuch an endeavor is instrumental in promoting ac-\ncountability and traceability among developers as\nthey document their models. The dataset for this re-\nsearch was collected using public REST APIs from\nHF Hub, Arxiv, and GitHub. We ensured that only\nopen-source model cards, data cards, and their as-\nsociated source documents were collected, strictly\nadhering to the stipulations of their respective li-\ncenses for research purposes, so there were no user\nprivacy concerns in the dataset. Our dataset and\nmethod should only be used for research purpose.\nOn the other hand, while the questions we pose to\nLLMs are technical and specific, there remains a\nrisk of receiving biased responses, particularly for\ncertain queries. For instance, the question about\nmodel limitations might yield biased answers, as\nsource papers and GitHub READMEs could con-\ntain overstated claims about their models. Conse-\nquently, our generated model cards could contain\nthese statements as well if the source texts contain-\ning them are retrieved.\nTo mitigate this, one reasonable approach is to in-\nsert a step after retrieval to filter out or neutralize\noverstatements. Additionally, we can explicitly\nprompt LLMs to account for such biases during\nthe generation stage. Another concern is the po-\ntential for content homogeneity when using LLMs\nfor model card generation. Excessive reliance on\ntemplates could limit model card creators\u2019 potential\nto discuss new issues not covered in the original\npapers or GitHub repositories (Nakadai et al., 2023;\nAcion et al., 2023).\nMoreover, one aspect of our approach is that we\nuse direct prompts to LLMs rather than fine-tuning\nthem on human-generated model cards, which can\nalso exhibit biases from the internal of LLMs, such\nas overstatements on well-known models or omis-\nsions of potential risks. In our analysis of 2495\nhuman-written model cards in our dataset, only\n30.54% mention \u201cweakness(es)\u201d or \u201climitation(s)\u201d,\nand 15.23% mention \u201cbias(es)\u201d. If future study can\ncollect more fairly-written human-generated model\ncards, they can also be used to finetune LLMs for\nbetter performance on this task.\nReferences\nAbubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan,\nAbdulrahman Alfozan, and James Zou. 2019. Gradio:\nHassle-free sharing and testing of ml models in the wild.\narXiv preprint arXiv:1906.02569.\nLaura Acion, Mariela Rajngewerc, Gregory Randall,\nand Lorena Etcheverry. 2023. Generative ai poses eth-\nical challenges for open science. Nature human be-\nhaviour, 7(11):1800\u20131801.\nDavid Adkins, Bilal Alsallakh, Adeel Cheema, Nar-\nine Kokhlikyan, Emily McReynolds, Pushkar Mishra,\nChavez Procope, Jeremy Sawruk, Erin Wang, and\nPolina Zvyagina. 2022. Prescriptive and descriptive\napproaches to machine-learning transparency. In CHI\nConference on Human Factors in Computing Systems\nExtended Abstracts, pages 1\u20139.\nAnthropic. 2024. The claude 3 model family: Opus,\nsonnet, haiku.\nVijay Arya, Rachel KE Bellamy, Pin-Yu Chen, Amit\nDhurandhar, Michael Hind, Samuel C Hoffman,\nStephanie Houde, Q Vera Liao, Ronny Luss, Aleksandra\nMojsilovi\u00b4c, et al. 2019. One explanation does not fit all:\nA toolkit and taxonomy of ai explainability techniques.\narXiv preprint arXiv:1909.03012.\nSolon Barocas and Andrew D. Selbst. 2016. Big data\u2019s\ndisparate impact. California Law Review, 104:671.\n\nMario Barrantes, Benedikt Herudek, and Richard Wang.\n2020. Adversarial nli for factual correctness in text sum-\nmarisation models. arXiv preprint arXiv:2005.11739.\nEmily M. Bender and Batya Friedman. 2018. Data state-\nments for natural language processing: Toward mitigat-\ning system bias and enabling better science. Transac-\ntions of the Association for Computational Linguistics,\n6:587\u2013604.\nEmily M. Bender, Batya Friedman, and Angelina\nMcMillan-Major. 2021. Data statements for nlp: To-\nwards best practices.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020.\nLanguage models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nSantiago Castro. 2017.\nFast Krippendorff:\nFast\ncomputation\nof\nKrippendorff\u2019s\nalpha\nagreement\nmeasure. https://github.com/pln-fing-udelar/\nfast-krippendorff.\nJifan Chen, Grace Kim, Aniruddh Sriram, Greg Dur-\nrett, and Eunsol Choi. 2023. Complex claim verifica-\ntion with evidence retrieved in the wild. arXiv preprint\narXiv:2305.11859.\nRoxana Daneshjou, Mary P. Smith, Mary D. Sun, Veron-\nica Rotemberg, and James Zou. 2021. Lack of Trans-\nparency and Potential Bias in Artificial Intelligence Data\nSets and Algorithms: A Scoping Review. JAMA Der-\nmatology, 157(11):1362\u20131369.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805.\nMark D\u00edaz, Ian Kivlichan, Rachel Rosen, Dylan Baker,\nRazvan Amironesei, Vinodkumar Prabhakaran, and\nEmily Denton. 2022.\nCrowdworksheets: Account-\ning for individual and collective identities underlying\ncrowdsourced dataset annotation. In Proceedings of the\n2022 ACM Conference on Fairness, Accountability, and\nTransparency, pages 2342\u20132351.\nEsin Durmus, He He, and Mona Diab. 2020. FEQA:\nA question answering evaluation framework for faith-\nfulness assessment in abstractive summarization. In\nProceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 5055\u20135070,\nOnline. Association for Computational Linguistics.\nShahul Es, Jithin James, Luis Espinosa-Anke, and\nSteven Schockaert. 2023. Ragas: Automated evalu-\nation of retrieval augmented generation. arXiv preprint\narXiv:2309.15217.\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. QAFactEval: Improved QA-\nbased factual consistency evaluation for summariza-\ntion. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\npages 2587\u20132601, Seattle, United States. Association\nfor Computational Linguistics.\nZhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin\nYang, and Bing Qin. 2023. Retrieval-generation syn-\nergy augmented large language models. arXiv preprint\narXiv:2310.05149.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nBoris A. Galitsky. 2023. Truth-o-meter: Collaborating\nwith llm in fighting its hallucinations. Preprints.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2023. Precise zero-shot dense retrieval without rele-\nvance labels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1762\u20131777, Toronto,\nCanada. Association for Computational Linguistics.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione,\nJennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9\nIii, and Kate Crawford. 2021. Datasheets for datasets.\nCommunications of the ACM, 64(12):86\u201392.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong\nShen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.\nCritic: Large language models can self-correct with tool-\ninteractive critiquing. arXiv preprint arXiv:2305.11738.\nRoss Gruetzemacher and Jess Whittlestone. 2022. The\ntransformative potential of artificial intelligence. Fu-\ntures, 135:102884.\nMichael G\u00fcnther, Jackmin Ong, Isabelle Mohr, Alaed-\ndine Abdessalem, Tanguy Abel, Mohammad Kalim\nAkram, Susana Guzman, Georgios Mastrapas, Saba Stu-\nrua, Bo Wang, Maximilian Werk, Nan Wang, and Han\nXiao. 2023. Jina embeddings 2: 8192-token general-\npurpose text embeddings for long documents.\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language model\ninference. arXiv preprint arXiv:2301.00303.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding sharing.\narXiv preprint arXiv:2111.09543.\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua\nJoseph, and Kasia Chmielinski. 2020. The dataset nutri-\ntion label. Data Protection and Privacy, 12(12):1.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and Yossi\nMatias. 2022. True: Re-evaluating factual consistency\nevaluation. arXiv preprint arXiv:2204.04991.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021. q2:\nEvaluating factual consistency in knowledge-grounded\ndialogues via question generation and question answer-\ning. In Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n\n7856\u20137870, Online and Punta Cana, Dominican Repub-\nlic. Association for Computational Linguistics.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen, Wei-\nhua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A\nsurvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions. arXiv\npreprint arXiv:2311.05232.\nBen Hutchinson, Andrew Smart, Alex Hanna, Emily\nDenton, Christina Greer, Oddur Kjartansson, Parker\nBarnes, and Margaret Mitchell. 2021. Towards account-\nability for machine learning datasets: Practices from\nsoftware engineering and infrastructure. In Proceedings\nof the 2021 ACM Conference on Fairness, Accountabil-\nity, and Transparency, pages 560\u2013575.\nAlon Jacovi, Ana Marasovi\u00b4c, Tim Miller, and Yoav\nGoldberg. 2021. Formalizing trust in artificial intelli-\ngence: Prerequisites, causes and goals of human trust in\nai. In Proceedings of the 2021 ACM conference on fair-\nness, accountability, and transparency, pages 624\u2013635.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume\nLample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv\npreprint arXiv:2310.06825.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with GPUs. IEEE Trans-\nactions on Big Data, 7(3):535\u2013547.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022.\nDecomposed prompting: A modular\napproach for solving complex tasks. arXiv preprint\narXiv:2210.02406.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-\nzalez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-\nory management for large language model serving with\npagedattention. In Proceedings of the ACM SIGOPS\n29th Symposium on Operating Systems Principles.\nPhilippe Laban, Wojciech Kry\u00b4sci\u00b4nski, Divyansh Agar-\nwal, Alexander R Fabbri, Caiming Xiong, Shafiq Joty,\nand Chien-Sheng Wu. 2023. Llms as factual reasoners:\nInsights from existing benchmarks and beyond. arXiv\npreprint arXiv:2305.14540.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neural\nInformation Processing Systems, 33:9459\u20139474.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented text\ngeneration. arXiv preprint arXiv:2202.01110.\nWeixin Liang, Girmaw Abebe Tadesse, Daniel Ho,\nL Fei-Fei, Matei Zaharia, Ce Zhang, and James Zou.\n2022. Advances, challenges and opportunities in creat-\ning data for trustworthy ai. Nature Machine Intelligence,\n4(8):669\u2013677.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74\u201381, Barcelona, Spain. Associa-\ntion for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment.\narXiv preprint arXiv:2303.16634.\nAdian Liusie, Potsawee Manakul, and Mark Gales. 2024.\nLLM comparative assessment: Zero-shot NLG evalua-\ntion through pairwise comparisons using large language\nmodels. In Proceedings of the 18th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 139\u2013151,\nSt. Julian\u2019s, Malta. Association for Computational Lin-\nguistics.\nBill MacCartney and Christopher D. Manning. 2008.\nModeling semantic containment and exclusion in nat-\nural language inference. In Proceedings of the 22nd\nInternational Conference on Computational Linguistics\n(Coling 2008), pages 521\u2013528, Manchester, UK. Coling\n2008 Organizing Committee.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales.\n2023. Selfcheckgpt: Zero-resource black-box halluci-\nnation detection for generative large language models.\narXiv preprint arXiv:2303.08896.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factuality in\nabstractive summarization. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1906\u20131919, Online. Association for\nComputational Linguistics.\nAngelina McMillan-Major, Salomey Osei, Juan Diego\nRodriguez, Pawan Sasanka Ammanamanchi, Sebastian\nGehrmann, and Yacine Jernite. 2021. Reusable tem-\nplates and guides for documenting datasets and models\nfor natural language processing and generation: A case\nstudy of the huggingface and gem data and model cards.\narXiv preprint arXiv:2108.07374.\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli\nCelikyilmaz, et al. 2023. Augmented language models:\na survey. arXiv preprint arXiv:2302.07842.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. Factscore: Fine-\ngrained atomic evaluation of factual precision in long\nform text generation. arXiv preprint arXiv:2305.14251.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson, Elena\nSpitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019.\nModel cards for model reporting. In Proceedings of\nthe conference on fairness, accountability, and trans-\nparency, pages 220\u2013229.\n\nRyosuke Nakadai, Yo Nakawake, and Shota Shibasaki.\n2023. Ai language tools risk scientific diversity and in-\nnovation. Nature human behaviour, 7(11):1804\u20141805.\nCurtis G Northcutt, Anish Athalye, and Jonas Mueller.\n2021.\nPervasive label errors in test sets destabi-\nlize machine learning benchmarks.\narXiv preprint\narXiv:2103.14749.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow instruc-\ntions with human feedback, 2022. URL https://arxiv.\norg/abs/2203.02155, 13.\nTrishan Panch, Heather Mattie, and Rifat Atun. 2019.\nArtificial intelligence and algorithmic bias: implications\nfor health systems. Journal of global health, 9(2).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th\nannual meeting of the Association for Computational\nLinguistics, pages 311\u2013318.\nP Jonathon Phillips, Carina A Hahn, Peter C Fontana,\nDavid A Broniatowski, and Mark A Przybocki. 2020.\nFour principles of explainable artificial intelligence.\nGaithersburg, Maryland, 18.\nMalte Pietsch, Soni Tanay, Chan Branden, M\u00f6ller Timo,\nand Kosti\u00b4c Bogdan. 2020. Deepset-ai/haystack.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring and\nnarrowing the compositionality gap in language models.\narXiv preprint arXiv:2210.03350.\nMahima Pushkarna, Andrew Zaldivar, and Oddur Kjar-\ntansson. 2022. Data cards: Purposeful and transparent\ndataset documentation for responsible ai. In Proceed-\nings of the 2022 ACM Conference on Fairness, Account-\nability, and Transparency, pages 1776\u20131826.\nNathan Raw, Adrin Jalali, and Sugato Ray. 2022. [link].\nNils Reimers and Iryna Gurevych. 2019a. Sentence-\nbert: Sentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nNils Reimers and Iryna Gurevych. 2019b. Sentence-\nbert: Sentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Association\nfor Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2020. Making mono-\nlingual sentence embeddings multilingual using knowl-\nedge distillation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing.\nAssociation for Computational Linguistics.\nSashank Santhanam, Behnam Hedayatnia, Spandana\nGella, Aishwarya Padmakumar, Seokhwan Kim, Yang\nLiu, and Dilek Hakkani-Tur. 2021.\nRome was\nbuilt in 1776: A case study on factual correctness\nin knowledge-grounded response generation.\narXiv\npreprint arXiv:2110.05456.\nChristin Seifert, Stefanie Scherzinger, and Lena Wiese.\n2019. Towards generating consumer labels for machine\nlearning models. In 2019 IEEE First International Con-\nference on Cognitive Machine Intelligence (CogMI),\npages 173\u2013179. IEEE.\nJaime Sevilla, Lennart Heim, Anson Ho, Tamay Be-\nsiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022.\nCompute trends across three eras of machine learning.\nIn 2022 International Joint Conference on Neural Net-\nworks (IJCNN), pages 1\u20138. IEEE.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie\nHuang, Nan Duan, and Weizhu Chen. 2023. Enhanc-\ning retrieval-augmented large language models with\niterative retrieval-generation synergy. arXiv preprint\narXiv:2305.15294.\nHong Shen, Wesley H. Deng, Aditi Chattopadhyay, Zhi-\nwei Steven Wu, Xu Wang, and Haiyi Zhu. 2021. Value\ncards: An educational toolkit for teaching social impacts\nof machine learning through deliberation. In Proceed-\nings of the 2021 ACM Conference on Fairness, Account-\nability, and Transparency, FAccT \u201921, page 850\u2013861,\nNew York, NY, USA. Association for Computing Ma-\nchinery.\nKaran Shukla, Suzen Fylke, Hannes Hapke, Kalvin Le-\nung, et al. 2021. Model card toolkit.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for deep\nlearning in nlp. arXiv preprint arXiv:1906.02243.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP), pages\n9275\u20139293, Online. Association for Computational Lin-\nguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n2023. Llama 2: Open foundation and fine-tuned chat\nmodels. arXiv preprint arXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need.\nAdvances in neural information processing systems, 30.\nPablo Villalobos, Jaime Sevilla, Tamay Besiroglu,\nLennart Heim, Anson Ho, and Marius Hobbhahn. 2022.\nMachine learning model sizes and the parameter gap.\narXiv preprint arXiv:2207.02852.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020a.\nMinilm: Deep self-\nattention distillation for task-agnostic compression of\npre-trained transformers. Advances in Neural Informa-\ntion Processing Systems, 33:5776\u20135788.\n\nZhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and\nChangyou Chen. 2020b. Towards faithful neural table-\nto-text generation with content-matching constraints. In\nProceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 1072\u20131086,\nOnline. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of\nthe 2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages\n1112\u20131122, New Orleans, Louisiana. Association for\nComputational Linguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022. Re-\nact: Synergizing reasoning and acting in language mod-\nels. arXiv preprint arXiv:2210.03629.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text generation.\nAdvances in Neural Information Processing Systems,\n34:27263\u201327277.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized embed-\ndings and earth mover distance.\nIn Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), pages 563\u2013578, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-\nhan Li, Dacheng Li, Eric Xing, et al. 2024. Judging\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\nvances in Neural Information Processing Systems, 36.\n\nA\nQuestion Templates\nTables 10 and 11 shows full question templates of\nmodel cards and data cards. We have 31 questions\nin total for generating model cards, and 21 ques-\ntions for generating data cards. We create these\nquestions based on the template provided by HF,8\nand include necessary requirements.\nB\nDataset Collection Details\nFor the model card evaluation set selection, we\nselect all 350 examples that are rewritten by the\nHF team with their unique disclaimers, as shown\nin Figure 3.\nFigure 3: bert-base-uncased (Devlin et al., 2018) as\na current model card example with a unique disclaimer\nsentence, indicating a modification by the HF team.\nC\nDataset Annotation Details\nHuman Annotation Guidelines\nTo evaluate pa-\nper links and direct GitHub links on the model\ncard evaluation set, we require the annotators to go\nthrough each current model card and provide all\npossible paper links and GitHub links to annotators.\nThey are asked to select the direct paper link and\nGitHub link from all candidate links, by looking\nat their positions of occurrences in the model card\nexample. If no direct links of either sources can be\ndetermined, they need to label this model card as\n\u201cInvalid\u201d.\nGPT Annotation Details\nWe show our two-shot\nprompts for asking GPT-3.5-turbo to select di-\nrect paper links in Figure 4. Direct GitHub link\nselection is prompted similarly.\n8https://github.com/huggingface/huggingface_\nhub/tree/main/src/huggingface_hub/templates\nYou are a helpful assistant.\nDescriptions about the model {model} that might contain its paper links are \nenclosed by ``` below:\n```\n<contexts_including_link1>\\n<contexts_including_link2>\\n\u2026\n```\nHere are candidate paper link references from the passage above: <link1>, \n<link2>, \u2026\nWhich paper should be the direct one that introduces the model? If none of \nthe papers are the direct reference to the model, please answer \"None\".\nThe direct paper link is <direct_link>.\nFigure 4: Prompts for calling GPT3.5 to select direct\npaper links. We prepend one positive example and one\nnegative example to the message list to improve its in-\nference quality.\nLLM\n# words\n# sentences\n# links\nGPT3.5\n4023.88\n215.17\n4.18\nLlama2 70B Chat\n6210.32\n323.56\n4.55\nLlama2 7B Chat\n5548.50\n302.73\n1.44\nMistral 7B Inst\n4126.07\n202.16\n2.65\nTable 12: Statistics about whole generated model cards\nD\nDataset Analysis\nWe provide the number of card examples with di-\nrect paper links in their human-generated cards,\nwith direct GitHub repository links, and with both\nlinks in Table 13. We also provide additional fig-\nures about the dataset task taxonomy in ????. The\ntaxonomy is obtained using the REST API of HF\nHub.\nSplit\nMeasure\n# w/ papers\n# w/ repos\n# w/ both\nModelCard\nall\n# samples\n5689\n4829\n2485\n# words\n1064\n948\n1134\ntest\n# samples\n344\n299\n294\n# words\n668\n710\n711\nDataCard\nall\n# samples\n660\n533\n328\n# words\n1394\n1104\n1416\ntest\n# samples\n86\n71\n50\n# words\n1003\n1290\n1155\nTable 13: Statistics for crawled model cards and data\ncards, including the number of examples with direct\npaper links or direct github links or both, and the average\nnumber of words in each category.\nE\nRetriever Details\nWe use FAISS as our embedding store database\n(Johnson et al., 2019). We fix the chunk size as\n512 and the chunk overlap as 64. After retrieving\nrelevant sections, we choose to obtain 8 chunks\nfrom these sections, together with 4 other chunks\nfrom other sections to reduce the bias propagation.\n\nQuestion\nRole\nPrompt\nSummary\nProject organizer\nProvide a 1-2 sentence summary of what the model is.\nDescription\nProject organizer\nProvide basic details about the model. This includes the model architecture,\ntraining procedures, parameters, and important disclaimers.\nFunded by\nProject organizer\nList the people or organizations that fund this project of the model.\nShared by\nDeveloper\nWho are the contributors that made the model available online as a GitHub\nrepo?\nModel type\nProject organizer\nSummarize the type of the model in terms of the training method, machine\nlearning type, and modality in one sentence.\nLanguage\nProject organizer\nSummarize what natural human language the model uses or processes in one\nsentence.\nLicense\nProject organizer\nProvide the name and link to the license being used for the model.\nFinetuned from\nProject organizer\nIf the model is fine-tuned from another model, provide the name and link to\nthat base model.\nDemo sources\nProject organizer\nProvide the link to the demo of the model.\nDirect use\nProject organizer\nExplain how the model can be used without fine-tuning, post-processing, or\nplugging into a pipeline. Provide a code snippet if necessary\nDownstream use\nProject organizer\nExplain how this model can be used when fine-tuned for a task or when\nplugged into a larger ecosystem or app. Provide a code snippet if necessary\nOut of scope use\nSociotechnic\nHow the model may foreseeably be misused and address what users ought\nnot do with the model.\nBias risks limitations\nSociotechnic\nWhat are the known or foreseeable issues stemming from this model? These\ninclude foreseeable harms, misunderstandings, and technical and sociotechni-\ncal limitations.\nBias recommendations\nSociotechnic\nWhat are recommendations with respect to the foreseeable issues about the\nmodel?\nTraining data\nDeveloper\nWrite 1-2 sentences on what the training data of the model is. Links to\ndocumentation related to data pre-processing or additional filtering may go\nhere as well as in More Information.\nPreprocessing\nDeveloper\nProvide detail tokenization, resizing/rewriting (depending on the modality),\netc. about the preprocessing for the data of the model.\nTraining regime\nDeveloper\nProvide detail training hyperparameters when training the model.\nSpeeds sizes times\nDeveloper\nProvide detail throughput, start or end time, checkpoint sizes, etc. about the\nmodel.\nTesting data\nDeveloper\nProvide benchmarks or datasets that the model evaluates on.\nTesting factors\nSociotechnic\nWhat are the foreseeable characteristics that will influence how the model\nbehaves? This includes domain and context, as well as population subgroups.\nEvaluation should ideally be disaggregated across factors in order to uncover\ndisparities in performance.\nTesting metrics\nDeveloper\nWhat metrics will be used for evaluation in light of tradeoffs between different\nerrors about the model?\nResults\nDeveloper\nProvide evaluation results of the model based on the Factors and Metrics.\nResults summary\nDeveloper\nSummarize the evaluation results about the model.\nModel examination\nDeveloper\nThis is an experimental section some developers are beginning to add, where\nwork on explainability/interpretability may go about the model.\nHardware\nDeveloper\nProvide the hardware type that the model is trained on.\nSoftware\nDeveloper\nProvide the software type that the model is trained on.\nHours used\nDeveloper\nProvide the amount of time used to train the model.\nCloud provider\nDeveloper\nProvide the cloud provider that the model is trained on.\nCo2 emitted\nDeveloper\nProvide the amount of carbon emitted when training the model.\nModel specs\nDeveloper\nProvide the model architecture and objective about the model.\nCompute infrastructure\nDeveloper\nProvide the compute infrastructure about the model.\nTable 10: Template of the all questions necessary for generating a whole model card.\nF\nGenerator Details\nOpen-sourced LLMs are inferenced through vllm\nKwon et al. (2023). Llama2-70B-Chat-HF is run\non 4 A6000s. Two 7B models are run on 1 A6000.\nWe fix temperature to 0 to ensure a stable gener-\nation quality. We show our prompt description\nof different roles in Table 14, and the generation\nprompt in Figure 8.\nG\nLLM Generated Model Card Statistics\nStatistics about LLM generated model cards are\nshown in Tables 12 and 16 to 18.\nH\nMetric Details\nFor standard metrics, we use the list of re-\ntrieved texts together with the generated answer\nas inputs.\nWe normalize all these scores to\nbe in the [0,1] range.\nSince the output of\n\nTask taxonomy for models in model cards\ntext-generation\ntext-to-image\nNone\nimage-classification\ntext-classification\nfill-mask\ntext2text-generation\nfeature-extraction\ntoken-classification\ntranslation\nsentence-similarity\nautomatic-speech-recognition\nquestion-answering\nobject-detection\nconversational\nsummarization\nimage-segmentation\nimage-to-text\nzero-shot-image-classification\n...\nTask taxonomy for datasets in dataset cards\ntext-classification\ntext-generation\nNone\ntoken-classification\ntranslation\nfill-mask\nsummarization\ntext2text-generation\nautomatic-speech-recognition\ntext-retrieval\nother\nconversational\ntabular-classification\nimage-classification\nobject-detection\nmultiple-choice\nimage-to-text\ntext-to-image\naudio-classification\n...\nFigure 5: The task taxonomy of models in the model cards dataset (left), and the task taxonomy of datasets in the\ndataset cards dataset (right), with the inner circle as the test set, and the outer circle as the whole set.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRelative position in number of downloads\n102\n103\n104\n105\n106\n107\n108\nNumber of downloads\nDistribution in Model/Dataset Card Downloads\nModel card test set\nModel card whole set\nDataset card test set\nDataset card whole set\nFigure 6: Distribution of the amount of downloads for\nthe whole dataset and the test set. Test set examples\ndistribute quite uniformly.\nnli-deberta-v3-large is in {\u201ccontradiction\u201d,\n\u201centailment\u201d, \u201cneutral\u201d}, we map these outputs\nto {0, 0.5, 1}, respectively to maintain a percent-\nage scale. We use the implementation of ROUGE\nscore by HF. We use official implementations for\nBERTScore and BARTScore.\nFor GPT metrics, we use GPT-4-1106-preview\nas evaluators for the main results, and use\nGPT-3.5-turbo for ablation studies.\nI\nHuman Annotation Details\nWe give two annotators the same set of examples\neach with seven model cards generated by LLMs\nand one written by human. For each model ex-\nample for which LLMs generate model cards, we\nprovide annotators with the model name, the corre-\nsponding paper link, GitHub link, and a collection\nof model cards created by humans or LLMs, as il-\nlustrated in Figure 9. We also provide the question\ntemplate set in Table 10, along with the following\n0\n500\n1000\n1500\n2000\nLength\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\nDensity\nComparison of Pseudo Answer Length Distributions\nGPT3.5\nMistral 7B Inst\nFigure 7: Distribution comparison of pseudo answer\nlength generated by GPT3.5 and Mistral 7B Instruct.\ninstructions:\nAnnotators are asked to rank the model cards based\non five criteria: completeness, accuracy, objectiv-\nity, understandability, and reference quality. The\nranking is asked to consider the summation of the\nbinary classification score of whether each question\nfrom the model card\u2019s question template is satis-\nfactorily answered according to the specific metric.\nThe final score reported in Table 4 is calculated\nby simply subtracting the rank from (1 + the total\nnumber of candidates). Further, we define each\nmetric as follows:\n\u2022 Completeness: Does the model card com-\nprehensively cover essential aspects such as\nmodel summary, description, intended uses,\nevaluation results, and information about bi-\nases or limitations?\n\u2022 Accuracy: Are answers to all the questions in\nthe model card consistent and accurate com-\n\nYou are a helpful assistant.\n{role_specification}Below is the reference to refer to and the question you need to answer for the dataset {model} that \nyou have worked on:\nReferences:\n```\n{reference}\n```\nQuestion:\n```\n{query}\n```\nPlease refer to the above contents of \"References\" as the knowledge source to answer the question about your dataset \n{model}. If you don't know the answer for a specific part, you should just say \"[More Information Needed]\". You can write \ncode only if you find a direct code block reference from above, otherwise just output \"[More Information Needed]\". Your \nanswer should be easy to read and succinct.\n<answer>\nFigure 8: Our generation prompt templates.\npared to the details provided in the model\u2019s\nofficial paper and GitHub READMEs?\n\u2022 Objectivity: Does the model card present a\nbalanced perspective of the model, recogniz-\ning both its strengths and weaknesses?\n\u2022 Understandability: Is the information in the\nmodel card clear and easily understandable for\nboth technical and non-technical audiences?\nAre complex technical concepts explained in\na manner that can be easily grasped by users\nwithout in-depth technical knowledge?\n\u2022 Reference Quality: Does the model card in-\nclude necessary citations and references to re-\nlated papers and links? Do all provided links\nredirect correctly to their intended URLs?\nIn cases where the summation scores for a question\nare tied for multiple models, we allow annotators\nthe discretion to rank based on the quality of an-\nswers to the most important questions, including\nmodel summary, description, intended uses, evalu-\nation results, and information about biases or limi-\ntations.\nWe calculate the Krippendorff\u2019s \u03b1 among the re-\nsults of two annotators, and got mean=0.68 and\nstd=0.29 for the agreement level. We report av-\neraged ranking scores in Table 4. Note that we\ndon\u2019t have direct comparison across human eval-\nuation metrics vs. automatic metrics, since our\nhuman metrics evaluate on a whole model card,\nwhile automatic metrics take each (Q, R, A) tuple\nfor evaluation and they have different scales. We\nneed to implement human metrics in this way to\nsupplement the limited scope of automatic metrics\u2019\nfocus.\nJ\nRetrieval Only Baseline Details\nFollowing Fu et al. (2023), we prompt GPT3.5 to\nassess the understandability and brevity of gener-\nated texts according to input questions. Since there\nare only two methods to evaluate: retrieval-only\nand CARDGEN, we use the comparative assess-\nment proposed by Liusie et al. (2024), to compare\nthese two candidates in a pairwise manner. The\ndefinition of understandability is the same as in the\nhuman annotation. Figures 10 and 11 shows the\nprompt templates we use to generate comparative\nresults. The order of two candidates in the prompt\nis randomly shuffled to avoid positional bias. We\nreport the ratio of one method better than another\nfor understandability in Table 8. Full results includ-\ning brevity are reported in Table 15.\nK\nPseudo Answer Ablation Study\nAnalyses\nWe show the distribution of pseudo answer length\ngenerated by GPT3.5 and Mistral 7B Instruct in\nFigure 7.\n\nFigure\n9:\nThe\nhuman\nannotation\ninterface\nbuilt\nby\ngradio\nwith\nan\nexample\nof\nmodel\nbert-large-cased-whole-word-masking (Abid et al., 2019; Devlin et al., 2018).\nThe information that\na model card is written by whom is hidden, and orders of five model cards shown at each time are randomly shuffled\nto avoid positional bias.\nL\nGeneration Only Ablation Study\nAnalyses\n\nFigure 10: Prompt template to compare CARDGEN\u2019s understandability to the retrieval-only baseline.\nFigure 11: Prompt template to compare CARDGEN\u2019s understandability to the retrieval-only baseline.\n\nQuestion\nRole\nPrompt\nDescription\nData manager\nProvide the homepage link for the dataset, just give me a link please.\nLeaderboard\nData manager\nProvide the Leaderboard link for the dataset.\nPointofcontact\nData manager\nProvide the Point of Contact for the dataset.\nSummary\nData manager\nProvide basic details about the dataset. Briefly summarize the dataset,\nits intended use and the supported tasks. Give an overview of how\nand why the dataset was created. The summary should explicitly\ndescribe the domain, topic, or genre covered.\nSupported tasks and leaderboards\nData analyst\nDescribe the tasks and leaderboards supported by the dataset. Include\ntask description, metrics, suggested models, and leaderboard details.\nLanguages\nData analyst\nProvide an overview of the languages represented in the dataset,\nincluding details like language type, script, and region. Include BCP-\n47 codes if available.\nData instances\nData scientist\nProvide a JSON-formatted example of a typical instance in the dataset\nwith a brief description. Include a link to more examples if available.\nDescribe any relationships between data points.\nData fields\nData architect\nList and describe the fields in the dataset, including their data type,\nusage in tasks, and attributes like span indices. Mention if the dataset\ncontains example IDs and their inherent meaning.\nData splits\nData manager\nDescribe the data splits in the dataset. Include details such as the\nnumber of splits, any criteria used for splitting the data, differences\nbetween the splits, and the sizes of each split. Provide descriptive\nstatistics for the features where appropriate, for example, average\nsentence length for each split.\nCuration rationale\nData manager\nWhat need or purpose motivated the creation of this dataset? Describe\nthe underlying reasons and major choices involved in its assembly.\nExplain the significance of the dataset in its field and any specific\ngaps or demands it aims to address.\nSource data\nData manager\nDescribe the source data used for this dataset. Describe the data\ncollection process. Describe any criteria for data selection or filtering.\nList any key words or search terms used. If possible, include runtime\ninformation for the collection process.\nSource language producers\nData manager\nClarifying the human or machine origin of the dataset. Avoiding\nassumptions about the identity or demographics of the data creators.\nProviding information about the people represented in the data, with\nreferences where applicable.\nAnnotations\nData manager\nDescribe the annotation process to the dataset. Detail the annotation\nprocess and tools used, or note if none were applied. Specify the\nvolume of data annotated.\nAnnotators\nData manager\nDescribe the annotator of the dataset. For annotations in the dataset,\nstate their human or machine-generated nature. Describe the creators\nof the annotations, their selection process, and any self-reported\ndemographic information.\nPersonal and sensitive information\nData manager\nCategorize how identity data, such as gender referencing Larson\n(2017), is sourced and used in the dataset. Indicate if the data in-\ncludes sensitive information or can identify individuals. Describe any\nanonymization methods applied.\nSocial impact of dataset\nData manager\nExplore the dataset\u2019s social impacts: its role in advancing technol-\nogy and enhancing quality of life. Consider negative effects like\ndecision-making opacity and reinforcing biases. Check if it includes\nlow-resource or under-represented languages. Assess its impact on\nunderserved communities.\nDiscussion of biases\nData manager\nWhen constructing datasets, especially those including text-based\ncontent like Wikipedia articles, biases may be present. If there have\nbeen analyses to quantify these biases, it\u2019s important to summarize\nthese studies and note any measures taken to mitigate the biases.\nOther known limitation\nData analyst\nOutline and cite any known limitations of the dataset, such as annota-\ntion artifacts, in your studies.\nDataset curators\nData manager\nList the people involved in collecting the dataset and their affiliations.\nIf known, include information about funding sources for the dataset.\nThis should encompass individuals, organizations, and any collabora-\ntive efforts involved in the dataset creation.\nLicensing information\nLegal advisor\nProvide the license and link to the license webpage if available for\nthe dataset.\nContributions\nData manager\nWrite in 1-2 sentence about the contributers for the dataset.\nMention the GitHub username and provide their GitHub pro-\nfile link.\nYou should follows the format: Thanks to [@github-\nusername](https://github.com/<github-username>) for adding this\ndataset.\nTable 11: Template of the all questions necessary for generating a whole data card.\n\nCard\nRole\nDescription\nModelCard\nDeveloper\nwho writes the code and runs training\nSociotechnic\nwho is skilled at analyzing the interaction of technology and society long-term (this\nincludes lawyers, ethicists, sociologists, or rights advocates)\nProject organizer\nwho understands the overall scope and reach of the model and can roughly fill out each\npart of the card, and who serves as a contact person for model card updates\nDataCard\nData curator\nwho collects and organizes the data\nData analyst\nwho is skilled at understanding and documenting dataset characteristics and biases\nData manager\nwho oversees dataset versioning, availability, and usage guidelines\nTable 14: Our prompts for different roles in answering specific questions.\nModel\nMethod\n# Words\nAR\nUnderstandability\nBrevity\nGPT3.5\nRetrieval only\n613.95\n81.28\n5.60%\n1.60%\nCARDGEN\n200.74\n90.84 (+9.56)\n94.40%\n98.40%\nLlama2 70B\nRetrieval only\n645.91\n81.61\n1.60%\n3.20%\nCARDGEN\n230.42\n90.32 (+8.71)\n98.40%\n96.80%\nLlama2 7B\nRetrieval only\n603.45\n81.32\n4.40%\n2.80%\nCARDGEN\n203.70\n90.78 (+9.46)\n95.60%\n97.20%\nMistral 7B\nRetrieval only\n590.35\n81.49\n2.40%\n2.40%\nCARDGEN\n189.11\n89.83 (+8.34)\n97.60%\n97.60%\nTable 15: GPT3.5 evaluation results of the retrieval-only baseline and CARDGEN on word numbers, answer\nrelevance, understandability, brevity.\n\nContext precision\nContext relevance\nFaithfulness\nMetric types\n10\n20\n30\n40\n50\n60\n70\nMetric Score (%)\n50.87\n41.90\n67.28\n42.80\n28.02\n64.16\n40.43\n26.96\n65.32\nGPT3.5 + Jina-v2-base\nGPT3.5 + MiniLM-l6-v2\nGPT3.5 + Mpnet-base-v2\nFigure 12: Comparison of three embedding models on\ncontext precision, context relevance, and faithfulness.\nQuestion\nGPT3.5\nLlama2 70B\nLlama2 7B\nMistral 7B\nSummary\n53.91\n89.40\n71.93\n63.61\nDescription\n275.47\n276.50\n187.40\n264.11\nFunded by\n78.29\n96.10\n91.97\n31.15\nShared by\n33.41\n108.62\n57.94\n43.69\nModel type\n46.11\n115.77\n67.69\n56.07\nLanguage\n30.24\n100.23\n57.52\n21.67\nLicense\n47.56\n94.86\n43.05\n42.63\nFinetuned from\n93.95\n137.65\n115.16\n65.91\nDemo sources\n76.70\n150.54\n228.09\n141.35\nDirect use\n227.26\n247.95\n260.14\n211.97\nDownstream use\n287.05\n256.03\n301.56\n254.17\nOut of scope use\n305.64\n341.98\n339.81\n225.52\nBias risks limitations\n305.09\n330.94\n317.83\n274.26\nBias recommendations\n298.46\n333.96\n336.44\n309.82\nTraining data\n61.17\n103.41\n72.18\n85.98\nPreprocessing\n169.67\n285.66\n228.65\n222.67\nTraining regime\n110.71\n208.14\n162.46\n179.76\nSpeeds sizes times\n170.33\n250.69\n211.52\n192.81\nTesting data\n112.20\n144.15\n87.29\n87.16\nTesting factors\n230.03\n293.02\n344.08\n245.14\nTesting metrics\n64.45\n267.89\n226.08\n137.77\nResults\n137.94\n276.72\n263.82\n210.40\nResults summary\n154.57\n230.82\n215.33\n136.51\nModel examination\n214.29\n317.01\n264.26\n169.52\nHardware\n24.87\n81.48\n72.26\n21.44\nSoftware\n64.71\n91.29\n49.32\n23.53\nHours used\n27.95\n172.74\n164.28\n58.86\nCloud provider\n26.13\n82.82\n56.88\n18.55\nCo2 emitted\n36.01\n220.29\n243.23\n33.65\nModel specs\n207.91\n276.66\n204.47\n161.47\nCompute infrastructure\n51.80\n227.01\n205.86\n134.92\nTable 16: Number of words in generated model cards\nper question averaged by all samples in the test set.\nQuestion\nGPT3.5\nLlama2 70B\nLlama2 7B\nMistral 7B\nSummary\n1.95\n3.23\n2.61\n2.39\nDescription\n14.51\n13.87\n8.93\n12.87\nFunded by\n4.25\n4.96\n6.40\n1.89\nShared by\n1.86\n4.53\n3.18\n2.41\nModel type\n1.51\n3.47\n2.68\n1.70\nLanguage\n1.10\n4.30\n1.84\n1.09\nLicense\n2.78\n4.74\n2.79\n2.49\nFinetuned from\n4.81\n5.96\n5.98\n3.47\nDemo sources\n3.83\n7.42\n12.81\n6.48\nDirect use\n8.78\n7.45\n12.20\n6.29\nDownstream use\n10.23\n8.11\n16.29\n7.30\nOut of scope use\n16.50\n21.69\n20.71\n10.20\nBias risks limitations\n19.07\n22.76\n19.05\n16.36\nBias recommendations\n18.04\n22.13\n19.88\n18.44\nTraining data\n3.14\n4.54\n3.31\n4.01\nPreprocessing\n11.06\n18.20\n13.34\n12.46\nTraining regime\n4.82\n12.66\n7.19\n11.08\nSpeeds sizes times\n8.41\n12.74\n10.62\n9.40\nTesting data\n7.98\n9.00\n5.55\n4.96\nTesting factors\n13.26\n17.23\n21.64\n11.60\nTesting metrics\n3.67\n14.11\n14.20\n7.12\nResults\n7.69\n16.85\n16.22\n10.50\nResults summary\n9.01\n10.94\n9.79\n6.21\nModel examination\n11.32\n17.74\n15.67\n8.47\nHardware\n1.73\n4.29\n3.43\n1.39\nSoftware\n3.50\n4.54\n2.45\n1.47\nHours used\n2.06\n7.52\n8.29\n2.86\nCloud provider\n1.82\n4.38\n2.92\n1.32\nCo2 emitted\n2.40\n9.14\n10.27\n2.13\nModel specs\n10.52\n12.12\n9.90\n7.17\nCompute infrastructure\n3.59\n12.94\n12.61\n6.61\nTable 17: Number of sentences in generated model cards\nper question averaged by all samples in the test set.\nQuestion\nGPT3.5\nLlama2 70B\nLlama2 7B\nMistral 7B\nSummary\n0.02\n0.05\n0.00\n0.01\nDescription\n0.17\n0.04\n0.01\n0.04\nFunded by\n0.37\n0.06\n0.05\n0.06\nShared by\n0.36\n0.58\n0.04\n0.12\nModel type\n0.00\n0.00\n0.00\n0.00\nLanguage\n0.01\n0.00\n0.00\n0.01\nLicense\n0.53\n0.82\n0.17\n0.36\nFinetuned from\n0.26\n1.06\n0.30\n0.49\nDemo sources\n0.66\n0.82\n0.51\n0.94\nDirect use\n0.34\n0.05\n0.01\n0.09\nDownstream use\n0.17\n0.03\n0.02\n0.04\nOut of scope use\n0.20\n0.00\n0.00\n0.00\nBias risks limitations\n0.01\n0.00\n0.00\n0.00\nBias recommendations\n0.04\n0.01\n0.00\n0.00\nTraining data\n0.29\n0.24\n0.00\n0.02\nPreprocessing\n0.04\n0.03\n0.00\n0.01\nTraining regime\n0.00\n0.03\n0.01\n0.01\nSpeeds sizes times\n0.21\n0.10\n0.02\n0.05\nTesting data\n0.01\n0.01\n0.03\n0.02\nTesting factors\n0.01\n0.00\n0.00\n0.01\nTesting metrics\n0.01\n0.02\n0.00\n0.01\nResults\n0.03\n0.05\n0.03\n0.04\nResults summary\n0.04\n0.03\n0.04\n0.09\nModel examination\n0.19\n0.04\n0.02\n0.02\nHardware\n0.00\n0.02\n0.04\n0.01\nSoftware\n0.03\n0.12\n0.00\n0.04\nHours used\n0.01\n0.02\n0.00\n0.01\nCloud provider\n0.03\n0.11\n0.04\n0.02\nCo2 emitted\n0.01\n0.11\n0.00\n0.00\nModel specs\n0.11\n0.04\n0.01\n0.03\nCompute infrastructure\n0.02\n0.05\n0.05\n0.10\nTable 18: Number of links in generated model cards per\nquestion averaged by all samples in the test set.\n"
}